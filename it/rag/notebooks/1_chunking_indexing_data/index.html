

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>General &mdash; CASTIEL2 Multi-GPU AI  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=9c3e77be" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx_lesson.css?v=e9df6548" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/term_role_formatting.css?v=4194e21c" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx_rtd_theme_ext_color_contrast.css?v=8e8ea19f" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/overrides.css?v=d560b895" />

  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
      <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../_static/documentation_options.js?v=187304be"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../_static/copybutton.js?v=35a8b989"></script>
      <script src="../../../../_static/minipres.js?v=a0d29692"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../../../_static/togglebutton.js?v=1ae7504c"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../search/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../" class="icon icon-home">
            CASTIEL2 Multi-GPU AI
              <img src="../../../../_static/CASTIEL2.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The lesson materials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../leonardo/README/">1M: Access to Leonardo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../se/deep-learning-intro/">1A: Introduction to Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nl/">2M: PyTorch Distributed Data Parallel</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../quick-reference/">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide/">Instructor’s guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../directives/">Directives</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../">CASTIEL2 Multi-GPU AI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">General</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/castiel-multi-gpu-ai/blob/main/content/it/rag/notebooks/1_chunking_indexing_data.ipynb" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="general">
<h1>General<a class="headerlink" href="#general" title="Link to this heading"></a></h1>
<p>Retrieval augmented generation is a technique introduced by researchers at Facebook AI Research (FAIR) in paper from 2020 titled “Retrieval-augmented Generation for Knowledge-Intensive NLP Tasks”.</p>
<p>The idea behind RAG is that we can add knowledge to an already trained LLM by injecting into the context useful information retrieved after receiving the user’s query. This is very useful because otherwise the only way to “add” new knowledge to an LLM would be to finetune the model over new data.</p>
<p><img alt="rag_sequence_diagram" src="../../../../_images/rag_seq_diagram.png" /></p>
<p>Retrieval Augmented Generation systems are interesting for companies because it’s a way to have a model that:</p>
<ul class="simple">
<li><p>their users can query about internal company procedures;</p></li>
<li><p>can be updated “on the fly” by indexing new documents/reindexing updated documents;</p></li>
<li><p>the company has total control over model knowledge base (i.e. a document is no longer relevant? Just remove it from the knowledge base);</p></li>
</ul>
<p>Let’s see a brief demo of how RAG works.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="imports">
<h1>Imports<a class="headerlink" href="#imports" title="Link to this heading"></a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">socket</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">glob</span><span class="w"> </span><span class="kn">import</span> <span class="n">glob</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">concurrent.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">ThreadPoolExecutor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers</span><span class="w"> </span><span class="kn">import</span> <span class="n">ContextualCompressionRetriever</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers.document_compressors</span><span class="w"> </span><span class="kn">import</span> <span class="n">CrossEncoderReranker</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.cross_encoders</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceCrossEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.documents</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_text_splitters</span><span class="w"> </span><span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">,</span> <span class="n">SentenceTransformersTokenTextSplitter</span><span class="p">,</span> <span class="n">CharacterTextSplitter</span><span class="p">,</span> <span class="n">MarkdownTextSplitter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_chroma.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span> 
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">chromadb</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chromadb.errors</span><span class="w"> </span><span class="kn">import</span> <span class="n">NotFoundError</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rank_bm25</span><span class="w"> </span><span class="kn">import</span> <span class="n">BM25Okapi</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics.pairwise</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/leonardo/home/userinternal/rmioli00/git/cinecaxtpc25/session_5/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="env-config">
<h1>Env config<a class="headerlink" href="#env-config" title="Link to this heading"></a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="n">date</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[ :-]&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">t0</span><span class="p">)[:</span><span class="mi">19</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Last execution </span><span class="si">{</span><span class="n">t0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">load_dotenv</span><span class="p">()</span>

<span class="c1"># Models</span>
<span class="n">EMBEDDER</span> <span class="o">=</span> <span class="s2">&quot;BAAI/bge-m3&quot;</span>
<span class="n">RERANKER</span> <span class="o">=</span> <span class="s2">&quot;BAAI/bge-reranker-v2-m3&quot;</span>
<span class="n">LLM</span> <span class="o">=</span> <span class="s2">&quot;mistralai/Mistral-Small-3.1-24B-Instruct-2503&quot;</span>

<span class="c1"># Endpoints</span>
<span class="n">VLLM_OPENAI_ENDPOINT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_OPENAI_ENDPOINT&quot;</span><span class="p">]</span>
<span class="n">VLLM_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_KEY&quot;</span><span class="p">]</span>

<span class="c1"># Paths</span>
<span class="n">PROMPT_PATH</span> <span class="o">=</span> <span class="s2">&quot;../data/prompts&quot;</span>
<span class="n">INPUT_PATH</span> <span class="o">=</span> <span class="s2">&quot;../data/input&quot;</span>
<span class="n">OUTPUT_PATH</span> <span class="o">=</span> <span class="s2">&quot;../data/output/chunking&quot;</span>

<span class="n">Path</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="p">)</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Use GPU 3 for this notebook. GPUs 0 and 1 are used to load the llm</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">VLLM_OPENAI_ENDPOINT</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">VLLM_KEY</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">LLM</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_completion_tokens</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Last execution 2025-07-28 16:39:27.599879
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="why-do-we-need-rag-systems">
<h1>Why do we need RAG systems?<a class="headerlink" href="#why-do-we-need-rag-systems" title="Link to this heading"></a></h1>
<p>Let us evaluate the responses of an off-the-shelf pretrained model to a set of questions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_questions</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;What GPUs are available on Leonardo?&quot;</span><span class="p">,</span> <span class="s2">&quot;Is there any partition without gpus?&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;What GPUs are available on the Cloud?&quot;</span><span class="p">,</span> <span class="s2">&quot;Can I associate a domain name to a vm?&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;What are the naming conventions I should follow when asking for a domain name for a vm machine?&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;What is Cineca AI and how do I enable it?&quot;</span><span class="p">,</span> 
                  <span class="s2">&quot;What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">test_questions</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[QUESTION]: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">answ</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">([(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;You are an helpful assistant, answer to the user&#39;s questions in a precise and concise manner.&quot;</span><span class="p">),</span>
                       <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="n">question</span><span class="p">)])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[ANSWER]: &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">answ</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[QUESTION]: What GPUs are available on Leonardo?
[ANSWER]: As of my last update in October 2023, Leonardo, which is part of the EuroHPC Joint Undertaking, is designed to be one of the most powerful supercomputers in Europe. It is equipped with advanced GPUs to handle its high-performance computing tasks. Specifically, Leonardo is known to feature NVIDIA A100 GPUs. These GPUs are part of the NVIDIA Ampere architecture and are widely used in supercomputing and AI applications due to their high performance and efficiency.

For the most current and detailed information, I recommend checking the official EuroHPC or Leonardo supercomputer documentation, as hardware specifications can be updated or expanded over time.

[QUESTION]: Is there any partition without gpus?
[ANSWER]: Yes, there are partitions without GPUs. In many high-performance computing (HPC) environments, clusters are often divided into different partitions to cater to various types of workloads. Some common types of partitions include:

1. **CPU-only partitions**: These partitions are dedicated to jobs that do not require GPU acceleration. They typically consist of nodes with multiple CPU cores but no GPUs.

2. **GPU partitions**: These partitions are equipped with nodes that have one or more GPUs, suitable for tasks that benefit from GPU acceleration, such as machine learning, deep learning, and certain scientific computations.

3. **Large memory partitions**: These partitions have nodes with a significant amount of RAM, ideal for memory-intensive tasks.

4. **High-throughput partitions**: These partitions are designed to handle a large number of smaller jobs efficiently.

5. **Debugging partitions**: These partitions are used for testing and debugging code, often with shorter time limits and fewer resources.

To determine the specific partitions available in a particular HPC system, you should refer to the documentation or help resources provided by the system administrators.

[QUESTION]: What GPUs are available on the Cloud?
[ANSWER]: The availability of GPUs on the cloud varies by provider, but here are some of the most commonly offered GPUs from major cloud service providers:

1. **Amazon Web Services (AWS)**:
   - NVIDIA Tesla V100
   - NVIDIA A10G
   - NVIDIA T4
   - NVIDIA K80
   - AMD Radeon Pro V520
   - NVIDIA A100 (80GB and 40GB versions)

2. **Microsoft Azure**:
   - NVIDIA Tesla V100
   - NVIDIA A100 (80GB and 40GB versions)
   - NVIDIA T4
   - NVIDIA K80
   - AMD Radeon Instinct MI25

3. **Google Cloud Platform (GCP)**:
   - NVIDIA Tesla V100
   - NVIDIA A100 (80GB and 40GB versions)
   - NVIDIA T4
   - NVIDIA K80

4. **IBM Cloud**:
   - NVIDIA Tesla V100
   - NVIDIA T4
   - NVIDIA K80

5. **Oracle Cloud Infrastructure (OCI)**:
   - NVIDIA Tesla V100
   - NVIDIA A100 (80GB and 40GB versions)
   - NVIDIA T4

6. **Alibaba Cloud**:
   - NVIDIA Tesla V100
   - NVIDIA T4
   - NVIDIA K80

These GPUs are typically available in various instance types and configurations to suit different workloads, such as machine learning, deep learning, high-performance computing, and graphics-intensive applications. Always check the latest offerings on the respective cloud provider&#39;s website, as new GPU options are frequently added.

[QUESTION]: Can I associate a domain name to a vm?
[ANSWER]: Yes, you can associate a domain name with a virtual machine (VM). This process typically involves several steps:

1. **Obtain a Domain Name**: Purchase a domain name from a domain registrar if you don&#39;t already have one.

2. **Set Up DNS Records**: Configure the DNS records for your domain. This usually involves setting up an A record or CNAME record to point to the IP address of your VM.

   - **A Record**: Maps a domain name directly to an IP address.
   - **CNAME Record**: Maps a domain name to another domain name.

3. **Configure Your VM**: Ensure that your VM is configured to handle incoming traffic on the appropriate ports (e.g., HTTP/HTTPS for web servers).

4. **Update Firewall Settings**: Make sure that any firewalls (both on the VM and in the network) allow traffic to the necessary ports.

5. **Test the Configuration**: Verify that the domain name resolves to your VM&#39;s IP address and that the VM is serving the expected content.

6. **SSL/TLS (Optional)**: If you&#39;re serving HTTPS, you may need to obtain and install an SSL/TLS certificate for your domain.

Here&#39;s a simple example of setting up an A record:

- **Domain Name**: example.com
- **A Record**: example.com -&gt; 192.0.2.1 (IP address of your VM)

After configuring the DNS records, it may take some time for the changes to propagate across the internet.

[QUESTION]: What are the naming conventions I should follow when asking for a domain name for a vm machine?
[ANSWER]: When naming a domain for a virtual machine (VM), follow these conventions to ensure clarity, consistency, and ease of management:

1. **Descriptive and Unique**:
   - Use a name that describes the VM&#39;s purpose or function.
   - Ensure the name is unique within your domain namespace.

2. **Consistent Naming Scheme**:
   - Follow a consistent pattern, such as `environment-role-app.domain.com` (e.g., `prod-db-mysql.example.com`).

3. **Use Hyphens or Underscores**:
   - If using multi-word names, separate words with hyphens (e.g., `web-server`) or underscores (e.g., `web_server`), but be consistent in your choice.

4. **Avoid Special Characters**:
   - Stick to alphanumeric characters and hyphens or underscores. Avoid spaces, slashes, and other special characters.

5. **Include Environment**:
   - Indicate the environment (e.g., `dev`, `test`, `prod`) to avoid confusion (e.g., `dev-web-server.example.com`).

6. **Use Fully Qualified Domain Names (FQDN)**:
   - Include the full domain name, not just the hostname (e.g., `web-server.example.com` instead of just `web-server`).

7. **Follow Organizational Standards**:
   - Adhere to any specific naming conventions or standards established by your organization.

8. **Keep it Short but Meaningful**:
   - Aim for names that are short enough to be easily typed and remembered, but meaningful enough to convey the VM&#39;s purpose.

### Examples:
- `prod-db-mysql.example.com`
- `dev-web-app.example.com`
- `test-api-server.example.com`
- `staging-file-server.example.com`

By following these conventions, you can create a clear and manageable naming structure for your VMs.

[QUESTION]: What is Cineca AI and how do I enable it?
[ANSWER]: Cineca AI is a suite of artificial intelligence services provided by Cineca, a leading Italian supercomputing center. It offers tools and resources for AI research, development, and deployment. To enable Cineca AI, follow these general steps:

1. **Access the Cineca AI Portal**: Visit the official Cineca AI website and create an account if you don&#39;t have one.

2. **Request Access**: Submit a request to access the Cineca AI services. You may need to provide details about your project or research.

3. **Approval**: Wait for approval from Cineca. Once approved, you will receive instructions on how to access the services.

4. **Set Up Your Environment**: Follow the provided guidelines to set up your computing environment. This may include installing specific software or configuring your system.

5. **Start Using Cineca AI**: Once your environment is set up, you can start using the various AI tools and resources provided by Cineca.

For specific details, refer to the Cineca AI documentation or contact their support team.

[QUESTION]: What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?
[ANSWER]: As of my last update, the Leonardo supercomputer&#39;s BOOSTER partition typically supports several Quality of Service (QoS) queues to manage different types of jobs. While specific names can vary, common QoS queues on such high-performance computing systems often include:

1. **debug**: For short, quick tests and debugging.
2. **standard**: For standard batch jobs.
3. **express**: For jobs that need quicker turnaround.
4. **long**: For long-running jobs.
5. **gpu**: For jobs that require GPU resources.
6. **bigmem**: For jobs that require large amounts of memory.

For the most accurate and up-to-date information, you should refer to the official documentation or support resources provided by the Leonardo supercomputer&#39;s administrators.
</pre></div>
</div>
</div>
</div>
<p>Except for the first answer, all the following answers are incorrect.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We know where to find data, so we read the correspondent file. In a while we are going to automate also this step.</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">INPUT_PATH</span><span class="p">,</span> <span class="s2">&quot;leonardo.rst.txt&quot;</span><span class="p">),</span>  <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>

<span class="n">answ</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">([(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;You are an helpful assistant, answer to the user questions in a precise and concise manner.&quot;</span><span class="p">),</span>
                   <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Given the following data, what are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?</span><span class="se">\n\n</span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)])</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">answ</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The QOS queues available on the Leonardo supercomputer BOOSTER partition are:

- normal
- boost_qos_dbg
- boost_qos_bprod
- boost_qos_lprod
- boost_qos_fuabprod
- qos_fualowprio
</pre></div>
</div>
</div>
</div>
<p>In the following section, we will implement a Retrieval-Augmented Generation (RAG) system using the HPC documentation provided by Cineca to their users. As you will observe, the primary task in building RAG systems involves rigorously testing and evaluating different configurations of the information retrieval component to measure their impact on overall system performance. In a RAG pipeline, the generative AI component represents only the final stage of the process; the majority of the effort is dedicated to designing a robust retrieval system tailored to your data.</p>
<p>In our case, the documents are in <code class="docutils literal notranslate"><span class="pre">.rst</span></code> format, so we are not concerned with reading files in other formats such as <code class="docutils literal notranslate"><span class="pre">.pdf</span></code> or <code class="docutils literal notranslate"><span class="pre">docx</span></code>. However, in real-world scenarios, this step can be handled using libraries such as marker, docling, or similar tools, depending on your needs. In this tutorial, we will not focus on document parsing, as our files are already in raw text format. Additionally, the choice of parsing libraries is highly dependent on the structure of your data and your specific preferences.</p>
<section id="rag-building-blocks">
<h2>RAG building blocks<a class="headerlink" href="#rag-building-blocks" title="Link to this heading"></a></h2>
<p>The development and deployment of a RAG system typically involves five main steps:</p>
<ol class="arabic simple">
<li><p><strong>loading</strong> documents from a data source;</p></li>
<li><p><strong>chunking</strong> these documents in some (optimal) way;</p></li>
<li><p><strong>storing</strong> the chunked data in a vector database;</p></li>
<li><p><strong>retrieving</strong> relevant data to answer user queries;</p></li>
<li><p>passing the <strong>retrieve</strong>d data to an LLM for generation.</p></li>
</ol>
<p>Steps 1 to 3 are performed “offline”, while steps 4 and 5 are executed “online”.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="classical-information-retrieval-approaches">
<h1>Classical information retrieval approaches<a class="headerlink" href="#classical-information-retrieval-approaches" title="Link to this heading"></a></h1>
<p>Classical information retrieval systems operated by matching keywords in user queries with terms found in document collections, ranking documents based on <strong>term frequency and inverse document frequency (TF-IDF)</strong> without understanding the meaning of words. For example, bag-of-words models represented documents as unordered collections of terms, capturing term frequency but ignoring word order and context; search engines would then rank documents with the highest TF-IDF scores for the query terms.</p>
<p>Another classical approach was <strong>exact match retrieval using tags</strong>, where documents were labeled with controlled keywords (tags), and retrieval was based on finding documents with matching tags, as in early library catalogs or enterprise document management systems.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="semantic-search">
<h1>Semantic search<a class="headerlink" href="#semantic-search" title="Link to this heading"></a></h1>
<p>In contrast, semantic search goes beyond exact term matching by <strong>using vector representations (embeddings) of documents and queries</strong>, capturing the meaning of words and their relationships. This allows retrieval systems to surface relevant documents even when they do not contain the exact keywords used in the query but are semantically related, enabling systems to understand that “car” and “automobile” refer to the same concept or that a document discussing “heart attacks” may be relevant for the query “myocardial infarction.”</p>
<p><img alt="embeddings" src="../../../../_images/use-transformer-variant.png" /></p>
<p><img alt="embeddings_2" src="../../../../_images/use-goal.png" /></p>
<p>Image courtesy of: https://amitness.com/posts/universal-sentence-encoder.</p>
<p>The embedding phase is performed twice: over the documents, and over the query. Then, we search the document (chunk) with the lower distance with respect to the query.</p>
<section id="chunking">
<h2>Chunking<a class="headerlink" href="#chunking" title="Link to this heading"></a></h2>
<p>To compute document embeddings effectively, it is necessary to partition the text before processing. This requirement arises from the <strong>limited context length</strong> of embedding models. For example:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">BAAI/bge-m3</span> <span class="pre">embedder</span></code> supports a maximum sequence length of approximately 8,000 tokens.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">all-mpnet-base-v2</span></code> model has a context length limit of 384 tokens.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">all-MiniLM-L12-v2</span></code> model allows a maximum sequence length of 256 tokens.</p></li>
</ul>
<p>These limitations define the upper bound for the sequence length we can embed at once.</p>
<p>We will now evaluate various chunking strategies on our document collection.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">INPUT_DOCS</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">INPUT_PATH</span><span class="p">,</span> <span class="s2">&quot;*&quot;</span><span class="p">))</span>
<span class="n">INPUT_DOCS</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;../data/input/generic_share_create.rst.txt&#39;,
 &#39;../data/input/database.rst.txt&#39;,
 &#39;../data/input/hpc_software.rst.txt&#39;,
 &#39;../data/input/gaia.rst.txt&#39;,
 &#39;../data/input/miniconda.rst.txt&#39;,
 &#39;../data/input/secgroups_create.rst.txt&#39;,
 &#39;../data/input/index_storage_ops.rst.txt&#39;,
 &#39;../data/input/interactive_computing.rst.txt&#39;,
 &#39;../data/input/security_guidelines.rst.txt&#39;,
 &#39;../data/input/fip_association.rst.txt&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a list of document, with some metadata associated to each document.</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">INPUT_DOCS</span><span class="p">:</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Document</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;doc_name&quot;</span><span class="p">:</span><span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;scraped_on&quot;</span><span class="p">:</span> <span class="s2">&quot;2025-07-22&quot;</span><span class="p">}))</span>
        
<span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Document(metadata={&#39;doc_name&#39;: &#39;generic_share_create.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&#39;.. _shares_generic_create_card:\n\nCreate and use a GENERIC_TYPE share\n===================================\n\nThe following sections describe the steps needed to create a share and mount it on two VMs attached to a local network. \nNote that the user needs to configure the VMs in a way that allows logging in via ssh. \n\nRequest to be enabled to the service\n------------------------------------\n\nThe user willing to make use of the Manila service needs to send an email to superc@cineca.it, communicating \n\n- how many shares are needed.\n- their dimensions (GB).\n- the tenant\&#39;s name.\n\nOnce the tenant is enabled to the service by the User Support Team, all users of the tenant will be able to use the service. \n\nCreate share network\n--------------------\n\nAs a first step, in the :ref:`cloud/os_overview/management_tools/dashboard:horizon dashboard` you need to create the share network \nby clicking on *&quot;Create Share Network&quot;* in *&quot;Share â\x86\x92 Share Networks&quot;* and set the value for the following attributes:\n\n- Share network name.\n- network: choose the desired network, in our example example_share_guide_net.\n- subnet: choose the desired subnet, in our example example_share_guide_subnet.\n- Click on the *&quot;save&quot;* button.\n\n.. image:: /cloud/_img/op_share_generic_img1.png\n\nCreate the share\n----------------\n\nCreate the share by clicking on *&quot;Create Share&quot;* in *&quot;Share â\x86\x92 Shares&quot;* and setting the following information:\n\n- share name\n- share protocol  == &quot;NFS&quot;\n- size (on the right side is visualized information about the actual available and used space within the tenant)\n- Type == &quot;generic_type&quot;\n- Leave blank the option &quot;Make visible for all projects&quot; because it is not enabled \n- In the end, click on the *&quot;create&quot;* button.\n\n.. image:: /cloud/_img/op_share_generic_img2.png\n\n\nSet the access rule(s) on the share just created. \n\n- On the OpenStack dashboard click on *&quot;Share â\x86\x92 Shares&quot;* \n- select the share just created\n- in the menu on the right select *&quot;Manage Rules&quot;.*\n\n.. image:: /cloud/_img/op_share_generic_img3.png\n\nClick on *&quot;Add rule&quot;* and set:\n\n- access type: Choose &quot;ip&quot;, the rest of options displayed are not available for NFS share\&#39;s protocol.\n- access level: read-write or read-only (depending on your needs)\n- access to: write the IP with permission to access the share. Only one entry is allowed per rule, therefore, you will have to include a rule for the fixed-IP of each VM. \n- Finally, click on the &quot;add&quot; button.\n\n.. image:: /cloud/_img/op_share_generic_img4.png\n\nMount the share on the VMs\n--------------------------\n\nYou are now ready to mount the share on VMs. In the following example, we will consider two VM with Ubuntu 22.04 OS. **Please refer to the network guide of the operating system of your VM to be sure about the actions to be performed.**\n\n- Login into the first VM.\n- Upgrade the packages installed in the VM\n\n.. code-block:: bash\n    \n    sudo apt update\n    sudo apt upgrade\n\n- Install the client. The package name is *&quot;nfs-common&quot;*.\n\n.. code-block:: bash\n    \n    sudo apt install nfs-common\n\n- Identify or create the directory in which the share will be mounted (e.g., &quot;/mnt/share_manila&quot;) \n\n.. code-block:: bash\n   \n   sudo mkdir &lt;MOUNT_PATH&gt;\n\n- To mount the share you will need the share &lt;ACCESS_PATH&gt; displayed on the *&quot;Share Overview&quot;* page on OpenStack dashboard under the keyword *&quot;Export Location/Path&quot;*. Gather this information and proceed. \n\n.. image:: /cloud/_img/op_share_generic_img5.png\n\n- Mount the share with the following command. Beware that different versions of nfs-common are available for different versions of Ubuntu and the syntax of the mount command could change.\n\n.. code-block:: bash\n   \n   sudo mount -t nfs -v &lt;ACCESS_PATH&gt; &lt;MOUNT_PATH&gt;\n\n- Then, repeat the same steps for the second VM. &#39;)
</pre></div>
</div>
</div>
</div>
<p>These are some of the most basic chunking strategies:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Sentence</span> <span class="pre">length</span></code>: Split documents so that each sequence has a fixed length based on character length.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Recursive</span> <span class="pre">Character</span> <span class="pre">Text</span> <span class="pre">Splitter</span></code>: Split documents so that each sequence is <strong>at most</strong> a specified length <strong>and ends at a designated delimiter</strong> character (e.g., punctuation <code class="docutils literal notranslate"><span class="pre">\n</span></code>, etc).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Format-specific</span> <span class="pre">text</span> <span class="pre">splitters</span></code>: Similar to the above, but designed specifically for certain formats such as Markdown, HTML, Python code, etc., to preserve structure and syntax (these splitters use specific delimiters based on the format you are splitting).</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split at char level, 100 chars max, 0 overlap between chunks</span>
<span class="n">cts</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="p">(</span><span class="n">separator</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">chunk_overlap</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">cts</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;.. _shares_generic_create_card:\n\nCreate and use a GENERIC_TYPE share\n===============================&#39;,
 &#39;====\n\nThe following sections describe the steps needed to create a share and mount it on two VMs att&#39;,
 &#39;ached to a local network. \nNote that the user needs to configure the VMs in a way that allows loggin&#39;,
 &#39;g in via ssh. \n\nRequest to be enabled to the service\n------------------------------------\n\nThe user&#39;,
 &#39;willing to make use of the Manila service needs to send an email to superc@cineca.it, communicating&#39;,
 &quot;- how many shares are needed.\n- their dimensions (GB).\n- the tenant&#39;s name.\n\nOnce the tenant is en&quot;,
 &#39;abled to the service by the User Support Team, all users of the tenant will be able to use the servi&#39;,
 &#39;ce. \n\nCreate share network\n--------------------\n\nAs a first step, in the :ref:`cloud/os_overview/man&#39;,
 &#39;agement_tools/dashboard:horizon dashboard` you need to create the share network \nby clicking on *&quot;Cr&#39;,
 &#39;eate Share Network&quot;* in *&quot;Share â\x86\x92 Share Networks&quot;* and set the value for the following attributes:&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here, chunk size is an upper limit. This splitter attempts to create </span>
<span class="c1"># chunks close to this size, but will split earlier by following a hierarchy of separators.</span>
<span class="n">rcts</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">chunk_overlap</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">rcts</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;.. _shares_generic_create_card:&#39;,
 &#39;Create and use a GENERIC_TYPE share\n===================================&#39;,
 &#39;The following sections describe the steps needed to create a share and mount it on two VMs attached&#39;,
 &#39;to a local network.&#39;,
 &#39;Note that the user needs to configure the VMs in a way that allows logging in via ssh.&#39;,
 &#39;Request to be enabled to the service\n------------------------------------&#39;,
 &#39;The user willing to make use of the Manila service needs to send an email to superc@cineca.it,&#39;,
 &#39;communicating&#39;,
 &quot;- how many shares are needed.\n- their dimensions (GB).\n- the tenant&#39;s name.&quot;,
 &#39;Once the tenant is enabled to the service by the User Support Team, all users of the tenant will be&#39;]
</pre></div>
</div>
</div>
</div>
<p>Typically, we use token length rather than character length for chunking because when working with embedders, <strong>the maximum context length is defined in tokens, not characters</strong>.</p>
<p>We can count the number of tokens using the embedder’s tokenizer and then split the document with a strategy similar to the Recursive Character Text Splitter (RCTS), but based on the token count as the chunk size.</p>
<p>The embedder we are using for this tutorial has a maximum context length of 8k tokens, this will be our upper limit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedder</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">EMBEDDER</span><span class="p">)</span>
<span class="n">embedder</span><span class="o">.</span><span class="n">max_seq_length</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8192
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stts</span> <span class="o">=</span> <span class="n">SentenceTransformersTokenTextSplitter</span><span class="p">(</span><span class="n">tokens_per_chunk</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="n">EMBEDDER</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stts</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[CHUNK_CONTENT]:</span><span class="se">\n</span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="se">\n</span><span class="s2">===========&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;doc_name&#39;: &#39;generic_share_create.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}
[CHUNK_CONTENT]:
.. _shares_generic_create_card: Create and use a GENERIC_TYPE share =================================== The following sections describe the steps needed to create a share and mount it on two VMs attached to a local network. Note that the user needs to configure the VMs in a way that allows logging in via ssh.
===========
[CHUNK_CONTENT]:
Request to be enabled to the service ------------------------------------ The user willing to make use of the Manila service needs to send an email to superc@cineca.it, communicating - how many shares are needed. - their dimensions (GB). - the tenant&#39;s name. Once the tenant is enabled to the service by the User Support Team,
===========
</pre></div>
</div>
</div>
</div>
<p>The problem of the aformentioned method is that <code class="docutils literal notranslate"><span class="pre">SentenceTransformersTokenTextSplitter</span></code> does not support the use of specific separators. But we can create a recursive character text splitter using an huggingface tokenizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rcts_hftokenizer</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_huggingface_tokenizer</span><span class="p">(</span><span class="n">embedder</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">chunk_overlap</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">)</span>
<span class="n">rcts_hftokenizer</span><span class="o">.</span><span class="n">_separators</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>
<span class="c1"># Our embedder has a context length of 8k tokens, which is big... that&#39;s why you see only a single chunk here.</span>
<span class="c1"># What do you think it would happen if we use an embedder with a limited max sequence length?</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">rcts_hftokenizer</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[CHUNK_CONTENT]:</span><span class="se">\n</span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="se">\n</span><span class="s2">===========&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;doc_name&#39;: &#39;generic_share_create.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}
[CHUNK_CONTENT]:
.. _shares_generic_create_card:

Create and use a GENERIC_TYPE share
===================================

The following sections describe the steps needed to create a share and mount it on two VMs attached to a local network. 
Note that the user needs to configure the VMs in a way that allows logging in via ssh. 

Request to be enabled to the service
------------------------------------

The user willing to make use of the Manila service needs to send an email to superc@cineca.it, communicating 

- how many shares are needed.
- their dimensions (GB).
- the tenant&#39;s name.

Once the tenant is enabled to the service by the User Support Team, all users of the tenant will be able to use the service. 

Create share network
--------------------

As a first step, in the :ref:`cloud/os_overview/management_tools/dashboard:horizon dashboard` you need to create the share network 
by clicking on *&quot;Create Share Network&quot;* in *&quot;Share â Share Networks&quot;* and set the value for the following attributes:

- Share network name.
- network: choose the desired network, in our example example_share_guide_net.
- subnet: choose the desired subnet, in our example example_share_guide_subnet.
- Click on the *&quot;save&quot;* button.

.. image:: /cloud/_img/op_share_generic_img1.png

Create the share
----------------

Create the share by clicking on *&quot;Create Share&quot;* in *&quot;Share â Shares&quot;* and setting the following information:

- share name
- share protocol  == &quot;NFS&quot;
- size (on the right side is visualized information about the actual available and used space within the tenant)
- Type == &quot;generic_type&quot;
- Leave blank the option &quot;Make visible for all projects&quot; because it is not enabled 
- In the end, click on the *&quot;create&quot;* button.

.. image:: /cloud/_img/op_share_generic_img2.png


Set the access rule(s) on the share just created. 

- On the OpenStack dashboard click on *&quot;Share â Shares&quot;* 
- select the share just created
- in the menu on the right select *&quot;Manage Rules&quot;.*

.. image:: /cloud/_img/op_share_generic_img3.png

Click on *&quot;Add rule&quot;* and set:

- access type: Choose &quot;ip&quot;, the rest of options displayed are not available for NFS share&#39;s protocol.
- access level: read-write or read-only (depending on your needs)
- access to: write the IP with permission to access the share. Only one entry is allowed per rule, therefore, you will have to include a rule for the fixed-IP of each VM. 
- Finally, click on the &quot;add&quot; button.

.. image:: /cloud/_img/op_share_generic_img4.png

Mount the share on the VMs
--------------------------

You are now ready to mount the share on VMs. In the following example, we will consider two VM with Ubuntu 22.04 OS. **Please refer to the network guide of the operating system of your VM to be sure about the actions to be performed.**

- Login into the first VM.
- Upgrade the packages installed in the VM

.. code-block:: bash
    
    sudo apt update
    sudo apt upgrade

- Install the client. The package name is *&quot;nfs-common&quot;*.

.. code-block:: bash
    
    sudo apt install nfs-common

- Identify or create the directory in which the share will be mounted (e.g., &quot;/mnt/share_manila&quot;) 

.. code-block:: bash
   
   sudo mkdir &lt;MOUNT_PATH&gt;

- To mount the share you will need the share &lt;ACCESS_PATH&gt; displayed on the *&quot;Share Overview&quot;* page on OpenStack dashboard under the keyword *&quot;Export Location/Path&quot;*. Gather this information and proceed. 

.. image:: /cloud/_img/op_share_generic_img5.png

- Mount the share with the following command. Beware that different versions of nfs-common are available for different versions of Ubuntu and the syntax of the mount command could change.

.. code-block:: bash
   
   sudo mount -t nfs -v &lt;ACCESS_PATH&gt; &lt;MOUNT_PATH&gt;

- Then, repeat the same steps for the second VM.
===========
</pre></div>
</div>
</div>
</div>
<p>Other chunking options we will not explore today:</p>
<ul class="simple">
<li><p>Semantic chunking</p></li>
<li><p>LLM based chunking</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="finding-the-right-chunk-size">
<h1>Finding the right chunk size<a class="headerlink" href="#finding-the-right-chunk-size" title="Link to this heading"></a></h1>
<p>It is true that the maximum sequence length of our embedder imposes an upper limit on the size of the embeddings. However, this does not imply that this upper limit corresponds to the optimal chunk size. <strong>Larger chunks often encompass multiple topics, which can result in embeddings that are less focused</strong> and “dilute” the thematic coherence.</p>
<p>How can we determine the optimal chunk size?</p>
<p>If a labeled dataset of questions and answers is available, various evaluation metrics can be used to measure how the relevance of retrieved resources changes with different chunk sizes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="k">class</span><span class="w"> </span><span class="nc">QA</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">document_name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">question</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">answer</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">start_index</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">end_index</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">chunk_size_tok</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span><span class="nb">float</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">document_name</span> <span class="o">=</span> <span class="n">document_name</span> <span class="c1"># The document name containing the answ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">question</span> <span class="o">=</span> <span class="n">question</span> <span class="c1"># The question created by an llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">answer</span> <span class="o">=</span> <span class="n">answer</span> <span class="c1"># The answer </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_index</span> <span class="o">=</span> <span class="n">start_index</span> <span class="c1"># Character index where the question starts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_index</span> <span class="o">=</span> <span class="n">end_index</span> <span class="c1"># Character index where the question ends</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size_tok</span> <span class="o">=</span> <span class="n">chunk_size_tok</span> <span class="c1"># Total token size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span> <span class="c1"># Seed used to create the answ</span>

<span class="c1"># Let&#39;s generate a set of syntetic questions and answers</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_qa_pairs</span><span class="p">(</span><span class="n">document</span><span class="p">:</span><span class="n">Document</span><span class="p">,</span> <span class="n">embedder</span><span class="p">:</span><span class="n">SentenceTransformer</span><span class="p">,</span> 
                      <span class="n">llm</span><span class="p">:</span><span class="n">ChatOpenAI</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span><span class="nb">float</span><span class="o">|</span><span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QA</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts a list of questions and answers from the given document.</span>

<span class="sd">    Args:</span>
<span class="sd">        document (langchain.Document): The document from which to extract an answer chunk for question generation.</span>
<span class="sd">        embedder: The embedder to be used in the process.</span>
<span class="sd">        llm (langchain.LLM): An instance of a Langchain LLM used for generating questions.</span>
<span class="sd">        seed (float, optional): A seed value to ensure reproducibility. If None (default), the current Unix timestamp will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        QA Object: An object containing the question-answer data with the following fields:</span>
<span class="sd">            - `document_name`: The name of the source document.</span>
<span class="sd">            - `question`: The generated question corresponding to the extracted chunk.</span>
<span class="sd">            - `answer`: The answer text extracted from the document.</span>
<span class="sd">            - `start_index`: The starting index of the answer within the document.</span>
<span class="sd">            - `end_index`: The ending index of the answer within the document.</span>
<span class="sd">            - `chunk_size_tok`: The size of the chunk in tokens.</span>
<span class="sd">            - `seed`: The seed value used to select the answer for question generation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Set a seed for replicability</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">seed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        
        <span class="c1"># Here we don&#39;t use always the same sequence length, otherwise we are</span>
        <span class="c1"># influencing our average chunks length to be always that long</span>
        <span class="n">rand_seq_len</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>

        <span class="n">chunker</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_huggingface_tokenizer</span><span class="p">(</span><span class="n">embedder</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> 
                                                                            <span class="n">chunk_overlap</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
                                                                            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">rand_seq_len</span><span class="p">,</span>
                                                                            <span class="n">add_start_index</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">chunker</span><span class="o">.</span><span class="n">_separators</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;..&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">]</span>

        <span class="c1"># Extract the answer to be used for question generation.</span>
        <span class="n">document_chunks</span> <span class="o">=</span> <span class="n">chunker</span><span class="o">.</span><span class="n">split_documents</span><span class="p">([</span><span class="n">document</span><span class="p">])</span>
        <span class="c1"># Add end index to the document metadata</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">document_chunks</span><span class="p">)):</span>
            <span class="n">document_chunks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;end_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">document_chunks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;start_index&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">document_chunks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> 
            
        <span class="c1"># Select a random chunk so that we don&#39;t always generate the same</span>
        <span class="c1"># set of Q&amp;A pairs</span>
        <span class="n">answer</span> <span class="o">=</span> <span class="n">document_chunks</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">document_chunks</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>

        <span class="c1"># Use an llm to generate a question about this piece of text</span>
        <span class="k">class</span><span class="w"> </span><span class="nc">AnswerFormat</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
            <span class="n">question</span><span class="p">:</span><span class="nb">str</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">PROMPT_PATH</span><span class="p">,</span> <span class="s2">&quot;qa_syntetic_testset_prompt.txt&quot;</span><span class="p">),</span> <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="c1"># Read the question prompt and append the content of the document to the end</span>
            <span class="n">question_template</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">answer</span><span class="o">.</span><span class="n">page_content</span>
        
        <span class="n">item</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">AnswerFormat</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question_template</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">QA</span><span class="p">(</span><span class="n">answer</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">],</span> <span class="n">item</span><span class="o">.</span><span class="n">question</span><span class="p">,</span> 
                  <span class="n">answer</span><span class="o">.</span><span class="n">page_content</span><span class="p">,</span> <span class="n">answer</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;start_index&quot;</span><span class="p">],</span> <span class="n">answer</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;end_index&quot;</span><span class="p">],</span> 
                  <span class="nb">len</span><span class="p">(</span><span class="n">embedder</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">answer</span><span class="o">.</span><span class="n">page_content</span><span class="p">)),</span> <span class="n">seed</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encountered an exception while processing doc </span><span class="si">{</span><span class="n">document</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s1">&#39;doc_name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">QA</span><span class="p">(</span><span class="n">document</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">],</span> <span class="s2">&quot;Error while parsing doc&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

<span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">e</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">generate_qa_pairs</span><span class="p">,</span> <span class="n">embedder</span> <span class="o">=</span> <span class="n">embedder</span><span class="p">,</span> <span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span><span class="p">),</span> <span class="n">documents</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1.73 s, sys: 22 ms, total: 1.75 s
Wall time: 9.15 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qa_set</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;doc&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">qa</span><span class="o">.</span><span class="n">document_name</span> <span class="k">for</span> <span class="n">qa</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="s2">&quot;seed&quot;</span><span class="p">:[</span><span class="n">qa</span><span class="o">.</span><span class="n">seed</span> <span class="k">for</span> <span class="n">qa</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span>
                       <span class="s2">&quot;chunk_size_tok&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">qa</span><span class="o">.</span><span class="n">chunk_size_tok</span> <span class="k">for</span> <span class="n">qa</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span>  
                       <span class="s2">&quot;start_index&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">qa</span><span class="o">.</span><span class="n">start_index</span> <span class="k">for</span> <span class="n">qa</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="s2">&quot;end_index&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">qa</span><span class="o">.</span><span class="n">end_index</span> <span class="k">for</span> <span class="n">qa</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span>
                       <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">qa</span><span class="o">.</span><span class="n">question</span> <span class="k">for</span> <span class="n">qa</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">qa</span><span class="o">.</span><span class="n">answer</span> <span class="k">for</span> <span class="n">qa</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]})</span>
<span class="n">display</span><span class="p">(</span><span class="n">qa_set</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Save to csv for further inspection</span>
<span class="n">qa_set</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;qa_questions_</span><span class="si">{</span><span class="n">date</span><span class="si">}</span><span class="s2">.csv&quot;</span><span class="p">),</span> <span class="n">index</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>doc</th>
      <th>seed</th>
      <th>chunk_size_tok</th>
      <th>start_index</th>
      <th>end_index</th>
      <th>question</th>
      <th>answer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>generic_share_create.rst.txt</td>
      <td>42</td>
      <td>644</td>
      <td>0</td>
      <td>2429</td>
      <td>What specific details must a user include in t...</td>
      <td>.. _shares_generic_create_card:\n\nCreate and ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>database.rst.txt</td>
      <td>42</td>
      <td>220</td>
      <td>0</td>
      <td>1019</td>
      <td>How does the Trove component of OpenStack faci...</td>
      <td>.. _database_card:\n\nDatabase\n========\n\n`T...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>hpc_software.rst.txt</td>
      <td>42</td>
      <td>734</td>
      <td>0</td>
      <td>3131</td>
      <td>What steps must a user take to gain access to ...</td>
      <td>Software\n========\n\n| On CINECA clusters, se...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>gaia.rst.txt</td>
      <td>42</td>
      <td>471</td>
      <td>0</td>
      <td>1611</td>
      <td>What are the specific models of Nvidia GPUs th...</td>
      <td>.. _gaia_card:\n\nGAIA\n====\n\n**PAGE UNDER C...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>miniconda.rst.txt</td>
      <td>42</td>
      <td>737</td>
      <td>0</td>
      <td>3062</td>
      <td>What are the recommended steps to clean up pre...</td>
      <td>.. _miniconda_card:\n\nMiniconda \n=========\n...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: Questions and answers may vary due to multithreading even if we set a seed, that&#39;s because multithreading may change the order of execution.</span>
<span class="c1"># So we load this Q&amp;A set to ensure replicability of the next steps</span>
<span class="n">qa_set</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/output/chunking/qa_questions_2025_07_23_14_54_35.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="mean-reciprocal-rank">
<h2>Mean Reciprocal Rank<a class="headerlink" href="#mean-reciprocal-rank" title="Link to this heading"></a></h2>
<p><span class="math notranslate nohighlight">\( MeanReciprocalRank = \frac{1}{|Q|} \sum_{i=1}^{|Q|}\frac{1}{rank_i} \)</span></p>
<p>Given a set of questions <span class="math notranslate nohighlight">\( Q \)</span>, we rank all available chunks by their similarity to each question and examine the position of the relevant chunk within this ranking. The average of the reciprocal ranks across all questions is known as the <span class="math notranslate nohighlight">\( Mean Reciprocal Rank \)</span> (MRR).</p>
<p>To identify the optimal chunk size, the following procedure can be employed:</p>
<ol class="arabic simple">
<li><p>Define a test set with questions and their related answers (we can do it manually or… with an LLM). We already done this step;</p></li>
<li><p>Chunk data testing various configurations of chunk sizes (e.g. 100 tokens, 200 tokens, etc..);</p></li>
<li><p>For each chunk config, calculate the mean reciprocal rank;</p></li>
<li><p>Choose the chunking configuration which maximizes the MRR (the MRR is constrained between 0 and 1; so, the higher the better) and minimizes the chunk size.</p></li>
</ol>
</section>
<section id="test-various-chunking-config">
<h2>Test various chunking config<a class="headerlink" href="#test-various-chunking-config" title="Link to this heading"></a></h2>
<p>We experiment with various chunking setups as described earlier. We start with an initial chunk size of 100 tokens and increase it by 100 tokens each time.</p>
<p>Although our embedder supports a context window of around 8000 tokens, we limit the maximum chunk size to 1500 tokens. This is because beyond 1500 tokens, the text chunks become quite long, and we want to avoid making them overly broad in terms of topics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">find_optimal_chunk_size</span><span class="p">(</span><span class="n">initial_chunk_size</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">step_size</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">max_chunk_size</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> 
                            <span class="n">documents</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">],</span> <span class="n">qa_set</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="c1"># Save the mean reciprocal rank for each config tested</span>
    <span class="n">tested_chunk_size</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">mean_reciprocal_rank</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">chunk_size</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">initial_chunk_size</span><span class="p">,</span> <span class="n">max_chunk_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step_size</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="si">}</span><span class="s2"> - Testing chunk size of: </span><span class="si">{</span><span class="n">chunk_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">rcts_chunker</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_huggingface_tokenizer</span><span class="p">(</span><span class="n">embedder</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> 
                                                                                <span class="n">chunk_overlap</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
                                                                                <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">chunk_size</span><span class="p">,</span>
                                                                                <span class="n">add_start_index</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="c1"># The . was removed as in rst documents it has a very specific meaning </span>
        <span class="c1"># and this would cause document &quot;oversplitting&quot;</span>
        <span class="n">rcts_chunker</span><span class="o">.</span><span class="n">_separators</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;..&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">]</span>

        <span class="c1"># The split_documents method creates one document for each chunk. Each document has the same metadata </span>
        <span class="c1"># as the original document. The list is &quot;flat&quot;, we don&#39;t have nested lists</span>
        <span class="n">splitted_docs</span> <span class="o">=</span> <span class="n">rcts_chunker</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        <span class="c1"># Add end position for each chunk</span>
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">splitted_docs</span><span class="p">:</span>
            <span class="n">chunk</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;end_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;start_index&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> 

        <span class="c1"># Calculate the embedding for each document chunk</span>
        <span class="n">documents_embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">([</span><span class="n">chunk</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">splitted_docs</span><span class="p">])</span>
        <span class="n">embedded_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;content&quot;</span><span class="p">:[</span><span class="n">chunk</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">splitted_docs</span><span class="p">],</span>
                                    <span class="s2">&quot;metadata&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">chunk</span><span class="o">.</span><span class="n">metadata</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">splitted_docs</span><span class="p">],</span>
                                    <span class="s2">&quot;embeddings&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">embeddings</span> <span class="k">for</span> <span class="n">embeddings</span> <span class="ow">in</span> <span class="n">documents_embeddings</span><span class="p">]})</span>

        <span class="c1"># Unpack metadata columns</span>
        <span class="n">embedded_data</span><span class="p">[[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">embedded_data</span><span class="p">[</span><span class="s2">&quot;metadata&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()]]</span> <span class="o">=</span> <span class="n">embedded_data</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;metadata&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">result_type</span><span class="o">=</span><span class="s2">&quot;expand&quot;</span><span class="p">)</span>
        <span class="n">embedded_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;metadata&quot;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Embed each question and calc similarity with respect to the chunks</span>
        <span class="c1"># created with the chunk size configuration we are testing</span>
        <span class="n">embedded_questions</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">qa_set</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">])</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedded_questions</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">embedded_data</span><span class="p">[</span><span class="s2">&quot;embeddings&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()))</span>

        <span class="c1"># Create a table with question_id, chunk_id, similarity score</span>
        <span class="n">similarity_abt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">similarity</span><span class="p">)</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
        <span class="n">similarity_abt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;question_id&quot;</span><span class="p">,</span> <span class="s2">&quot;chunk_id&quot;</span><span class="p">,</span> <span class="s2">&quot;similarity&quot;</span><span class="p">]</span>

        <span class="c1"># Add chunk informations</span>
        <span class="n">similarity_abt</span> <span class="o">=</span> <span class="n">similarity_abt</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">embedded_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(),</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;inner&quot;</span><span class="p">,</span> <span class="n">left_on</span> <span class="o">=</span> <span class="s2">&quot;chunk_id&quot;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span> <span class="s2">&quot;index&quot;</span><span class="p">)</span>
        <span class="n">similarity_abt</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="s2">&quot;embeddings&quot;</span><span class="p">,</span> <span class="s2">&quot;scraped_on&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">similarity_abt</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span><span class="s2">&quot;start_index&quot;</span><span class="p">:</span> <span class="s2">&quot;chunk_start_index&quot;</span><span class="p">,</span> <span class="s2">&quot;end_index&quot;</span><span class="p">:</span><span class="s2">&quot;chunk_end_index&quot;</span><span class="p">,</span> <span class="s2">&quot;doc_name&quot;</span><span class="p">:</span><span class="s2">&quot;chunk_doc_provenance&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;chunk_content&quot;</span><span class="p">},</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Join qa set information</span>
        <span class="n">similarity_abt</span> <span class="o">=</span> <span class="n">similarity_abt</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">qa_set</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(),</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;inner&quot;</span><span class="p">,</span> <span class="n">left_on</span> <span class="o">=</span> <span class="s2">&quot;question_id&quot;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span> <span class="s2">&quot;index&quot;</span><span class="p">)</span>
        <span class="n">similarity_abt</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="s2">&quot;chunk_size_tok&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">similarity_abt</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span><span class="s2">&quot;start_index&quot;</span><span class="p">:</span> <span class="s2">&quot;answer_start_index&quot;</span><span class="p">,</span> <span class="s2">&quot;end_index&quot;</span><span class="p">:</span><span class="s2">&quot;answer_end_index&quot;</span><span class="p">,</span> <span class="s2">&quot;doc&quot;</span><span class="p">:</span><span class="s2">&quot;qa_doc_provenance&quot;</span><span class="p">},</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>    
        <span class="c1">#display(similarity_abt[[&quot;question_id&quot;, &quot;chunk_id&quot;, &quot;similarity&quot;, &quot;chunk_doc_provenance&quot;, &quot;chunk_start_index&quot;, &quot;chunk_end_index&quot;, </span>
        <span class="c1">#                        &quot;qa_doc_provenance&quot;, &quot;chunk_start_index&quot;,&quot;chunk_end_index&quot;, &quot;question&quot;, &quot;answer&quot;, &quot;chunk_content&quot;]])</span>
        <span class="c1">#break</span>
        
        <span class="k">def</span><span class="w"> </span><span class="nf">calc_rank</span><span class="p">(</span><span class="n">df</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="c1"># Sort all the values and use the index as rank</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;similarity&quot;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Keep only rows where qa question matches chunk document provenance </span>
            <span class="c1"># and where chunk contains a piece of the answer</span>
            <span class="n">relevant_docs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;chunk_doc_provenance&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;qa_doc_provenance&quot;</span><span class="p">]</span>
            <span class="n">chunk_contains_beginning_answ</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;answer_start_index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;chunk_start_index&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;chunk_end_index&quot;</span><span class="p">])</span>
            <span class="n">chunk_contains_end_answ</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;answer_end_index&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;chunk_start_index&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;chunk_end_index&quot;</span><span class="p">])</span>
            <span class="n">chunk_contains_mid_answ</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;chunk_start_index&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;answer_start_index&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;chunk_end_index&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;answer_end_index&quot;</span><span class="p">])</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">relevant_docs</span> <span class="o">&amp;</span> <span class="p">(</span> <span class="n">chunk_contains_beginning_answ</span> <span class="o">|</span> <span class="n">chunk_contains_end_answ</span> <span class="o">|</span> <span class="n">chunk_contains_mid_answ</span><span class="p">)]</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">names</span> <span class="o">=</span> <span class="s2">&quot;rank&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            
        <span class="c1"># For each question of the qa, calc the (avg) rank</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">similarity_abt</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;question_id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">X</span><span class="p">:</span> <span class="n">calc_rank</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">include_groups</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">reciprocal_rank</span> <span class="o">=</span> <span class="n">rank</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># Rank starts from zero, so we remap from 1 adding a + 1</span>
        <span class="n">mrr</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">rank</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span><span class="n">reciprocal_rank</span><span class="p">)</span>
        
        <span class="c1"># Save results for plotting</span>
        <span class="n">mean_reciprocal_rank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mrr</span><span class="p">)</span>
        <span class="n">tested_chunk_size</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">)</span>
    
    <span class="c1"># Check mrr trend and plot against the chunk size</span>
    <span class="n">mrr_tests_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;chunk_size&quot;</span><span class="p">:</span> <span class="n">tested_chunk_size</span><span class="p">,</span> <span class="s2">&quot;mrr&quot;</span><span class="p">:</span> <span class="n">mean_reciprocal_rank</span><span class="p">})</span>
    <span class="n">mrr_tests_data</span><span class="p">[</span><span class="s2">&quot;improvement&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mrr_tests_data</span><span class="p">[</span><span class="s2">&quot;mrr&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mrr_tests_data</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">mrr_test_data</span> <span class="o">=</span> <span class="n">find_optimal_chunk_size</span><span class="p">(</span><span class="n">initial_chunk_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">step_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">max_chunk_size</span> <span class="o">=</span> <span class="mi">1500</span><span class="p">,</span> <span class="n">documents</span> <span class="o">=</span> <span class="n">documents</span><span class="p">,</span> <span class="n">qa_set</span> <span class="o">=</span> <span class="n">qa_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-07-28 16:40:24.248507 - Testing chunk size of: 100
2025-07-28 16:40:32.288890 - Testing chunk size of: 200
2025-07-28 16:40:38.834541 - Testing chunk size of: 300
2025-07-28 16:40:45.030032 - Testing chunk size of: 400
2025-07-28 16:40:51.192913 - Testing chunk size of: 500
2025-07-28 16:40:57.272516 - Testing chunk size of: 600
2025-07-28 16:41:03.475425 - Testing chunk size of: 700
2025-07-28 16:41:09.648438 - Testing chunk size of: 800
2025-07-28 16:41:15.897295 - Testing chunk size of: 900
2025-07-28 16:41:22.109958 - Testing chunk size of: 1000
2025-07-28 16:41:28.252650 - Testing chunk size of: 1100
2025-07-28 16:41:34.423317 - Testing chunk size of: 1200
2025-07-28 16:41:40.952498 - Testing chunk size of: 1300
2025-07-28 16:41:47.153977 - Testing chunk size of: 1400
2025-07-28 16:41:53.681691 - Testing chunk size of: 1500
CPU times: user 1min 50s, sys: 174 ms, total: 1min 50s
Wall time: 1min 36s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">mrr_test_data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">mrr_test_data</span><span class="p">[</span><span class="s2">&quot;chunk_size&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Max tested chunk size&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;0-128 - Tiny chunks&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;128-256 - Small chunks&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="s2">&quot;orangered&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;256-512 - Medium chunks&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;512-1024 - Large chunks&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mrr_test_data</span><span class="p">[</span><span class="s2">&quot;chunk_size&quot;</span><span class="p">],</span> <span class="n">mrr_test_data</span><span class="p">[</span><span class="s2">&quot;mrr&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;MRR&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MRR&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;chunk_size&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean reciprocal rank tests - step size </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">mrr_test_data</span><span class="p">[</span><span class="s1">&#39;chunk_size&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s2"> tok&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chunk_size</th>
      <th>mrr</th>
      <th>improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>100</td>
      <td>0.058761</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>200</td>
      <td>0.204699</td>
      <td>0.145937</td>
    </tr>
    <tr>
      <th>2</th>
      <td>300</td>
      <td>0.366927</td>
      <td>0.162228</td>
    </tr>
    <tr>
      <th>3</th>
      <td>400</td>
      <td>0.482120</td>
      <td>0.115193</td>
    </tr>
    <tr>
      <th>4</th>
      <td>500</td>
      <td>0.602926</td>
      <td>0.120806</td>
    </tr>
    <tr>
      <th>5</th>
      <td>600</td>
      <td>0.670944</td>
      <td>0.068018</td>
    </tr>
    <tr>
      <th>6</th>
      <td>700</td>
      <td>0.780495</td>
      <td>0.109551</td>
    </tr>
    <tr>
      <th>7</th>
      <td>800</td>
      <td>0.929104</td>
      <td>0.148610</td>
    </tr>
    <tr>
      <th>8</th>
      <td>900</td>
      <td>0.935323</td>
      <td>0.006219</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1000</td>
      <td>0.907046</td>
      <td>-0.028278</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1100</td>
      <td>0.884655</td>
      <td>-0.022391</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1200</td>
      <td>0.936816</td>
      <td>0.052161</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1300</td>
      <td>0.933547</td>
      <td>-0.003269</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1400</td>
      <td>0.925821</td>
      <td>-0.007726</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1500</td>
      <td>0.944527</td>
      <td>0.018706</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../../../_images/0ca7decfcca5a7a2148a24472340e5eae8cf48fcbf1c0ed3cc858aab933fb474.png" src="../../../../_images/0ca7decfcca5a7a2148a24472340e5eae8cf48fcbf1c0ed3cc858aab933fb474.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="chunk-and-index-all-documents">
<h1>Chunk and index all documents<a class="headerlink" href="#chunk-and-index-all-documents" title="Link to this heading"></a></h1>
<p>Once we found a reasonable chunk size, we are ready to split all our docs with the chosen lenght. Then we need to save our collection of splitted docs in a <a class="reference external" href="https://weaviate.io/blog/vector-library-vs-vector-database">vector database</a>.</p>
<section id="vector-databases">
<h2>Vector databases<a class="headerlink" href="#vector-databases" title="Link to this heading"></a></h2>
<p>A vector database is a specialized type of database designed to efficiently store and search data represented as high-dimensional vectors. These databases are optimized to perform similarity searches efficiently. Given a query vector, the database quickly finds vectors that are closest to it in terms of distance metrics like cosine similarity, Euclidean distance, etc.</p>
<p>The choice of a vector database depends on the needs of the project, for a comparison see: <a class="reference external" href="https://www.datacamp.com/blog/the-top-5-vector-databases#5-best-vector-databases-in-2025-theli">1</a>, <a class="reference external" href="https://benchmark.vectorview.ai/vectordbs.html">2</a>, <a class="reference external" href="https://medium.com/the-ai-forum/which-vector-database-should-you-use-choosing-the-best-one-for-your-needs-5108ec7ba133">3</a>. More “traditional” dbs have added support for vectors (e.g. Postgres, DuckDB).</p>
<p>For this tutorial we are going to use ChromaDB.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate our splitter with the chunk size we identified during the previous step</span>
<span class="n">rcts_chunker</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_huggingface_tokenizer</span><span class="p">(</span><span class="n">embedder</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> 
                                                                         <span class="n">chunk_overlap</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
                                                                         <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span>
                                                                         <span class="n">add_start_index</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">rcts_chunker</span><span class="o">.</span><span class="n">_separators</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;..&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">]</span>

<span class="n">chunks</span> <span class="o">=</span> <span class="n">rcts_chunker</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">chunks</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Document(metadata={&#39;doc_name&#39;: &#39;generic_share_create.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;start_index&#39;: 0}, page_content=&#39;.. _shares_generic_create_card:\n\nCreate and use a GENERIC_TYPE share\n===================================\n\nThe following sections describe the steps needed to create a share and mount it on two VMs attached to a local network. \nNote that the user needs to configure the VMs in a way that allows logging in via ssh. \n\nRequest to be enabled to the service\n------------------------------------\n\nThe user willing to make use of the Manila service needs to send an email to superc@cineca.it, communicating \n\n- how many shares are needed.\n- their dimensions (GB).\n- the tenant\&#39;s name.\n\nOnce the tenant is enabled to the service by the User Support Team, all users of the tenant will be able to use the service. \n\nCreate share network\n--------------------\n\nAs a first step, in the :ref:`cloud/os_overview/management_tools/dashboard:horizon dashboard` you need to create the share network \nby clicking on *&quot;Create Share Network&quot;* in *&quot;Share â\x86\x92 Share Networks&quot;* and set the value for the following attributes:\n\n- Share network name.\n- network: choose the desired network, in our example example_share_guide_net.\n- subnet: choose the desired subnet, in our example example_share_guide_subnet.\n- Click on the *&quot;save&quot;* button.\n\n.. image:: /cloud/_img/op_share_generic_img1.png\n\nCreate the share\n----------------\n\nCreate the share by clicking on *&quot;Create Share&quot;* in *&quot;Share â\x86\x92 Shares&quot;* and setting the following information:\n\n- share name\n- share protocol  == &quot;NFS&quot;\n- size (on the right side is visualized information about the actual available and used space within the tenant)\n- Type == &quot;generic_type&quot;\n- Leave blank the option &quot;Make visible for all projects&quot; because it is not enabled \n- In the end, click on the *&quot;create&quot;* button.\n\n.. image:: /cloud/_img/op_share_generic_img2.png\n\n\nSet the access rule(s) on the share just created. \n\n- On the OpenStack dashboard click on *&quot;Share â\x86\x92 Shares&quot;* \n- select the share just created\n- in the menu on the right select *&quot;Manage Rules&quot;.*\n\n.. image:: /cloud/_img/op_share_generic_img3.png\n\nClick on *&quot;Add rule&quot;* and set:\n\n- access type: Choose &quot;ip&quot;, the rest of options displayed are not available for NFS share\&#39;s protocol.\n- access level: read-write or read-only (depending on your needs)\n- access to: write the IP with permission to access the share. Only one entry is allowed per rule, therefore, you will have to include a rule for the fixed-IP of each VM. \n- Finally, click on the &quot;add&quot; button.\n\n.. image:: /cloud/_img/op_share_generic_img4.png\n\nMount the share on the VMs\n--------------------------\n\nYou are now ready to mount the share on VMs. In the following example, we will consider two VM with Ubuntu 22.04 OS. **Please refer to the network guide of the operating system of your VM to be sure about the actions to be performed.**\n\n- Login into the first VM.\n- Upgrade the packages installed in the VM\n\n.. code-block:: bash\n    \n    sudo apt update\n    sudo apt upgrade\n\n- Install the client. The package name is *&quot;nfs-common&quot;*.&#39;),
 Document(metadata={&#39;doc_name&#39;: &#39;generic_share_create.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;start_index&#39;: 2972}, page_content=&#39;.. code-block:: bash\n    \n    sudo apt install nfs-common\n\n- Identify or create the directory in which the share will be mounted (e.g., &quot;/mnt/share_manila&quot;) \n\n.. code-block:: bash\n   \n   sudo mkdir &lt;MOUNT_PATH&gt;\n\n- To mount the share you will need the share &lt;ACCESS_PATH&gt; displayed on the *&quot;Share Overview&quot;* page on OpenStack dashboard under the keyword *&quot;Export Location/Path&quot;*. Gather this information and proceed. \n\n.. image:: /cloud/_img/op_share_generic_img5.png\n\n- Mount the share with the following command. Beware that different versions of nfs-common are available for different versions of Ubuntu and the syntax of the mount command could change.\n\n.. code-block:: bash\n   \n   sudo mount -t nfs -v &lt;ACCESS_PATH&gt; &lt;MOUNT_PATH&gt;\n\n- Then, repeat the same steps for the second VM.&#39;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s check the chunk distribution</span>
<span class="n">chunk_count</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">chunk</span><span class="o">.</span><span class="n">metadata</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">])</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;doc_name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">chunk_count</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;chunk_count&quot;</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">chunk_count</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;N-Chunks per document distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;N-Chunks&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Freq&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Freq&#39;)
</pre></div>
</div>
<img alt="../../../../_images/e979f756386c59474017930695b9f689d9fcf3d8e73d63b2016c3ec529972482.png" src="../../../../_images/e979f756386c59474017930695b9f689d9fcf3d8e73d63b2016c3ec529972482.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Util function to create embeddings and add them to a chromadb collection</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_vector_store</span><span class="p">(</span><span class="n">documents</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">],</span> <span class="n">embedder</span><span class="p">,</span> 
                        <span class="n">vector_store_name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">writing_path</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">is_incremental</span><span class="p">:</span><span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">])</span>

    <span class="n">chroma_client</span> <span class="o">=</span> <span class="n">chromadb</span><span class="o">.</span><span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="n">writing_path</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_incremental</span><span class="p">:</span>
        <span class="c1"># Drop collection if exists, we want to start from a fresh state</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">chroma_client</span><span class="o">.</span><span class="n">delete_collection</span><span class="p">(</span><span class="n">vector_store_name</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">NotFoundError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">create_collection</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="n">vector_store_name</span><span class="p">,</span> <span class="n">get_or_create</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Add all docs to the collection</span>
    <span class="n">collection</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">],</span>
                <span class="n">metadatas</span>  <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">],</span>
                <span class="n">ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;__&quot;</span> <span class="o">+</span> \
                        <span class="nb">str</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;start_index&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">],</span>
                <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize our vector store</span>
<span class="n">chroma_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="p">,</span> <span class="s2">&quot;chroma&quot;</span><span class="p">)</span>
<span class="n">Path</span><span class="p">(</span><span class="n">chroma_path</span><span class="p">)</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">create_vector_store</span><span class="p">(</span><span class="n">documents</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">,</span> <span class="n">embedder</span> <span class="o">=</span> <span class="n">embedder</span><span class="p">,</span> 
                    <span class="n">vector_store_name</span><span class="o">=</span><span class="s2">&quot;hpc_wiki&quot;</span><span class="p">,</span> <span class="n">writing_path</span> <span class="o">=</span> <span class="n">chroma_path</span><span class="p">,</span> <span class="n">is_incremental</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a client for the db and check the top 3 retrieved docs for a question</span>
<span class="n">lc_embedder</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">EMBEDDER</span><span class="p">)</span>
<span class="n">hpc_store</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;hpc_wiki&quot;</span><span class="p">,</span> <span class="n">embedding_function</span> <span class="o">=</span> <span class="n">lc_embedder</span><span class="p">,</span> 
                   <span class="n">persist_directory</span><span class="o">=</span> <span class="n">chroma_path</span><span class="p">)</span>
<span class="n">hpc_store</span><span class="o">.</span><span class="n">similarity_search_with_score</span><span class="p">(</span><span class="s2">&quot;What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(Document(id=&#39;leonardo.rst.txt__0&#39;, metadata={&#39;start_index&#39;: 0, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;leonardo.rst.txt&#39;}, page_content=&#39;.. _leonardo_card:\n\nLeonardo\n========\n\nLeonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.\nThis guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.\n\n.. |ico2| image:: img/leonardo_logo.png\n   :height: 55px\n   :class: no-scaled-link\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. \n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:\n\n * login01-ext.leonardo.cineca.it\n * login02-ext.leonardo.cineca.it\n * login05-ext.leonardo.cineca.it\n * login07-ext.leonardo.cineca.it\n\n.. warning::\n    \n    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\nSystem Architecture\n-------------------\n\nThe cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:\n\n* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.\n* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.\n\nThe overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. \n\nThe **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.\nThe **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.\n\nHardware Details\n^^^^^^^^^^^^^^^^\n\n.. tab-set::\n\n    .. tab-item:: Booster&#39;),
  0.7387087941169739),
 (Document(id=&#39;specific_users.rst.txt__2544&#39;, metadata={&#39;doc_name&#39;: &#39;specific_users.rst.txt&#39;, &#39;start_index&#39;: 2544, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&#39;../files/2FA_EF.pdf&gt;`\n\n\nSLURM Partitions\n----------------\n\nOn :ref:`hpc/leonardo:Leonardo` and :ref:`hpc/pitagora:Pitagora` *Job Managing and SLURM Partitions* sections, you can find the description of SLURM partitions and QOS to submit your jobs. Notice that EUROfusion users have dedicated partitions and QOS: you are allowed to use the **&quot;_fua_&quot;** partitions and the related QOS, besides the **&quot;_all_serial&quot;** partition which is shared among all users.\n\n\nLow-priority jobs\n^^^^^^^^^^^^^^^^^\n\n1) **If all the budget assigned to your Project Account has been consumed**, you can keep running on Leonardo boost_fua_prod and dcgp_fua_prod partitions at low priority by requesting in your submission script the **qos_fualowprio** QOS:\n\n.. code-block:: bash\n\n  #SBATCH --account=&lt;YOUR Project Account&gt;\n  #SBATCH --qos=qos_fualowprio\n\nThe QOS is *automatically* added to your Project Account upon budget exhaustion.\n\n2) You can also request to run low priority jobs, **without having consumed all the budget of yout active Project Account**, by association to the **FUAL8_LOWPRIO** account on Booster and **FUA38_LOWPRIO_0** account on DCGP (write a mail to superc@cineca.it). You always need to specify also the **qos_fualowprio** QOS in your submission script.\n\n.. code-block:: bash\n  \n   #SBATCH --account=&lt;LOWPRIO Project Account&gt;\n   #SBATCH --qos=qos_fualowprio&#39;),
  0.7474587559700012),
 (Document(id=&#39;matlab.rst.txt__7416&#39;, metadata={&#39;start_index&#39;: 7416, &#39;doc_name&#39;: &#39;matlab.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&quot;.. code-block:: matlabsession\n        \n        &gt;&gt; % Specify QoS\n        &gt;&gt; c.AdditionalProperties.QoS = &#39;name-of-qos&#39;;\n\n        &gt;&gt; % Specify processor cores per node.  Default is 32 for Leonardo GPU nodes and 112 on Leonardo CPU nodes; 18 for Marconi and 48 for Galileo100.\n        &gt;&gt; c.AdditionalProperties.ProcsPerNode = 18;\n\n        &gt;&gt; % specify the number of GPUsPerNode. Valid only on Leonardo GPU partition\n        &gt;&gt; c.AdditionalProperties.GPUsPerNode = 1;\n\n        &gt;&gt; % Specify memory to use for MATLAB jobs, per core (default: 4gb)\n        &gt;&gt; c.AdditionalProperties.MemUsage = &#39;6gb&#39;;\n\n        &gt;&gt; % Require node exclusivity\n        &gt;&gt; c.AdditionalProperties.RequireExclusiveNode = true;\n\n        &gt;&gt; % Request to use a reservation\n        &gt;&gt; c.AdditionalProperties.Reservation = &#39;name-of-reservation&#39;;\n\n        &gt;&gt; % Specify e-mail address to receive notifications about your job\n        &gt;&gt; c.AdditionalProperties.EmailAddress = â\x80\x98test@foo.comâ\x80\x99;\n\n        &gt;&gt; % Turn onthe Debug Message.  Default is off (logical boolean true/false).\n        &gt;&gt; c.AdditionalProperties.DebugMessagesTurnedOn = true;\n\n\nTo check for the values of the current configuration options, call the AdditionalProperties without semicolon\n\n.. code-block:: matlabsession\n\n        &gt;&gt; % To view current configurations\n        &gt;&gt; c.AdditionalProperties\n\nTo clear a value, assign the property an empty value (â\x80\x98â\x80\x99, [], or false).\n\n.. code-block:: matlabsession\n\n        &gt;&gt; % To clear a configuration that takes a string as input \n        &gt;&gt; c.AdditionalProperties.EmailAddress = â\x80\x98 â\x80\x99;\n\nTo save a profile, with your configuration so you will find it in future sessions\n\n.. code-block:: matlabsession\n\n        &gt;&gt; c.saveProfile;\n\nSerial Jobs\n^^^^^^^^^^^\n\nUse the batch command to submit asynchronous jobs to the cluster.  The batch command will return a job object which is used to access the output of the submitted job.  See the MATLAB documentation for more help on batch.\n\n.. code-block:: matlabsession\n\n        &gt;&gt; % Get a handle to the cluster\n        &gt;&gt; c = parcluster;\n\nSubmit job to query where MATLAB is running on the cluster\n\n.. code-block:: matlabsession\n\n        &gt;&gt; j = c.batch(@pwd, 1, {});\n\nQuery job for state:  queued | running | finished\n\n.. code-block:: matlabsession\n\n        &gt;&gt; j.State\n\nIf state is finished, fetch results\n\n.. code-block:: matlabsession\n\n        &gt;&gt; j.fetchOutputs{:}\n\nor\n\n.. code-block:: matlabsession\n\n        &gt;&gt; fetchOutputs(j)\n\nDisplay the diary\n\n.. code-block:: matlabsession\n\n        &gt;&gt; diary(j)\n\nDelete the job after results are no longer needed\n\n.. code-block:: matlabsession\n\n        &gt;&gt; j.delete;\n\nTo retrieve a list of currently running or completed jobs, call parcluster to retrieve the cluster object. The cluster object stores an array of jobs that were run, are running, or are queued to run. This allows us to fetch the results of completed jobs. Retrieve and view the list of jobs as shown below.&quot;),
  0.8535576462745667)]
</pre></div>
</div>
</div>
</div>
<p>Maybe the answer can improve a bit if we add more retrieved documents…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpc_store</span><span class="o">.</span><span class="n">similarity_search_with_score</span><span class="p">(</span><span class="s2">&quot;What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(Document(id=&#39;leonardo.rst.txt__0&#39;, metadata={&#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;start_index&#39;: 0, &#39;doc_name&#39;: &#39;leonardo.rst.txt&#39;}, page_content=&#39;.. _leonardo_card:\n\nLeonardo\n========\n\nLeonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.\nThis guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.\n\n.. |ico2| image:: img/leonardo_logo.png\n   :height: 55px\n   :class: no-scaled-link\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. \n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:\n\n * login01-ext.leonardo.cineca.it\n * login02-ext.leonardo.cineca.it\n * login05-ext.leonardo.cineca.it\n * login07-ext.leonardo.cineca.it\n\n.. warning::\n    \n    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\nSystem Architecture\n-------------------\n\nThe cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:\n\n* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.\n* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.\n\nThe overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. \n\nThe **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.\nThe **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.\n\nHardware Details\n^^^^^^^^^^^^^^^^\n\n.. tab-set::\n\n    .. tab-item:: Booster&#39;),
  0.7387087941169739),
 (Document(id=&#39;specific_users.rst.txt__2544&#39;, metadata={&#39;doc_name&#39;: &#39;specific_users.rst.txt&#39;, &#39;start_index&#39;: 2544, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&#39;../files/2FA_EF.pdf&gt;`\n\n\nSLURM Partitions\n----------------\n\nOn :ref:`hpc/leonardo:Leonardo` and :ref:`hpc/pitagora:Pitagora` *Job Managing and SLURM Partitions* sections, you can find the description of SLURM partitions and QOS to submit your jobs. Notice that EUROfusion users have dedicated partitions and QOS: you are allowed to use the **&quot;_fua_&quot;** partitions and the related QOS, besides the **&quot;_all_serial&quot;** partition which is shared among all users.\n\n\nLow-priority jobs\n^^^^^^^^^^^^^^^^^\n\n1) **If all the budget assigned to your Project Account has been consumed**, you can keep running on Leonardo boost_fua_prod and dcgp_fua_prod partitions at low priority by requesting in your submission script the **qos_fualowprio** QOS:\n\n.. code-block:: bash\n\n  #SBATCH --account=&lt;YOUR Project Account&gt;\n  #SBATCH --qos=qos_fualowprio\n\nThe QOS is *automatically* added to your Project Account upon budget exhaustion.\n\n2) You can also request to run low priority jobs, **without having consumed all the budget of yout active Project Account**, by association to the **FUAL8_LOWPRIO** account on Booster and **FUA38_LOWPRIO_0** account on DCGP (write a mail to superc@cineca.it). You always need to specify also the **qos_fualowprio** QOS in your submission script.\n\n.. code-block:: bash\n  \n   #SBATCH --account=&lt;LOWPRIO Project Account&gt;\n   #SBATCH --qos=qos_fualowprio&#39;),
  0.7474587559700012),
 (Document(id=&#39;matlab.rst.txt__7416&#39;, metadata={&#39;start_index&#39;: 7416, &#39;doc_name&#39;: &#39;matlab.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&quot;.. code-block:: matlabsession\n        \n        &gt;&gt; % Specify QoS\n        &gt;&gt; c.AdditionalProperties.QoS = &#39;name-of-qos&#39;;\n\n        &gt;&gt; % Specify processor cores per node.  Default is 32 for Leonardo GPU nodes and 112 on Leonardo CPU nodes; 18 for Marconi and 48 for Galileo100.\n        &gt;&gt; c.AdditionalProperties.ProcsPerNode = 18;\n\n        &gt;&gt; % specify the number of GPUsPerNode. Valid only on Leonardo GPU partition\n        &gt;&gt; c.AdditionalProperties.GPUsPerNode = 1;\n\n        &gt;&gt; % Specify memory to use for MATLAB jobs, per core (default: 4gb)\n        &gt;&gt; c.AdditionalProperties.MemUsage = &#39;6gb&#39;;\n\n        &gt;&gt; % Require node exclusivity\n        &gt;&gt; c.AdditionalProperties.RequireExclusiveNode = true;\n\n        &gt;&gt; % Request to use a reservation\n        &gt;&gt; c.AdditionalProperties.Reservation = &#39;name-of-reservation&#39;;\n\n        &gt;&gt; % Specify e-mail address to receive notifications about your job\n        &gt;&gt; c.AdditionalProperties.EmailAddress = â\x80\x98test@foo.comâ\x80\x99;\n\n        &gt;&gt; % Turn onthe Debug Message.  Default is off (logical boolean true/false).\n        &gt;&gt; c.AdditionalProperties.DebugMessagesTurnedOn = true;\n\n\nTo check for the values of the current configuration options, call the AdditionalProperties without semicolon\n\n.. code-block:: matlabsession\n\n        &gt;&gt; % To view current configurations\n        &gt;&gt; c.AdditionalProperties\n\nTo clear a value, assign the property an empty value (â\x80\x98â\x80\x99, [], or false).\n\n.. code-block:: matlabsession\n\n        &gt;&gt; % To clear a configuration that takes a string as input \n        &gt;&gt; c.AdditionalProperties.EmailAddress = â\x80\x98 â\x80\x99;\n\nTo save a profile, with your configuration so you will find it in future sessions\n\n.. code-block:: matlabsession\n\n        &gt;&gt; c.saveProfile;\n\nSerial Jobs\n^^^^^^^^^^^\n\nUse the batch command to submit asynchronous jobs to the cluster.  The batch command will return a job object which is used to access the output of the submitted job.  See the MATLAB documentation for more help on batch.\n\n.. code-block:: matlabsession\n\n        &gt;&gt; % Get a handle to the cluster\n        &gt;&gt; c = parcluster;\n\nSubmit job to query where MATLAB is running on the cluster\n\n.. code-block:: matlabsession\n\n        &gt;&gt; j = c.batch(@pwd, 1, {});\n\nQuery job for state:  queued | running | finished\n\n.. code-block:: matlabsession\n\n        &gt;&gt; j.State\n\nIf state is finished, fetch results\n\n.. code-block:: matlabsession\n\n        &gt;&gt; j.fetchOutputs{:}\n\nor\n\n.. code-block:: matlabsession\n\n        &gt;&gt; fetchOutputs(j)\n\nDisplay the diary\n\n.. code-block:: matlabsession\n\n        &gt;&gt; diary(j)\n\nDelete the job after results are no longer needed\n\n.. code-block:: matlabsession\n\n        &gt;&gt; j.delete;\n\nTo retrieve a list of currently running or completed jobs, call parcluster to retrieve the cluster object. The cluster object stores an array of jobs that were run, are running, or are queued to run. This allows us to fetch the results of completed jobs. Retrieve and view the list of jobs as shown below.&quot;),
  0.8535576462745667),
 (Document(id=&#39;hpc_intro.rst.txt__6244&#39;, metadata={&#39;start_index&#39;: 6244, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;hpc_intro.rst.txt&#39;}, page_content=&quot;.. dropdown:: Example\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n   A user requests 1 node, 4 CPUs, 4 GPUs, and 3 hours of walltime on the Booster partition of Leonardo. However, the job runs for only 2 hours.\n\n   From this information, we have:\n\n   * T = 2 h (elapsed time)\n\n   * N = 1 node\n\n   * C = 32 CPUs (number of CPUs available on a Leonardo Booster compute node â\x80\x94 see :ref:`hpc/leonardo:Hardware Details`)\n\n   and, since:\n\n   .. math::\n\n      \\frac{\\text{Allocated}(\\text{CPU})}{\\text{Total}(\\text{CPU})} = \\frac{4}{32} = 0.125\n\n   .. math::\n\n      \\frac{\\text{Allocated}(\\text{GPU})}{\\text{Total}(\\text{GPU})} = \\frac{4}{4} = 1.0\n\n   the maximum of the resources requested per node is determined by the GPUs, therefore *R* = 1.0, and the billed hours are then calculated as:\n\n   .. math::\n\n      B_{H} = T \\cdot N \\cdot R \\cdot C = 2 \\cdot 1 \\cdot 1.0 \\cdot 32 = 64 \\text{CPUh}\n\n   This means the job consumes 64 effective CPU hours from the project&#39;s budget.\n\n----\n\n.. note::\n\n   * The **serial partition** is available for limited post-production data analysis and can be used even after a Project Account has expired. Usage of this partition is excluded from STDH billing (**free of charge**).\n\n   * By default, the amount of memory allocated per node is proportional to the number of CPUs requested.\n\n   * When nodes are requested in **exclusive mode** (see :ref:`hpc/hpc_scheduler:Scheduler and Job Submission` section), the entire node is reserved for the job, regardless of the specific resources requested. In such cases, the allocated resources may exceed the explicitly requested ones.\n\n   * The **resources per node** are listed in the **Hardware Details** section for each cluster. Refer to the :ref:`hpc/hpc_clusters:Cluster Specifics` section for the complete list of Cineca&#39;s HPC systems.\n\nBudget Linearization\n^^^^^^^^^^^^^^^^^^^^\n\nA linearization policy governs the priority of scheduled jobs across Cineca clusters. To each Project Account is assigned a monthly quota (MQ) calculated as: \n\n.. math::\n\n   MQ = TB/NM\n\nTB = total assigned budget\n\nNM = total number of months\n\nBeginning on the first day of each month, any User Accounts belonging a Project Account may utilize their quota at full priority. \nAs the budget is consumed, submitted jobs progressively lose priority until the monthly quota is exhausted. \nSubsequently, these jobs are still considered for execution but with reduced priority compared to accounts with remaining quota. \nThis policy aligns with practices at other prominent HPC centers globally, aiming to enhance response times by aligning CPU hour usage with budget sizes.&quot;),
  0.8583566546440125),
 (Document(id=&#39;galileo.rst.txt__2334&#39;, metadata={&#39;start_index&#39;: 2334, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;galileo.rst.txt&#39;}, page_content=&#39;.. list-table:: \n\t:widths: 10 10 20 10 10 10 10 20\n\t:header-rows: 1\n\t:class: tight-table\n\n\t* - **Partition**\n\t  - **QOS**\n\t  - **#Cores per job**\n\t  - **Walltime**\n\t  - **Max jobs/resources per user**\n\t  - **Max memory per node (MB)**\n\t  - **Priority**\n\t  - **Notes**\n\t* - g100_all_serial\n\n\t    (default)\n\t  - noQOS\n\t  - 4 cores\n\t  - 04:00:00\n\t  - 4 cores\n\n\t    120 submitted jobs\n\t  - 31,200\n\t  \n\t    (30 GB)\n\t  - 40\n\t  - on two login nodes\n\n\t    **budget free**\n\t* - g100_all_serial\n\n\t    (default)\n\t  - qos_install\n\t  - 16 cores\n\t  - 04:00:00\n\t  - 16 cores\n\n\t    1 running job\n\t  - 100 GB\n\t  - 40\n\t  - request to superc@cineca.it\n\t* - g100_usr_dbg\n\t  - noQOS\n\t  - 2 nodes\n\t  - 01:00:00\n\t  -\n\t  - 375,300\n\n\t    (366 GB)\n\t  - 40\n\t  -\n\t* - g100_usr_dbg\n\t  - qos_ind\n\t  - Depending on the specific agreement\n\t  - Depending on the specific agreement\n\t  -\n\t  - 375,300\n\n\t    (366 GB)\n\t  - 90\n\t  - Partition dedicated to specific kinds of users.\n\t* - g100_usr_prod\n\t\n\t    *g100_usr_smem*\n\t  \n\t    **g100_usr_pmem**\n\t  - noQOS\n\t  - min = 1\n\t  \n\t    max =  32 nodes\n\t  - 24:00:00\n\t  - 100 running jobs\n\t  \n\t    120 submitted jobs\n\t  - 375,300\n\t  \n\t    (366 GB)\n\t  - 40\n\t  - runs on thin and persistent memory nodes\n\t\n\t    *runs only on thin nodes*\n\t  \n\t    **runs only on persistent memory nodes**\n\t* - g100_usr_prod\n\t\n\t    *g100_usr_smem*\n\t  \n\t    **g100_usr_pmem**\n\t  - g100_qos_bprod\n\t  - min = 1537 (33 nodes)\n\t  \n\t    max =  3072 (64 nodes)\n\t  - 24:00:00\n\t  - 100 running jobs\n\t  \n\t    120 submitted jobs\n\t  - 375,300\n\t  \n\t    (366 GB)\n\t  - 60\n\t  - runs on thin and persistent memory nodes\n\t\n\t    *runs only on thin nodes*\n\t  \n\t    **runs only on persistent memory nodes**\n\t* - g100_usr_prod\n\t\n\t    *g100_usr_smem*\n\t  \n\t    **g100_usr_pmem**\n\t  - g100_qos_lprod\n\t  - min = 1\n\t  \n\t    max =  2 nodes\n\t  - 4-00:00:00\n\t  - 2 nodes\n\t  \n\t    100 running jobs\n\t  \n\t    120 submitted jobs\n\t  - 375,300\n\t  \n\t    (366 GB)\n\t  - 40\n\t  - runs on thin and persistent memory nodes\n\t\n\t    *runs only on thin nodes*\n\t  \n\t    **runs only on persistent memory nodes**\n\t* - g100_usr_prod\n\t\n\t    *g100_usr_smem*\n\t  \n\t    **g100_usr_pmem**\n\t  - qos_special\n\t  - &gt; 32 nodes\n\t  - &gt; 24:00:00\n\t  -\n\t  - 375,300\n\t  \n\t    (366 GB)\n\t  - 40\n\t  - request to superc@cineca.it\n\t* - g100_usr_bmem\n\t  - noQOS\n\t  - 25 nodes\n\t  - 24:00:00\n\t  - 100 running jobs\n\t  \n\t    120 submitted jobs\n\t  - 3,036,000\n\t  \n\t    (3 TB)\n\t  - 40\n\t  - runs on fat nodes\n\t* - g100_usr_interactive\n\t  - noQOS\n\t  - max = 0.5 node\n\t  - 8:00:00\n\t  - 100 running jobs\n\t  \n\t    120 submitted jobs\n\t  - 375,300\n\t  \n\t    (366 GB)\n\t  - 40\n\t  - on nodes with GPUs\n\n\t    --gres=gpu:N (N=1)\n\t* - g100_meteo_prod\n\t  - qos_meteo\n\t  - \n\t  - 24:00:00\n\t  - \n\t  - 375,300\n\t  \n\t    (366 GB)\n\t  - 40\n\t  - Partition reserved to meteo services&#39;),
  0.8675727844238281),
 (Document(id=&#39;qe.rst.txt__0&#39;, metadata={&#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;qe.rst.txt&#39;, &#39;start_index&#39;: 0}, page_content=&quot;.. _quantum_espresso_card:\n\nQuantumESPRESSO\n===============\n\nThe following guide describes how to load, configure and use QuantumESPRESSO @ CINECA&#39;s cluster.\nQuantumESPRESSO is available on :ref:`hpc/leonardo:Leonardo` and :ref:`hpc/galileo:Galileo100` clusters.\n\nRelevant links\n^^^^^^^^^^^^^^\n\n- QE repository: https://gitlab.com/QEF/q-e.git\n- MaX benchmarks: https://gitlab.com/max-centre/benchmarks-max3.git\n- JUBE xmls: https://gitlab.com/max-centre/JUBE4MaX.git\n- spack recipe: https://gitlab.com/spack/spack/-/blob/develop/var/spack/repos/builtin.mock/packages/quantum-espresso/package.py\n\nModules\n^^^^^^^\n\nCPU-based and GPU-based machines deploy QuantumESPRESSO with different software stacks, to fully exploit the underlying hardware. In particular:\n\n- **Intel/Oneapi** compiler and MPI implementation on G100 and Leonardo DCGP, plus **MKL** for FFT, BLAS/LAPACK and SCALAPACK\n- **NVHPC** compiler and **OpenMPI/HPCX-MPI** on Leonardo Booster, plus **OpenBLAS** and **FFTW** libraries.\n\nInstallations based on gcc compiler do not provide performance, and are provided for postprocessing executables.\n\nAlternative Installations\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf you wish installing your own version of QuantumESPRESSO, we suggesting using CMake and the options provided in the `Wiki of the official repository &lt;https://gitlab.com/QEF/q-e/-/wikis/Developers/CMake-build-system&gt;`_ for the CINECA cluster in use. \n\nParallelization strategies\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nQuantumESPRESSO supports different parallelization strategies. \n\n- R&amp;G (`-npw` or no options) processes to distribute real/reciprocal spaces\n- pools (`-nk`) to distribute k-points\n- images (`-ni`) to distribute irreducible representations or q-points in a dispersion\n- band processes (`-nbnd`) to distribute the Kohn-Sham states\n- linear algebra processes (auto) to distribute diagonalization, via scalapack or custom algorithm. For GPU installations, the diagonalization is done on a single GPU (scalapack are not used\n\nWe suggest the following for optimal performance on Leonardo Booster:\n\n- prioritize pools over R&amp;G , in particular for workloads with hundreds of planes or less in the z-direction, also for intra-node distribution. \n- The minimum number of k-points per pool (kunit) in PWSCF is the number of k-points (`kunit=1`), while in phonon is usually `kunit=2`&quot;),
  0.8792144060134888),
 (Document(id=&#39;leonardo.rst.txt__20840&#39;, metadata={&#39;doc_name&#39;: &#39;leonardo.rst.txt&#39;, &#39;start_index&#39;: 20840, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&#39;.. raw:: html\n\n          &lt;p&gt;Each Booster cell is composed of:&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;6 Ã\x97 Atos BullSequana XH2000 racks&lt;/strong&gt;, each containing:\n              &lt;ul&gt;\n                &lt;li&gt;3 Ã\x97 Level 2 (L2) switches&lt;/li&gt;\n                &lt;li&gt;3 Ã\x97 Level 1 (L1) switches&lt;/li&gt;\n                &lt;li&gt;30 compute nodes â\x80\x94 each equipped with 4 GPUs, each connected via a dedicated 100 Gbps port&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Total per Booster cell:&lt;/strong&gt; 18 L2 switches, 18 L1 switches, and 180 compute nodes.&lt;/p&gt;\n\n          &lt;h4&gt;Connectivity Overview&lt;/h4&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 2 (L2) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 22 Ã\x97 200 Gbps ports connecting to L2 switches in other cells&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 18 Ã\x97 200 Gbps ports connecting to L1 switches within the cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription:&lt;/strong&gt; 0.8:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 1 (L1) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 18 Ã\x97 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 40 Ã\x97 100 Gbps ports connected to GPUs across 10 compute nodes&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription:&lt;/strong&gt; 1.11:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n        .. figure:: img/leo-net-booster_cell.png\n          :height: 750px\n          :align: center\n      \n      .. tab-item:: DCGP\n        \n        .. raw:: html&#39;),
  0.8816628456115723),
 (Document(id=&#39;leonardo.rst.txt__7317&#39;, metadata={&#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;leonardo.rst.txt&#39;, &#39;start_index&#39;: 7317}, page_content=&#39;.. tab-item:: Booster\n\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**   | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+=================================+==============+=====================================+\n        | lrd_all_serial | normal             | 4 cores                 | 04:00:00     | 1 node / 4 cores                | 40           | No GPUs&#39;),
  0.8856500387191772),
 (Document(id=&#39;singularity.rst.txt__21115&#39;, metadata={&#39;doc_name&#39;: &#39;singularity.rst.txt&#39;, &#39;start_index&#39;: 21115, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&#39;.. code-block:: bash\n\n            salloc -t 03:00:00 --nodes=6 --ntasks-per-node=4 --ntasks=24 --gres=gpu:4 -p boost_usr_prod -A &lt;Account_name&gt;\n            &lt;load the necessary modules and/or export necessary variables&gt;\n            export OMP_NUM_THREADS=8\n            srun --nodes=6 --ntasks-per-node=4 --ntasks=24 singularity exec --nv &lt;container_img&gt; &lt;container_cmd&gt;\n\n        \n\n    .. tab-item:: Galileo100\n\n        On Galileo100, Singularity version 3.8.0 is available on the login nodes and on the partitions. \n        Beware that, for the Galileo00 cluster, nodes with GPU are available only under \n        a reservation (send an email to superc@cineca.it) and through the interactive computing service; moreover, there one \n        can at most request one node with **2 GPUs** so no internode communication will actually be performed. \n        The necessary MPI, Singularity and CUDA modules are the following:\n\n            * ``module load profile/advanced`` (profile with additional modules)\n            * ``module load autoload singularity/3.8.0--bind--openmpi--4.1.1``\n            * ``module load cuda/11.5.0``\n\n        .. note::\n            \n            The ``module load autoload singularity/3.8.0--bind--openmpi--4.1.1`` command automatically loads the following modules:\n\n            * ``singularity/3.8.0--bindâ\x80\x93openmpiâ\x80\x934.1.1``\n            * ``zlib/1.2.11--gccâ\x80\x9310.2.0``\n            * ``openmpi/4.1.1--gcc--10.2.0-cudaâ\x80\x9311.1.0``\n\n        The following code snippet is an example of a Slurm job script for running MPI parallel containerized applications on the GALILEO100 cluster. \n        Notice that the ``--cpus-per-task`` option has been set to **48** to fully exploit the CPUs in the ``g100_usr_prod`` partition.\n\n        .. code-block:: bash\n\n            #!/bin/bash\n \n            #SBATCH --nodes=6\n            #SBATCH --tasks-per-node=1\n            #SBATCH --cpu-per-task=48\n            #SBATCH --mem=30GB\n            #SBATCH --time=00:10:00\n            #SBATCH --out=slurm.%j.out\n            #SBATCH --err=slurm.%j.err\n            #SBATCH --account=&lt;Account_name&gt;\n            #SBATCH --partition=g100_usr_prod\n  \n            module purge\n            module load profile/advanced\n            module load autoload singularity/3.8.0--bind--openmpi--4.1.1\n            module load cuda/11.5.0\n \n            mpirun -np 6 singularity exec &lt;container_img&gt; &lt;container_cmd&gt;&#39;),
  0.9354047179222107),
 (Document(id=&#39;leonardo.rst.txt__18311&#39;, metadata={&#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;leonardo.rst.txt&#39;, &#39;start_index&#39;: 18311}, page_content=&#39;.. note::\n\n          The partitions: **dcgp_fua_dbg, dcgp_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.\n\nNetwork Architecture\n--------------------\n\n.. raw:: html\n\n  &lt;p&gt;&lt;strong&gt;Leonardo&lt;/strong&gt; features a state-of-the-art interconnect system tailored for high-performance computing (HPC). It delivers &lt;em&gt;low latency&lt;/em&gt; and &lt;em&gt;high bandwidth&lt;/em&gt; by leveraging &lt;strong&gt;NVIDIA Mellanox InfiniBand HDR&lt;/strong&gt; (High Data Rate) technology, powered by &lt;a href=&quot;https://nvdam.widen.net/s/zmbw7rdjml/infiniband-qm8700-datasheet-us-nvidia-1746790-r12-web&quot;&gt;NVIDIA QUANTUM QM8700 Smart Switches&lt;/a&gt;, and a &lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/7885210&quot;&gt;Dragonfly+ topology&lt;/a&gt;&lt;/strong&gt;. Below is an overview of its architecture and key features:&lt;/p&gt;\n\n  &lt;ul&gt;\n    &lt;li&gt;&lt;strong&gt;Hierarchical Cell Structure:&lt;/strong&gt; The system is structured into multiple &lt;em&gt;cells&lt;/em&gt;, each comprising a group of interconnected compute nodes.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Inter-cell Connectivity:&lt;/strong&gt; As illustrated in the figure below, cells are connected via an all-to-all topology. Each pair of distinct cells is linked by 18 independent connections, each passing through a dedicated Layer 2 (L2) switch. This design ensures high availability and reduces congestion.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Intra-cell Topology:&lt;/strong&gt; Inside each cell, a non-blocking two-layer fat-tree topology is used, allowing scalable and efficient intra-cell communication.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;System Composition:&lt;/strong&gt;\n      &lt;ul&gt;\n        &lt;li&gt;19 cells dedicated to the &lt;em&gt;Booster&lt;/em&gt; partition.&lt;/li&gt;\n        &lt;li&gt;2 cells for the &lt;em&gt;DCGP&lt;/em&gt; (Data-Centric General Purpose) partition.&lt;/li&gt;\n        &lt;li&gt;1 hybrid cell with both accelerated (36 Booster nodes) and conventional (288 DCGP nodes) compute resources.&lt;/li&gt;\n        &lt;li&gt;1 cell allocated for management, storage, and login services.&lt;/li&gt;\n      &lt;/ul&gt;\n    &lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Adaptive Routing:&lt;/strong&gt; The network employs adaptive routing, dynamically optimizing data paths to alleviate congestion and maintain performance under load.&lt;/li&gt;\n  &lt;/ul&gt;\n  \n.. figure:: img/leo-net-all2all.png\n   :height: 350px\n   :align: center\n   :class: no-scaled-link\n\n.. image:: img/spacer.png\n   :align: center\n   :class: no-scaled-link\n   \n.. dropdown:: Cell Configuration and Intra-cell Connectivity\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n   .. tab-set::\n\n      .. tab-item:: Booster&#39;),
  0.940382719039917),
 (Document(id=&#39;pitagora.rst.txt__0&#39;, metadata={&#39;start_index&#39;: 0, &#39;doc_name&#39;: &#39;pitagora.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&quot;.. _pitagora_card:\n\nPitagora\n========\n\n.. figure:: ../img/warning3.png\n   :align: center\n   :class: no-scaled-link\n   :height: 150px\n\n.. figure:: ../img/spacer.png\n   :align: center\n   :class: no-scaled-link\n   :height: 20px\n\nPitagora is the new EUROfusion supercomputer hosted by **CINECA** and currently built in the CINECA&#39;s headquarter in Casalecchio di Reno, Bologna, Italy. The cluster is supplied by Lenovo corp. and is composed of two partitions: A general purpose partition cpu-based named **DCPG** and an accelerated partition based on NVIDIA H100 accelerators named **Booster**.\n\nThe specific guide for the **Pitagora** cluster contains unique information that deviates from the general behavior described in the HPC Clusters sections.\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.pitagora.cineca.it**. \n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Pitagora** using one the specific login hostname points:\n\n * login01-ext.pitagora.cineca.it\n * login02-ext.pitagora.cineca.it\n * login03-ext.pitagora.cineca.it\n * login04-ext.pitagora.cineca.it\n * login05-ext.pitagora.cineca.it\n * login06-ext.pitagora.cineca.it\n\n.. warning::\n    \n    **The mandatory access to Pitagora is the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\nSystem Architecture\n-------------------\n\nThe system, supplied by Lenovo, is based on two new specifically-designed compute blades, which are available throught two distinct SLURM partitios \non the Cluster:\n\n* **GPU** blade based on NVIDIA NVIDIA H100 accelerators - **Booster** partition.\n* **CPU**-only blade based on  AMD Turin 128c processors - **Data Centric General Purpose (DCGP)** partition.\n\nThe overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. \n\n\nHardware Details\n^^^^^^^^^^^^^^^^\n.. tab-set::\n\n    .. tab-item:: Booster&quot;),
  0.9409757852554321),
 (Document(id=&#39;matlab.rst.txt__5663&#39;, metadata={&#39;start_index&#39;: 5663, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;matlab.rst.txt&#39;}, page_content=&quot;.. code-block:: matlabsession\n        \n        &gt;&gt; DEFAULTPROFILE=&#39;galileo100 R2024b&#39;\n\non Galileo100 and similarly on Leonardo.\n\nConfiguring Jobs\n^^^^^^^^^^^^^^^^\n\nPrior to submitting the job, various parameters have to be specified in order to be passed to jobs, such as queue, username, e-mail, etc. \n\n.. note::\n Any parameters specified using the below workflow will be persistent between MATLAB sessions if saved at the end of the configuration.\n\nBefore specifying any parameters, you will need to obtain a handle to the cluster object.\n\n.. code-block:: matlabsession\n\n        &gt;&gt; % Get a handle to the cluster\n        &gt;&gt; c = parcluster;\n\nYou are now **required** to specify an Account Name, a Queue Name and the Wall Time (visit :ref:`hpc/hpc_intro:Budget and Accounting` to see how to retrieve your Budget Account Name using the saldo command)\n\n.. code-block:: matlabsession\n\n        &gt;&gt; % Specify an Account to use for MATLAB jobs\n        &gt;&gt; c.AdditionalProperties.AccountName = &#39;account_name&#39;;\n\n        &gt;&gt; % Specify a queue to use for MATLAB jobs\n        &gt;&gt; c.AdditionalProperties.Partition = &#39;partition-name&#39;;\n\n        &gt;&gt; % Specify the walltime (e.g. 5 hours)\n        &gt;&gt; c.AdditionalProperties.WallTime = &#39;05:00:00&#39;;\n\nOn Leonardo cluster there are two partitions: &#39;boost_usr_prod&#39; to access GPU nodes and &#39;dcgp_usr_prod&#39; to access CPU nodes. You can find additional info on the :ref:`hpc/leonardo:Leonardo` dedicated pages.\nFor Galileo100 cluster the main partition is &#39;g100_usr_prod&#39;. In :ref:`hpc/galileo:Galileo100` dedicated page you can find other possible Partitions and QOS available allowing for different combinations of nodes, walltime and priority.\n\nYou can specify other **additional** (not-mandatory) parameters along with your job.&quot;),
  0.975282609462738),
 (Document(id=&#39;hpc_data_storage.rst.txt__40289&#39;, metadata={&#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;hpc_data_storage.rst.txt&#39;, &#39;start_index&#39;: 40289}, page_content=&#39;.. code-block:: bash\n\n                    ssh -xt &lt;username&gt;@data.&lt;cluster_name&gt;.cineca.it wget http://ftp.gnu.org/gnu/wget/wget2-2.0.0.tar.gz -P /absolute/path/to/\n\n                Please note that is mandatory to use the flag -P with the absolute path of the destination folder, because of the fake /home directory.\n\n\n            - **curl**\n\n                Sometimes, the 10-minute CPU time limit or the 4-hour wall time limit on the serial queue is not enough to download a large dataset for ML. In this case, you can use curl from the datamover. Here you can find a simple example\n\n                .. code-block:: bash\n\n                    ssh -xt &lt;username&gt;@data.&lt;cluster_name&gt;.cineca.it curl https://curl.se/download/curl-8.2.1.tar.gz --output /absolute/path/to/curl-8.2.1.tar.gz\n\n                Please note that is mandatory to use the flag --output with the absolute path of the destination file, because of the fake /home directory.\n\n\n            - **rclone**\n\n                Rclone is a powerful tool that supports different transfer protocols, and a lot of data [providers](https://rclone.org/#providers). At the moment is available only on Leonardo datamovers. It needs a configuration file. If you are able, you car write the configuration file using your favourite editor (VIM) or you can rely on the rclone config command:\n\n                .. code-block:: bash\n\n                    ssh -xt &lt;username&gt;@data.leonardo.cineca.it rclone --config /leonardo/home/userexternal/&lt;username&gt;/.rclone.conf config\n\n                When your configuration is ready you can use rclone to manage data between Leonardo filesystem and the remote host you have configures. For example:&#39;),
  0.9759920239448547),
 (Document(id=&#39;specific_users.rst.txt__0&#39;, metadata={&#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;specific_users.rst.txt&#39;, &#39;start_index&#39;: 0}, page_content=&#39;.. _spec_users_card:\n\nEUROfusion\n==========\n\n.. figure:: ../img/warning3.png\n   :align: center\n   :class: no-scaled-link\n   :height: 150px\n\n.. figure:: ../img/spacer.png\n   :align: center\n   :class: no-scaled-link\n   :height: 20px\n\n.. |ico1| image:: img/EUROfusion.png\n   :height: 35px\n   :class: no-scaled-link\n\nThe |ico1| community has access to the following CINECA HPC systems:\n\n * Leonardo\n\n    - Booster partition\n    - DCGP partition\n\n * Pitagora\n\n.. important::\n   The general environment defined on our clusters is the same for all the users, so EUROfusion users are invited to refer to the general documentation.\n   Essential links below.\n\nFor general information regarding the access to the HPC clusters:\n\n* :ref:`general/getting_started:Getting Started`\n* :ref:`general/users_account:Users and Accounts`\n* :ref:`general/access:Access to the Systems`\n\nFor general information regarding the environment on the HPC clusters:\n\n* :ref:`hpc/hpc_intro:Introduction HPC Resources`\n* :ref:`hpc/hpc_data_storage:File Systems and Data Management`\n* :ref:`hpc/hpc_scheduler:Scheduler and Job Submission`\n* :ref:`hpc/hpc_enviroment:Environment and Customization`\n\nFor specific information regarding the HPC clusters used by the EUROfusion community:\n\n* :ref:`hpc/leonardo:Leonardo`\n* :ref:`hpc/pitagora:Pitagora`\n\n\nDedicated tutorials\n-------------------\n\n.. |tutorial| image:: /specific_users/img/tutorial_icon.png\n   :width: 35px\n   :class: no-scaled-link\n\nA presentation of each supercomputer, and of the access method via two-factor authentication (2FA), have been dedicated to the EUROfusion community. You can find the slides, including a report of the final Q&amp;A session, and the recording at the following links (you should log in through the button :bdg-black-line:`Access as a guest`).\n\n.. card:: |tutorial| Leonardo Booster: *Introduction to Leonardo supercomputer for Eurofusion*\n\n   June 6th, 2023\n\n   `Leonardo Booster webinar page &lt;https://learn.cineca.it/course/view.php?id=1461&gt;`_ with slides and recording.\n\n   :download:`Leonardo Booster slides &lt;../files/Leonardo_Booster_EF.pdf&gt;`\n\n.. card:: |tutorial| Leonardo DCGP: *Introduction to Leonardo DCGP for Eurofusion*\n   \n   February 18th, 2025\n   \n   `Leonardo DCGP webinar page &lt;https://learn.cineca.it/course/view.php?id=2025&gt;`_ with slides and recording.\n\n   :download:`Leonardo DCGP slides &lt;../files/Leonardo_DCGP_EF.pdf&gt;`\n\n.. card:: |tutorial| 2FA: *Introduction to two-factor authentication (2FA) on CINECA HPC clusters*\n   \n   June 7th, 2023\n   \n   :download:`2FA slides &lt;&#39;),
  0.9863142967224121),
 (Document(id=&#39;leonardo.rst.txt__12699&#39;, metadata={&#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;leonardo.rst.txt&#39;, &#39;start_index&#39;: 12699}, page_content=&#39;+----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**        | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+======================================+==============+=====================================+\n        | lrd_all_serial | normal             | max = 4 cores           | 04:00:00     | 1 node / 4 cores                     | 40           | Hyperthreading x 2                  |\n        |                |                    |                         |              |                                      |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                       |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_usr_prod  | normal             | 16 nodes                | 24:00:00     | 512 nodes per prj. account           | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_dbg       | 2 nodes                 | 00:30:00     | 2 nodes / 224 cores per user account | 80           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_bprod     | min = 17 nodes          | 24:00:00     | 128 nodes per user account           | 60           | GrpTRES = 1536 nodes                |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 128 nodes         |              | 512 nodes per prj. account           |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_lprod     | 3 nodes                 | 4-00:00:00   | 3 nodes / 336 cores per user account | 40           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_dbg   | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 224 cores                  | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_prod  | normal             | 16 nodes                | 24:00:00     |                                      | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+&#39;),
  0.9868746399879456),
 (Document(id=&#39;pitagora.rst.txt__4589&#39;, metadata={&#39;start_index&#39;: 4589, &#39;doc_name&#39;: &#39;pitagora.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;}, page_content=&#39;.. tab-item:: Booster\n\n        +------------------+------------------------+---------------------------+--------------+----------------------------+--------------+-------------------------------------+\n        | **Partition**    | **QOS**                | **#Nodes/#per job**       | **Walltime** | **#Max Nodes/#per user**   | **Priority** | **Notes**                           |\n        +==================+========================+===========================+==============+============================+==============+=====================================+\n        | boost_fua_prod   | normal                 | max = 16                  | 24:00:00     | 32                         | 40           |                                     |\n        +                  +------------------------+---------------------------+--------------+----------------------------+--------------+-------------------------------------+\n        |                  | boost_qos_fuadbg       | max = 2                   | 00:10:00     |  2                         | 80           |                                     |\n        +                  +------------------------+---------------------------+--------------+----------------------------+--------------+-------------------------------------+\n        |                  | boost_qos_fuaprod      | min = 17 (full nodes)     | 24:00:00     | 32                         | 60           | runs on 96 nodes (GrpTRES)          |\n        |                  |                        | max = 32                  |              |                            |              |                                     |\n        +                  +------------------------+---------------------------+--------------+----------------------------+--------------+-------------------------------------+\n        |                  | boost_qos_fualprod     | max = 3                   | 4-00:00:00   |  3                         | 40           |                                     |\n        +------------------+------------------------+---------------------------+--------------+----------------------------+--------------+-------------------------------------+&#39;),
  1.0001477003097534),
 (Document(id=&#39;leonardo.rst.txt__12453&#39;, metadata={&#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;start_index&#39;: 12453, &#39;doc_name&#39;: &#39;leonardo.rst.txt&#39;}, page_content=&#39;.. note::\n\n          The partitions: **boost_fua_dbg, boost_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.&#39;),
  1.0093889236450195),
 (Document(id=&#39;galileo.rst.txt__0&#39;, metadata={&#39;doc_name&#39;: &#39;galileo.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;start_index&#39;: 0}, page_content=&#39;.. _galileo_card:\n\nGalileo100\n==========\n\nGalileo100 is a new infrastructure co-funded by the European ICEI (Interactive Computing e-Infrastructure) project and engineered by DELL. It is the national Tier-1 system for scientific research and is available to the Italian public and industrial researchers since September 2021. It also features 77 cloud computing servers and was expanded in November 2022 with 82 additional nodes. **Galileo100** is used for high-end technical and industrial HPC projects, as well as meteorology and environmental studies.\n\nThe specific guide for the **Galileo100** cluster contains unique information that deviates from the general behavior described in the HPC Clusters sections.\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.g100.cineca.it**.\n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Galileo100** using one the specific login hostname points:\n\n* login01-ext.g100.cineca.it\n* login02-ext.g100.cineca.it \n* login03-ext.g100.cineca.it \n\n.. warning::\n    \n    **The mandatory access to Galileo100 is the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\n\nSystem Architecture\n-------------------\n\n\n\nHardware Details\n^^^^^^^^^^^^^^^^\n.. list-table:: \n    :widths: 30 50\n    :header-rows: 1\n\n    * - **Type**\n      - **Specific**\n    * - Models\n      - Dual-soket Dell PowerEdge\n    * - Nodes\n      - 630\n    * - Processors/node\n      - 2xCPU x86 Intel Xeon Platinum 8276/L 2.4GHz\n    * - CPU/node\n      - 48 \n    * - Accelerators/node\n      - 2xGPU Nvidia V100 PCIe3 with 32 GB Ram on 36 Viz Nodes\n    * - RAM/node\n      - 384 GiB (+ 3.0 TiB Optane on 180 fat nodes)\n    * - Peak Performance\n      - 2 PFlop/s (3.53 TFlop/s in single node)\n    * - Internal Network\n      - Mellanox Infiniband 100GbE\n\n\nDisks and Filesystems\n---------------------\n\nThe storage organization conforms to **CINECA** infrastructure. General information are reported in :ref:`hpc/hpc_data_storage:File Systems and Data Management` section. In the following, only differences with respect to general behavior are listed and explained.\n\n\nJob Managing and SLURM Partitions \n---------------------------------&#39;),
  1.0247918367385864),
 (Document(id=&#39;singularity.rst.txt__13791&#39;, metadata={&#39;doc_name&#39;: &#39;singularity.rst.txt&#39;, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;start_index&#39;: 13791}, page_content=&#39;.. list-table:: \n     :widths: 30 30 30 30\n     :header-rows: 1\n     \n     * - \n       - **Driver Version**\n       - **CUDA Version**\n       - **GPU Model**\n     * - Leonardo\n       - 530.30.20\n       - 12.1\n       - NVIDIA A100 SXM6 64 GB HBM2\n     * - Galileo100\n       - 470.42.01\n       - 11.4\n       - NVIDIA V100 PCIe3 32 GB\n\nwhile the `CUDA compatibility &lt;https://docs.nvidia.com/deploy/cuda-compatibility/&gt;`_ table is:\n\n.. list-table:: \n    :widths: 50 50 \n    :header-rows: 1\n\n    * - **CUDA Version**\n      - **Required Drivers**\n    * - CUDA 12.x\n      - from 525.60.13\n    * - CUDA 11.x\n      - from 450.80.02\n\nOne can surely install a working version of CUDA on his own, for example via Spack. However, a simple and effective way to obtain a container image provided with a CUDA installation is to bootstrap from an NVIDIA HPC SDK docker container, which already comes equipped with CUDA, OpenMPI and the NVHPC compilers. Such containers are available at the `NVIDIA catalog &lt;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc/tags&gt;`_. Their tag follows a simple structure, ``$NVHPC_VERSION-$BUILD_TYPE-cuda$CUDA_VERSION-$OS``,  where:\n\n1. ``$BUILD_TYPE``: can either take the value devel or runtime. The first ones are usually heavier and employed to compile and install applications. The second ones are lightweight containers for deployment, stripped of all the compilers and applications not needed at runtime execution.\n2. ``$CUDA_VERSION``: an either take a specific value (e.g. ) or be a ``multi``.  The multi flavors hold up to three different CUDA version, and as such are much heavier. However, they can be useful to deploy the same base container on HPC with different CUDA specifics or to try out the performance of the various versions.\n\nIn the following we provide a minimal Singularity definition file following the above principles, namely: bootstrap from a develop NVIDIA HPC SDK container, install the needed applications, copy the necessary binaries and files for runtime, pass to a lightweight container. This technique is called multistage build, more information available `here &lt;https://docs.sylabs.io/guides/3.7/user-guide/definition_files.html#multi-stage-builds&gt;`_.&#39;),
  1.028136968612671),
 (Document(id=&#39;leonardo.rst.txt__2145&#39;, metadata={&#39;start_index&#39;: 2145, &#39;scraped_on&#39;: &#39;2025-07-22&#39;, &#39;doc_name&#39;: &#39;leonardo.rst.txt&#39;}, page_content=&#39;.. list-table:: \n            :widths: 30 50\n            :header-rows: 1\n\n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2135, Da Vinci single-node GPU\n            * - Racks\n              - 116\n            * - Nodes\n              - 3456\n            * - Processors/node\n              - 1x `Intel Ice Lake Intel Xeon Platinum 8358 &lt;https://www.intel.com/content/www/us/en/products/sku/212282/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 32\n            * - Accelerators/node\n              - 4x `NVIDIA Ampere100 custom &lt;https://doi.org/10.17815/jlsrf-8-186&gt;`_, 64GiB HBM2e NVLink 3.0 (200 GB/s)\n            * - Local Storage/node (tmfs)\n              - (none)\n            * - RAM/node \n              - 512 GiB DDR4 3200 MHz\n            * - Rmax\n              - 241.2 PFlop/s (`top500 &lt;https://www.top500.org/system/180128/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology \n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n    .. tab-item:: DCGP\n\n        .. list-table::\n            :widths: 30 50\n            :header-rows: 1\n            \n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2140 three-node CPU blade\n            * - Racks\n              - 22\n            * - Nodes\n              - 1536\n            * - Processors/node\n              - 2x `Intel Sapphire Rapids Intel Xeon Platinum 8480+ &lt;https://www.intel.com/content/www/us/en/products/sku/231746/intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 112 cores/node\n            * - Accelerators\n              - (none)\n            * - Local Storage/node (tmfs)\n              - 3 TiB\n            * - RAM/node\n              - 512(8x64) GiB DDR5 4800 MHz\n            * - Rmax\n              - 7.84 PFlop/s (`top500 &lt;https://www.top500.org/system/180204/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology\n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n\nFile Systems and Data Managment\n-------------------------------\n\nThe storage organization conforms to **CINECA** infrastructure. General information are reported in :ref:`hpc/hpc_data_storage:File Systems and Data Management` section. In the following, only differences with respect to general behavior are listed and explained.&#39;),
  1.0423932075500488)]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="evaluating-k-size-in-the-retrieval-step">
<h1>Evaluating k-size in the retrieval step<a class="headerlink" href="#evaluating-k-size-in-the-retrieval-step" title="Link to this heading"></a></h1>
<p>We can use the previously created Q&amp;A set to tune the parameter k, which indicates the number of documents returned by the vector search.
We are interested in observing how the number of relevant chunks returned for each question changes when we increment k.</p>
<p>In this case, we consider a chunk to be “relevant” if it comes from the document that answers the question we are evaluating.</p>
<section id="precision-and-recall">
<h2>Precision and recall<a class="headerlink" href="#precision-and-recall" title="Link to this heading"></a></h2>
<p>We can use the precision&#64;k and recall&#64;k metric, which are defined as:</p>
<p><span class="math notranslate nohighlight">\(Precision&#64;k = \frac{\#Relevant Documents Retrieved}{K}\)</span></p>
<p>Precision&#64;k indicates, for a given query, the proportion of retrieved documents that are relevant out of the total number of documents retrieved.</p>
<p><span class="math notranslate nohighlight">\(Recall&#64;k = \frac{\#Relevant Documents Retrieved}{\#Relevant Documents Total}\)</span></p>
<p>The Recall&#64;k indicates, for a given query, how many relevant documents were left out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">optimize_retriever</span><span class="p">(</span><span class="n">vector_store</span><span class="p">,</span> <span class="n">qa_set</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">max_k</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">reranker_name</span><span class="p">:</span><span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>

    <span class="n">k</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_k</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">precision_k</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">recall_k</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">f1_k</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">reranker_name</span><span class="p">:</span>
        <span class="c1"># The cross encoder we will use</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceCrossEncoder</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">reranker_name</span><span class="p">)</span>
    
    <span class="c1"># For each k calculate precision and recall</span>
    <span class="k">for</span> <span class="n">k_threshold</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="si">}</span><span class="s2"> - Testing k_threshold </span><span class="si">{</span><span class="n">k_threshold</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># The precision and recall at k achieved for question i</span>
        <span class="n">precision_q_i</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">recall_q_i</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">question_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">qa_set</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">question</span> <span class="o">=</span> <span class="n">qa_set</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">question_i</span><span class="p">][</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
            <span class="n">doc_provenance</span> <span class="o">=</span> <span class="n">qa_set</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">question_i</span><span class="p">][</span><span class="s2">&quot;doc&quot;</span><span class="p">]</span>
            
            <span class="c1"># Trigger search and get top 20 docs</span>
            <span class="n">retriever</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span><span class="mi">20</span><span class="p">})</span>

            <span class="k">if</span> <span class="n">reranker_name</span><span class="p">:</span>
                <span class="c1"># Use a cross-encoder to rank the documents with respect to the query and keep only the top k_threshold</span>
                <span class="n">compressor</span> <span class="o">=</span> <span class="n">CrossEncoderReranker</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="n">k_threshold</span><span class="p">)</span>
                <span class="n">compression_retriever</span> <span class="o">=</span> <span class="n">ContextualCompressionRetriever</span><span class="p">(</span><span class="n">base_compressor</span><span class="o">=</span><span class="n">compressor</span><span class="p">,</span> <span class="n">base_retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">)</span>
                <span class="n">retrieved_chunks</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span> <span class="o">=</span> <span class="n">compression_retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Trigger search and retrieve the top k docs</span>
                <span class="n">retrieved_chunks</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k_threshold</span><span class="p">)</span>
    
            <span class="c1"># Calculate precision @k and recall@k for this question</span>
            <span class="n">n_relevant</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">chunks</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">doc_provenance</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">chunks</span> <span class="ow">in</span> <span class="n">retrieved_chunks</span><span class="p">]</span>
            <span class="n">precision</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">n_relevant</span><span class="p">)</span><span class="o">/</span><span class="n">k_threshold</span>
            <span class="n">precision_q_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>
            
            <span class="n">total_relevant_chunks</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vector_store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">where</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;doc_name&quot;</span><span class="p">:</span> <span class="n">doc_provenance</span><span class="p">},</span> <span class="n">include</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">])[</span><span class="s2">&quot;documents&quot;</span><span class="p">])</span>
            <span class="n">recall_q_i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">n_relevant</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_relevant_chunks</span><span class="p">)</span>
    
        <span class="c1"># The precision achieved over all the set of questions for this level of k</span>
        <span class="n">macro_precision_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">precision_q_i</span><span class="p">)</span>
        <span class="n">macro_recall_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recall_q_i</span><span class="p">)</span>
        <span class="n">precision_k</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">macro_precision_k</span><span class="p">)</span>
        <span class="n">recall_k</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">macro_recall_k</span><span class="p">)</span>
        <span class="n">f1_k</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">macro_precision_k</span> <span class="o">*</span> <span class="n">macro_recall_k</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">macro_recall_k</span> <span class="o">+</span> <span class="n">macro_precision_k</span><span class="p">))</span>
        
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span> <span class="s2">&quot;precision_k&quot;</span><span class="p">:</span> <span class="n">precision_k</span><span class="p">,</span> <span class="s2">&quot;recall_k&quot;</span><span class="p">:</span> <span class="n">recall_k</span><span class="p">,</span> <span class="s2">&quot;f1_k&quot;</span><span class="p">:</span> <span class="n">f1_k</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">k_thresh_tests</span> <span class="o">=</span> <span class="n">optimize_retriever</span><span class="p">(</span><span class="n">vector_store</span> <span class="o">=</span> <span class="n">hpc_store</span><span class="p">,</span> <span class="n">qa_set</span> <span class="o">=</span> <span class="n">qa_set</span><span class="p">,</span> <span class="n">max_k</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">reranker_name</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-07-28 16:42:10.221331 - Testing k_threshold 1
2025-07-28 16:42:11.520177 - Testing k_threshold 2
2025-07-28 16:42:12.795102 - Testing k_threshold 3
2025-07-28 16:42:14.068262 - Testing k_threshold 4
2025-07-28 16:42:15.346388 - Testing k_threshold 5
2025-07-28 16:42:16.628977 - Testing k_threshold 6
2025-07-28 16:42:17.911770 - Testing k_threshold 7
2025-07-28 16:42:19.195639 - Testing k_threshold 8
2025-07-28 16:42:20.481491 - Testing k_threshold 9
2025-07-28 16:42:21.772858 - Testing k_threshold 10
2025-07-28 16:42:23.061653 - Testing k_threshold 11
2025-07-28 16:42:24.353821 - Testing k_threshold 12
2025-07-28 16:42:25.652520 - Testing k_threshold 13
2025-07-28 16:42:26.951166 - Testing k_threshold 14
2025-07-28 16:42:28.250966 - Testing k_threshold 15
2025-07-28 16:42:29.554705 - Testing k_threshold 16
2025-07-28 16:42:30.856730 - Testing k_threshold 17
2025-07-28 16:42:32.163973 - Testing k_threshold 18
2025-07-28 16:42:33.472672 - Testing k_threshold 19
2025-07-28 16:42:34.779222 - Testing k_threshold 20
CPU times: user 21.3 s, sys: 1.47 s, total: 22.8 s
Wall time: 25.9 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">k_thresh_tests</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;precision_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;precision_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;recall_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;recall_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;f1_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;f1_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;K threshold tests&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k</th>
      <th>precision_k</th>
      <th>recall_k</th>
      <th>f1_k</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.925373</td>
      <td>0.562928</td>
      <td>0.700017</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.619403</td>
      <td>0.670902</td>
      <td>0.644125</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0.462687</td>
      <td>0.701886</td>
      <td>0.557721</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0.380597</td>
      <td>0.745307</td>
      <td>0.503883</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0.319403</td>
      <td>0.757492</td>
      <td>0.449339</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>0.281095</td>
      <td>0.774453</td>
      <td>0.412477</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>0.251599</td>
      <td>0.783795</td>
      <td>0.380922</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>0.225746</td>
      <td>0.791868</td>
      <td>0.351334</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>0.203980</td>
      <td>0.794103</td>
      <td>0.324585</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>0.188060</td>
      <td>0.797382</td>
      <td>0.304342</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>0.176391</td>
      <td>0.807090</td>
      <td>0.289509</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>0.165423</td>
      <td>0.815928</td>
      <td>0.275076</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>0.154994</td>
      <td>0.817955</td>
      <td>0.260606</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>0.143923</td>
      <td>0.817955</td>
      <td>0.244777</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>0.135323</td>
      <td>0.820940</td>
      <td>0.232347</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>0.131530</td>
      <td>0.845449</td>
      <td>0.227644</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>0.123793</td>
      <td>0.845449</td>
      <td>0.215964</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>0.119403</td>
      <td>0.849974</td>
      <td>0.209391</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>0.114690</td>
      <td>0.851800</td>
      <td>0.202160</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>0.108955</td>
      <td>0.851800</td>
      <td>0.193198</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../../../_images/5ab38eee8eeb67ac77694d569de7801d7e4cc101766e31ee19ba60eeb01b3b07.png" src="../../../../_images/5ab38eee8eeb67ac77694d569de7801d7e4cc101766e31ee19ba60eeb01b3b07.png" />
</div>
</div>
<p>The higher the number of chunks, the lower the precision - this because the large majority of the docs have 1 chunk at most - but the recall raises because for docs with more chunks we leave out a smaller fraction of (possibly) relevant chunks.</p>
<p>We can try to use a reranker to see if our Precision&#64;K and Recall&#64;K improves.</p>
</section>
<section id="reranking-models">
<h2>Reranking models<a class="headerlink" href="#reranking-models" title="Link to this heading"></a></h2>
<p>A reranker is model specifically trained to rank the documents based on a query. In this notebook we are going to use a <strong>cross encoder</strong>. Cross encoders are models that process a query and a document <strong>jointly</strong> and output a relevance score for each document.</p>
<p>The standard semantic search approach uses bi-encoders, where <strong>the query and documents are encoded independently into fixed-size embeddings</strong>. The embeddings are then compared using some similarity measures. While bi-encoders are much faster at retrieval time (since document embeddings can be precomputed and stored in a vector database), they <strong>often sacrifice accuracy due to the lack of interaction between the query and document during encoding</strong>.</p>
<p>By <strong>joint encoding the query with each chunk</strong>, the model should capture <strong>fine-grained interactions between tokens in the query and the document</strong>, leading to accurate relevance judgments.</p>
<p>Let’s examine how our metrics change after applying reranking.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># Reranking with this token config requires 13minutes to complete all the tests, we can skip it and jump to the charts</span>
<span class="n">k_thresh_tests_reranked</span> <span class="o">=</span> <span class="n">optimize_retriever</span><span class="p">(</span><span class="n">vector_store</span> <span class="o">=</span> <span class="n">hpc_store</span><span class="p">,</span> <span class="n">qa_set</span> <span class="o">=</span> <span class="n">qa_set</span><span class="p">,</span> <span class="n">max_k</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">reranker_name</span> <span class="o">=</span> <span class="n">RERANKER</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-07-28 16:42:42.518987 - Testing k_threshold 1
2025-07-28 16:43:22.770098 - Testing k_threshold 2
2025-07-28 16:44:02.936232 - Testing k_threshold 3
2025-07-28 16:44:43.098456 - Testing k_threshold 4
2025-07-28 16:45:23.259636 - Testing k_threshold 5
2025-07-28 16:46:03.418688 - Testing k_threshold 6
2025-07-28 16:46:43.583112 - Testing k_threshold 7
2025-07-28 16:47:23.741345 - Testing k_threshold 8
2025-07-28 16:48:03.904720 - Testing k_threshold 9
2025-07-28 16:48:44.060776 - Testing k_threshold 10
2025-07-28 16:49:24.221430 - Testing k_threshold 11
2025-07-28 16:50:04.387229 - Testing k_threshold 12
2025-07-28 16:50:44.554619 - Testing k_threshold 13
2025-07-28 16:51:24.726631 - Testing k_threshold 14
2025-07-28 16:52:05.094475 - Testing k_threshold 15
2025-07-28 16:52:45.461225 - Testing k_threshold 16
2025-07-28 16:53:25.663023 - Testing k_threshold 17
2025-07-28 16:54:05.826152 - Testing k_threshold 18
2025-07-28 16:54:45.985537 - Testing k_threshold 19
2025-07-28 16:55:26.155932 - Testing k_threshold 20
CPU times: user 13min 35s, sys: 3.51 s, total: 13min 38s
Wall time: 13min 30s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">k_thresh_tests_reranked</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;precision_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;precision_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;recall_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;recall_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;f1_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;f1_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;K threshold tests&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k</th>
      <th>precision_k</th>
      <th>recall_k</th>
      <th>f1_k</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.895522</td>
      <td>0.519004</td>
      <td>0.657153</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.574627</td>
      <td>0.628471</td>
      <td>0.600344</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0.452736</td>
      <td>0.692863</td>
      <td>0.547633</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0.373134</td>
      <td>0.718776</td>
      <td>0.491249</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0.319403</td>
      <td>0.742097</td>
      <td>0.446591</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>0.281095</td>
      <td>0.761745</td>
      <td>0.410653</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>0.247335</td>
      <td>0.765224</td>
      <td>0.373838</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>0.222015</td>
      <td>0.771698</td>
      <td>0.344825</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>0.203980</td>
      <td>0.782543</td>
      <td>0.323608</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>0.186567</td>
      <td>0.786033</td>
      <td>0.301559</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>0.175034</td>
      <td>0.796974</td>
      <td>0.287030</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>0.162935</td>
      <td>0.804934</td>
      <td>0.271012</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>0.154994</td>
      <td>0.815016</td>
      <td>0.260457</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>0.148188</td>
      <td>0.823261</td>
      <td>0.251165</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>0.138308</td>
      <td>0.823261</td>
      <td>0.236829</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>0.131530</td>
      <td>0.833709</td>
      <td>0.227213</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>0.123793</td>
      <td>0.833709</td>
      <td>0.215576</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>0.118574</td>
      <td>0.844903</td>
      <td>0.207962</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>0.113119</td>
      <td>0.846146</td>
      <td>0.199559</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>0.108955</td>
      <td>0.851800</td>
      <td>0.193198</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../../../_images/90a5a05f22629f66a3c4f2837f66d68968c21ecc8b3a07c0fd825ba955eb2006.png" src="../../../../_images/90a5a05f22629f66a3c4f2837f66d68968c21ecc8b3a07c0fd825ba955eb2006.png" />
</div>
</div>
<p>Let’s plot the difference in performance metrics between the ranked version and the non reranked version.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;precision_k&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;precision_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;precision_k - diff&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;recall_k&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;recall_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;recall_k - diff&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_reranked</span><span class="p">[</span><span class="s2">&quot;f1_k&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;f1_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;f1_k - diff&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;K threshold tests (reranking improvement)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/3d93318f2e3e9ad85ecc224fc0c814e4c85686e0ed9b5e0c244a8f80f77134ea.png" src="../../../../_images/3d93318f2e3e9ad85ecc224fc0c814e4c85686e0ed9b5e0c244a8f80f77134ea.png" />
</div>
</div>
<p>Considering the lage majority of documents are short and many of them have 1 or two chunks at most, reranking does not appear to offer significant benefits for this particular document collection.<br />
However, in scenarios involving multiple document sources and a larger set of relevant candidates, reranking becomes a highly effective technique for identifying and retaining only the most relevant chunks.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-final-retriever">
<h1>The final retriever<a class="headerlink" href="#the-final-retriever" title="Link to this heading"></a></h1>
<p>We are ready to test the complete solution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SemanticRetriever</span><span class="p">():</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_k</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">collection_name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">chroma_path</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">embedder_name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">reranker_name</span><span class="p">:</span><span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reranker_name</span> <span class="o">=</span> <span class="n">reranker_name</span>
        
        <span class="n">lc_embedder</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">embedder_name</span><span class="p">)</span>
        <span class="n">vector_store</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">collection_name</span> <span class="o">=</span> <span class="n">collection_name</span><span class="p">,</span> <span class="n">embedding_function</span> <span class="o">=</span> <span class="n">lc_embedder</span><span class="p">,</span> <span class="n">persist_directory</span> <span class="o">=</span> <span class="n">chroma_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span><span class="mi">20</span><span class="p">})</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reranker_name</span><span class="p">:</span>
            <span class="n">reranker</span> <span class="o">=</span> <span class="n">HuggingFaceCrossEncoder</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">reranker_name</span><span class="p">)</span>
            <span class="n">compressor</span> <span class="o">=</span> <span class="n">CrossEncoderReranker</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">reranker</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compression_retriever</span> <span class="o">=</span> <span class="n">ContextualCompressionRetriever</span><span class="p">(</span><span class="n">base_compressor</span><span class="o">=</span><span class="n">compressor</span><span class="p">,</span> <span class="n">base_retriever</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">retriever</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate_answ</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">llm</span><span class="p">:</span><span class="n">ChatOpenAI</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reranker_name</span><span class="p">:</span>
            <span class="n">retrieved_chunks</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compression_retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">retrieved_chunks</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;[USER_QUERY]:</span><span class="se">\n\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">query</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">[RETRIEVED_RESOURCES]:</span><span class="se">\n\n</span><span class="s2">&quot;</span> 
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">retrieved_chunks</span><span class="p">:</span>
            <span class="n">page_content</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">page_content</span>
            <span class="n">doc_name</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">]</span>
            <span class="n">query</span> <span class="o">+=</span> <span class="s2">&quot;[DOCUMENT_TITLE]: &quot;</span> <span class="o">+</span> <span class="n">doc_name</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[DOCUMENT_CONTENT]:&quot;</span> <span class="o">+</span> <span class="n">page_content</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="k">return</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">([(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;You are an helpful assistant, answer to the user questions in a precise and concise manner.&quot;</span><span class="p">),(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="n">query</span><span class="p">)])</span>

<span class="n">semantic_retriever</span> <span class="o">=</span> <span class="n">SemanticRetriever</span><span class="p">(</span><span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;hpc_wiki&quot;</span><span class="p">,</span> <span class="n">chroma_path</span> <span class="o">=</span> <span class="n">chroma_path</span><span class="p">,</span> <span class="n">embedder_name</span> <span class="o">=</span> <span class="n">EMBEDDER</span><span class="p">,</span> <span class="n">reranker_name</span> <span class="o">=</span> <span class="n">RERANKER</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="let-s-test-the-system">
<h2>Let’s test the system<a class="headerlink" href="#let-s-test-the-system" title="Link to this heading"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">test_questions</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[QUESTION]: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">answ</span> <span class="o">=</span> <span class="n">semantic_retriever</span><span class="o">.</span><span class="n">generate_answ</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[ANSWER]: &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">answ</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[QUESTION]: What GPUs are available on Leonardo?
[ANSWER]: The GPUs available on the Leonardo supercomputer are NVIDIA Ampere A100-64 accelerators, which are part of the **Booster** partition.

[QUESTION]: Is there any partition without gpus?
[ANSWER]: Yes, there is a partition without GPUs. The &quot;lrd_all_serial&quot; partition on the Leonardo system does not have GPUs. It is designed for serial jobs with a maximum of 4 cores per job and a walltime limit of 4 hours.

[QUESTION]: What GPUs are available on the Cloud?
[ANSWER]: The available GPUs on the Cloud are:

- Nvidia A30
- Nvidia L40s
- Nvidia H100 NVL

[QUESTION]: Can I associate a domain name to a vm?
[ANSWER]: Yes, you can associate a domain name to a virtual machine (VM). To do this, you need to request a DNS name association from CINECA by sending an email to superc@cineca.it. Here are the key points to follow:

1. **Reverse DNS (PTR Record)**: The reverse of the Floating IP must be set to the hostname of the VM. The naming convention is:
   - For external users: `&lt;VM-name&gt;.ext.cineca.it`
   - For CINECA staff: `&lt;VM-name&gt;.cineca.it`

2. **A Record**: The A record in the DNS will be set according to the hostname specified.

3. **CNAME Record**: If you need the service to be exposed with a different name, you can request a CNAME record with the chosen name. If no other information is provided, only the A record will be set.

Additionally, you can set up a CNAME with your preferred DNS provider. However, note that if the A record is set on an external DNS, the PTR record cannot be set in CINECA DNS.

[QUESTION]: What are the naming conventions I should follow when asking for a domain name for a vm machine?
[ANSWER]: When asking for a domain name for a VM machine, follow these naming conventions:

1. **For external users**: Use the format `&lt;VM-name&gt;.ext.cineca.it`.
2. **For CINECA staff**: Use the format `&lt;VM-name&gt;.cineca.it`.

Replace `&lt;VM-name&gt;` with a descriptive name for your virtual machine.

Additionally, ensure that:
- The reverse of the Floating IP (PTR record) matches the hostname of the VM.
- The record A in the DNS is set accordingly.
- If you need a different name for the service, request a CNAME with the desired name.

[QUESTION]: What is Cineca AI and how do I enable it?
[ANSWER]: **Cineca AI** is a collection of artificial intelligence packages optimized for Cineca&#39;s clusters. It includes packages like TensorFlow, PyTorch, and XGBoost, among others.

To enable Cineca AI, follow these steps:

1. **Load the deeplrn profile:**
   ```bash
   module load profile/deeplrn
   ```

2. **Check available versions of Cineca AI:**
   ```bash
   module av cineca-ai
   ```

3. **Load the desired version of Cineca AI:**
   ```bash
   module load cineca-ai/&lt;version&gt;
   ```

4. **Verify the installed packages:**
   ```bash
   python -m pip list
   ```

5. **Use a specific package:**
   ```bash
   python -c &quot;import &lt;package&gt;&quot;
   ```

If you need to install additional packages, you can create a personal virtual environment:

1. **Create a virtual environment:**
   ```bash
   module load profile/deeplrn
   module load cineca-ai/&lt;version&gt;
   python -m venv &lt;myvenv&gt; --system-site-packages
   ```

2. **Activate the virtual environment:**
   ```bash
   source &lt;myvenv&gt;/bin/activate
   ```

3. **Install additional packages:**
   ```bash
   pip install &lt;package&gt;
   ```

4. **Deactivate the virtual environment when done:**
   ```bash
   deactivate
   ```

[QUESTION]: What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?
[ANSWER]: The QOS queues available on the Leonardo supercomputer BOOSTER partition are not explicitly listed in the provided documents. However, the documents mention the partitions and some general information about job configuration. For specific QOS queues, you would typically need to refer to the official documentation or support resources provided by CINECA, the organization hosting the Leonardo supercomputer.
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="improving-the-system">
<h1>Improving the system<a class="headerlink" href="#improving-the-system" title="Link to this heading"></a></h1>
<p>Some of the answers above are good, but the one on Leonardo’s QOS is imprecise, let’s check the chunks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpc_leonardo_docs</span> <span class="o">=</span> <span class="n">hpc_store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">where</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;doc_name&quot;</span><span class="p">:</span> <span class="s2">&quot;leonardo.rst.txt&quot;</span><span class="p">},</span> <span class="n">include</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">])</span>
<span class="n">hpc_leonardo_docs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ids&#39;: [&#39;leonardo.rst.txt__0&#39;,
  &#39;leonardo.rst.txt__2145&#39;,
  &#39;leonardo.rst.txt__5018&#39;,
  &#39;leonardo.rst.txt__7317&#39;,
  &#39;leonardo.rst.txt__8013&#39;,
  &#39;leonardo.rst.txt__12453&#39;,
  &#39;leonardo.rst.txt__12671&#39;,
  &#39;leonardo.rst.txt__12699&#39;,
  &#39;leonardo.rst.txt__17224&#39;,
  &#39;leonardo.rst.txt__18311&#39;,
  &#39;leonardo.rst.txt__20840&#39;,
  &#39;leonardo.rst.txt__22356&#39;,
  &#39;leonardo.rst.txt__24686&#39;],
 &#39;embeddings&#39;: None,
 &#39;documents&#39;: [&#39;.. _leonardo_card:\n\nLeonardo\n========\n\nLeonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.\nThis guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.\n\n.. |ico2| image:: img/leonardo_logo.png\n   :height: 55px\n   :class: no-scaled-link\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. \n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:\n\n * login01-ext.leonardo.cineca.it\n * login02-ext.leonardo.cineca.it\n * login05-ext.leonardo.cineca.it\n * login07-ext.leonardo.cineca.it\n\n.. warning::\n    \n    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\nSystem Architecture\n-------------------\n\nThe cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:\n\n* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.\n* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.\n\nThe overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. \n\nThe **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.\nThe **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.\n\nHardware Details\n^^^^^^^^^^^^^^^^\n\n.. tab-set::\n\n    .. tab-item:: Booster&#39;,
  &#39;.. list-table:: \n            :widths: 30 50\n            :header-rows: 1\n\n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2135, Da Vinci single-node GPU\n            * - Racks\n              - 116\n            * - Nodes\n              - 3456\n            * - Processors/node\n              - 1x `Intel Ice Lake Intel Xeon Platinum 8358 &lt;https://www.intel.com/content/www/us/en/products/sku/212282/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 32\n            * - Accelerators/node\n              - 4x `NVIDIA Ampere100 custom &lt;https://doi.org/10.17815/jlsrf-8-186&gt;`_, 64GiB HBM2e NVLink 3.0 (200 GB/s)\n            * - Local Storage/node (tmfs)\n              - (none)\n            * - RAM/node \n              - 512 GiB DDR4 3200 MHz\n            * - Rmax\n              - 241.2 PFlop/s (`top500 &lt;https://www.top500.org/system/180128/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology \n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n    .. tab-item:: DCGP\n\n        .. list-table::\n            :widths: 30 50\n            :header-rows: 1\n            \n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2140 three-node CPU blade\n            * - Racks\n              - 22\n            * - Nodes\n              - 1536\n            * - Processors/node\n              - 2x `Intel Sapphire Rapids Intel Xeon Platinum 8480+ &lt;https://www.intel.com/content/www/us/en/products/sku/231746/intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 112 cores/node\n            * - Accelerators\n              - (none)\n            * - Local Storage/node (tmfs)\n              - 3 TiB\n            * - RAM/node\n              - 512(8x64) GiB DDR5 4800 MHz\n            * - Rmax\n              - 7.84 PFlop/s (`top500 &lt;https://www.top500.org/system/180204/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology\n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n\nFile Systems and Data Managment\n-------------------------------\n\nThe storage organization conforms to **CINECA** infrastructure. General information are reported in :ref:`hpc/hpc_data_storage:File Systems and Data Management` section. In the following, only differences with respect to general behavior are listed and explained.&#39;,
  &quot;.. dropdown:: **$TMPDIR**\n\n * on the local SSD disks on login nodes (14 TB of capacity), mounted as ``/scratch_local`` (``TMPDIR=/scratch_local``). This is a shared area with no quota, remove all the files once they are not requested anymore. A cleaning procedure will be enforced in case of improper use of the area.   \n \n * on the local SSD disks on the serial node (``lrd_all_serial``, 14TB of capacity), managed via the Slurm ``job_container/tmpfs plugin``. This plugin provides a *job-specific*, private temporary file system space, with private instances of ``/tmp`` and ``/dev/shm`` in the job&#39;s user space (``TMPDIR=/tmp``, visible via the command ``df -h``), removed at the end of the serial job. You can request the resource via sbatch directive or srun option ``--gres=tmpfs:XX`` (for instance: ``--gres=tmpfs:200G``), with a maximum of 1 TB for the serial jobs. If not explicitly requested, the ``/tmp`` has the default dimension of 10 GB.\n \n * on the local SSD disks on DCGP nodes (3 TB  of capacity). As for the serial node, the local ``/tmp`` and ``/dev/shm`` areas are managed via plugin, which at the start of the jobs mounts private instances of ``/tmp`` and ``/dev/shm`` in the job&#39;s user space (``TMPDIR=/tmp``, visible via the command ``df -h /tmp``), and unmounts them at the end of the job (all data will be lost). You can request the resource via sbatch directive or srun option ``--gres=tmpfs:XX``, with a maximum of all the available 3 TB for DCGP nodes. As for the serial node, if not explicitly requested, the ``/tmp`` has the default dimension of 10 GB. Please note: for the DCGP jobs the requested amount of ``gres/tmpfs`` resource contributes to the consumed budget, changing the number of accounted equivalent core hours, see the dedicated section on the Accounting.\n \n * on RAM on the diskless booster nodes (with a fixed size of 10 GB, no increase is allowed, and the ``gres/tmpfs`` resource is disabled).\n\nJob Managing and Slurm Partitions \n---------------------------------\n\nIn the following table you can find informations about the Slurm partitions for **Booster** and **DCGP** partitions.  \n\n.. seealso:: \n  Further information about job submission are reported in the general section :ref:`hpc/hpc_scheduler:Scheduler and Job Submission`. \n\n.. tab-set::&quot;,
  &#39;.. tab-item:: Booster\n\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**   | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+=================================+==============+=====================================+\n        | lrd_all_serial | normal             | 4 cores                 | 04:00:00     | 1 node / 4 cores                | 40           | No GPUs&#39;,
  &#39;, Hyperthreading x 2         |\n        |                |                    |                         |              |                                 |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                  |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_usr_prod | normal             | 64 nodes                | 24:00:00     |                                 | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_dbg      | 2 nodes                 | 00:30:00     | 2 nodes / 64 cores / 8 GPUs     | 80           |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_bprod    | min = 65 nodes          | 24:00:00     | 256 nodes                       | 60           |                                     |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    | max = 256 nodes         |              |                                 |              |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_lprod    | 3 nodes                 | 4-00:00:00   | 3 nodes / 12 GPUs               | 40           |                                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_fua_dbg  | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 64 cores / 8 GPUs     | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_fua_prod | normal             | 16 nodes                | 24:00:00     | 4 running jobs per user account | 40           |                                     |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    |                         |              | 32 nodes / 3584 cores           |              |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_fuabprod | min = 17 nodes          | 24:00:00     | 32 nodes / 3584 cores           | 60           | Runs on 49 nodes                    |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    | max = 32 nodes          |              |                                 |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                 | 0            |                                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+&#39;,
  &#39;.. note::\n\n          The partitions: **boost_fua_dbg, boost_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.&#39;,
  &#39;.. tab-item:: DCGP&#39;,
  &#39;+----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**        | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+======================================+==============+=====================================+\n        | lrd_all_serial | normal             | max = 4 cores           | 04:00:00     | 1 node / 4 cores                     | 40           | Hyperthreading x 2                  |\n        |                |                    |                         |              |                                      |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                       |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_usr_prod  | normal             | 16 nodes                | 24:00:00     | 512 nodes per prj. account           | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_dbg       | 2 nodes                 | 00:30:00     | 2 nodes / 224 cores per user account | 80           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_bprod     | min = 17 nodes          | 24:00:00     | 128 nodes per user account           | 60           | GrpTRES = 1536 nodes                |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 128 nodes         |              | 512 nodes per prj. account           |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_lprod     | 3 nodes                 | 4-00:00:00   | 3 nodes / 336 cores per user account | 40           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_dbg   | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 224 cores                  | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_prod  | normal             | 16 nodes                | 24:00:00     |                                      | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+&#39;,
  &#39;|                | dcgp_qos_fuabprod  | min = 17 nodes          | 24:00:00     | 64 nodes / 7168 cores                | 60           | Runs on 130 nodes                   |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 64 nodes          |              |                                      |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                      | 0            |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+&#39;,
  &#39;.. note::\n\n          The partitions: **dcgp_fua_dbg, dcgp_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.\n\nNetwork Architecture\n--------------------\n\n.. raw:: html\n\n  &lt;p&gt;&lt;strong&gt;Leonardo&lt;/strong&gt; features a state-of-the-art interconnect system tailored for high-performance computing (HPC). It delivers &lt;em&gt;low latency&lt;/em&gt; and &lt;em&gt;high bandwidth&lt;/em&gt; by leveraging &lt;strong&gt;NVIDIA Mellanox InfiniBand HDR&lt;/strong&gt; (High Data Rate) technology, powered by &lt;a href=&quot;https://nvdam.widen.net/s/zmbw7rdjml/infiniband-qm8700-datasheet-us-nvidia-1746790-r12-web&quot;&gt;NVIDIA QUANTUM QM8700 Smart Switches&lt;/a&gt;, and a &lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/7885210&quot;&gt;Dragonfly+ topology&lt;/a&gt;&lt;/strong&gt;. Below is an overview of its architecture and key features:&lt;/p&gt;\n\n  &lt;ul&gt;\n    &lt;li&gt;&lt;strong&gt;Hierarchical Cell Structure:&lt;/strong&gt; The system is structured into multiple &lt;em&gt;cells&lt;/em&gt;, each comprising a group of interconnected compute nodes.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Inter-cell Connectivity:&lt;/strong&gt; As illustrated in the figure below, cells are connected via an all-to-all topology. Each pair of distinct cells is linked by 18 independent connections, each passing through a dedicated Layer 2 (L2) switch. This design ensures high availability and reduces congestion.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Intra-cell Topology:&lt;/strong&gt; Inside each cell, a non-blocking two-layer fat-tree topology is used, allowing scalable and efficient intra-cell communication.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;System Composition:&lt;/strong&gt;\n      &lt;ul&gt;\n        &lt;li&gt;19 cells dedicated to the &lt;em&gt;Booster&lt;/em&gt; partition.&lt;/li&gt;\n        &lt;li&gt;2 cells for the &lt;em&gt;DCGP&lt;/em&gt; (Data-Centric General Purpose) partition.&lt;/li&gt;\n        &lt;li&gt;1 hybrid cell with both accelerated (36 Booster nodes) and conventional (288 DCGP nodes) compute resources.&lt;/li&gt;\n        &lt;li&gt;1 cell allocated for management, storage, and login services.&lt;/li&gt;\n      &lt;/ul&gt;\n    &lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Adaptive Routing:&lt;/strong&gt; The network employs adaptive routing, dynamically optimizing data paths to alleviate congestion and maintain performance under load.&lt;/li&gt;\n  &lt;/ul&gt;\n  \n.. figure:: img/leo-net-all2all.png\n   :height: 350px\n   :align: center\n   :class: no-scaled-link\n\n.. image:: img/spacer.png\n   :align: center\n   :class: no-scaled-link\n   \n.. dropdown:: Cell Configuration and Intra-cell Connectivity\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n   .. tab-set::\n\n      .. tab-item:: Booster&#39;,
  &#39;.. raw:: html\n\n          &lt;p&gt;Each Booster cell is composed of:&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;6 Ã\x97 Atos BullSequana XH2000 racks&lt;/strong&gt;, each containing:\n              &lt;ul&gt;\n                &lt;li&gt;3 Ã\x97 Level 2 (L2) switches&lt;/li&gt;\n                &lt;li&gt;3 Ã\x97 Level 1 (L1) switches&lt;/li&gt;\n                &lt;li&gt;30 compute nodes â\x80\x94 each equipped with 4 GPUs, each connected via a dedicated 100 Gbps port&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Total per Booster cell:&lt;/strong&gt; 18 L2 switches, 18 L1 switches, and 180 compute nodes.&lt;/p&gt;\n\n          &lt;h4&gt;Connectivity Overview&lt;/h4&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 2 (L2) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 22 Ã\x97 200 Gbps ports connecting to L2 switches in other cells&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 18 Ã\x97 200 Gbps ports connecting to L1 switches within the cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription:&lt;/strong&gt; 0.8:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 1 (L1) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 18 Ã\x97 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 40 Ã\x97 100 Gbps ports connected to GPUs across 10 compute nodes&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription:&lt;/strong&gt; 1.11:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n        .. figure:: img/leo-net-booster_cell.png\n          :height: 750px\n          :align: center\n      \n      .. tab-item:: DCGP\n        \n        .. raw:: html&#39;,
  &#39;.. raw:: html\n\n          &lt;p&gt;Each DCGP cell is composed of:&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;8 Ã\x97 Atos BullSequana XH2000 racks&lt;/strong&gt;, each containing:\n              &lt;ul&gt;\n                &lt;li&gt;3 or 0 Level 2 (L2) switches&lt;/li&gt;\n                &lt;li&gt;2 Ã\x97 Level 1 (L1) switches&lt;/li&gt;\n                &lt;li&gt;78 compute nodes â\x80\x94 each connected via a dedicated 100 Gbps port&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Total per DCGP cell:&lt;/strong&gt; 18 L2 switches, 16 L1 switches, and 624 compute nodes.&lt;/p&gt;\n\n          &lt;h4&gt;Connectivity Overview&lt;/h4&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 2 (L2) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 22 Ã\x97 200 Gbps ports connecting to L2 switches in other cells&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 18 Ã\x97 200 Gbps ports connecting to L1 switches within the same cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription ratio:&lt;/strong&gt; 0.8:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 1 (L1) Switches:&lt;/strong&gt; (divided into two groups):&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;9 switches with 40 downlinks:&lt;/strong&gt;\n              &lt;ul&gt;\n                &lt;li&gt;UP: 18 Ã\x97 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n                &lt;li&gt;DOWN: 40 Ã\x97 100 Gbps ports connect&lt;/strong&gt;ed to compute nodes&lt;/li&gt;\n                &lt;li&gt;Oversubscription ratio: 1.11:1&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;9 switches with 38 downlinks:&lt;/strong&gt;\n              &lt;ul&gt;\n                &lt;li&gt;UP: 18 Ã\x97 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n                &lt;li&gt;DOWN: 38 Ã\x97 100 Gbps ports connected to compute nodes&lt;/li&gt;\n                &lt;li&gt;Oversubscription ratio: 1.05:1&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n        .. figure:: img/leo-net-dcgp_cell.png\n          :height: 750px\n          :align: center\n\nAdvanced Information\n^^^^^^^^^^^^^^^^^^^^\n\n.. dropdown:: Network Topology - Map\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    The topology is presented in a table format, where each row corresponds to a compute node. For each node, the table specifies the associated L1 switch and cell, providing a clear overview of the physical and logical network layout within the cluster.\n\n    :download:`Network Topology - Map &lt;../files/ntopology.dat&gt;`&#39;,
  &#39;.. dropdown:: Network Topology - Distance Matrix\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    The attached compressed CSV file contains the distance matrix of all compute nodes in the cluster. The matrix uses the following metric to represent the network distance between any two nodes:\n\n    * **0** â\x80\x93 Same nodes\n    * **1** â\x80\x93 Same L1 switch, same cell.\n    * **2** â\x80\x93 Different L1 switch, same cell.\n    * **3** â\x80\x93 Different L1 switch and different cell.\n    \n    This matrix can be used to analyze communication locality and optimize node selection for distributed workloads.\n\n    :download:`Distance Matrix &lt;../files/ntopology-dst_mtx.tar.bz2&gt;`\n\n.. dropdown:: Switch Naming Format\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    .. code-block::\n      \n      isw&lt;RRrrSS&gt;\n\n    where ``&lt;RRrrSS&gt;`` is a 5- or 6-digits number varies based on the location and type of the switch.\n\n    Specifically:\n\n    * ``RR`` = region number (1 or 2 digits)\n    * ``rr`` = rack number (2 digits)\n    * ``SS`` = switch id (2 digits)\n\n    .. note::\n      If ``SS`` is an even number, it refers to an L1 switch; if it is an odd number, it refers to an L2 switch.\n\nDocuments\n---------\n\n* Article on Leonardo architecture and the technologies adopted for its GPU-accelerated partition: CINECA Supercomputing Centre, SuperComputing Applications and Innovation Department. (2024). â\x80\x9cLEONARDO: A Pan-European Pre-Exascale Supercomputer for HPC and AI applications.â\x80\x9d, Journal of large-scale research facilities, 8, A186. https://doi.org/10.17815/jlsrf-8-186\n* Details about new technologies included in the Witley platform with Intel Xeon Icelake contained in the Leonardo pre-exascale system (`link &lt;https://urldefense.com/v3/__https://software.intel.com/content/www/us/en/develop/articles/third-generation-xeon-scalable-family-overview.html__;!!P1tgJ-3e!TrmMus5wzdLQ963vkc3yfy0BlhC1Hu8vOoce4SgltsTbkSSDrX2p1zTXPCIrpPm3$&gt;`_)\n* Additional documents (`link &lt;https://urldefense.com/v3/__https://software.intel.com/content/www/us/en/develop/articles/xeon-performance-tuning-and-solution-guides.html__;!!P1tgJ-3e!TrmMus5wzdLQ963vkc3yfy0BlhC1Hu8vOoce4SgltsTbkSSDrX2p1zTXPKZ5awkS$&gt;`_)\n\nSome tuning guides for dedicated enviroments (ML/DL or HPC Clusters):\n\n* :download:`Tuning Guide &lt;../files/Tuning_guide.pdf&gt;`\n\n* :download:`Deep Learning &lt;../files/Deep_learning.pdf&gt;`&#39;],
 &#39;uris&#39;: None,
 &#39;included&#39;: [&#39;documents&#39;],
 &#39;data&#39;: None,
 &#39;metadatas&#39;: None}
</pre></div>
</div>
</div>
</div>
<p>The problem is that for some specific questions we don’t have text associated to a chunk, but we only have “decontestualized” tables, which makes retrieving relevant chunks particularly difficult. Look at this section of our website:</p>
<p><img alt="Leonardo QOS" src="../../../../_images/leonardo_qos.jpeg" /></p>
<p>In fact if we tune our question a bit…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">answ</span> <span class="o">=</span> <span class="n">semantic_retriever</span><span class="o">.</span><span class="n">generate_answ</span><span class="p">(</span><span class="s2">&quot;What are the partitions available in Leonardo BOOST and their associated QOS?&quot;</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">answ</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>### Partitions and Associated QOS in Leonardo BOOST

1. **Booster Partition (GPU)**
   - **Standard QOS**: `boost_fua_prod` (for EUROfusion users) or `boost_usr_prod` (for all users)
   - **Low-priority QOS**: `qos_fualowprio` (for EUROfusion users when budget is exhausted or when using `FUAL8_LOWPRIO` account)

2. **Data Centric General Purpose (DCGP) Partition (CPU)**
   - **Standard QOS**: `dcgp_fua_prod` (for EUROfusion users) or `dcgp_usr_prod` (for all users)
   - **Low-priority QOS**: `qos_fualowprio` (for EUROfusion users when budget is exhausted or when using `FUA38_LOWPRIO_0` account)

### Low-Priority Jobs
- **Budget Exhaustion**: Use `qos_fualowprio` QOS in `boost_fua_prod` and `dcgp_fua_prod` partitions.
- **Without Budget Exhaustion**: Request `FUAL8_LOWPRIO` (Booster) or `FUA38_LOWPRIO_0` (DCGP) accounts and use `qos_fualowprio` QOS.

### Example Submission Script for Low-Priority Jobs
```bash
#SBATCH --account=&lt;YOUR Project Account or LOWPRIO Project Account&gt;
#SBATCH --qos=qos_fualowprio
```

### Additional Information
- **Booster Partition**: Based on NVIDIA Ampere A100-64 accelerators.
- **DCGP Partition**: Based on Intel Sapphire Rapids processors.
- **Network**: NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity.
</pre></div>
</div>
</div>
</div>
<p>A bit better, but the best relevant chunks still miss relevant context and they are not extracted… In fact, as we were saying before, they have NO RELEVANT context at all sorrounding them.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="a-combined-approach">
<h1>A combined approach<a class="headerlink" href="#a-combined-approach" title="Link to this heading"></a></h1>
<p>The simplest solution would be to combine semantic search with an exact or approximate matching approach. In this case, we aim to retrieve chunks that explicitly mention content related to QOS.</p>
<p>Let’s try a classic bag-of-words method. Here, we experiment with using <a class="reference external" href="https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables">BM25</a> to retrieve useful content. BM25 assumes that <strong>the more frequently a term appears in a document, the more relevant the document is</strong> to the query. The <strong>k in the formula ensures that this effect saturates</strong>, so that additional occurrences have diminishing impact. The <strong>score is normalized by document length</strong> to prevent longer documents from being favored. Additionally, <strong>rare terms are given more importance</strong> through the use of the IDF component.</p>
<p><span class="math notranslate nohighlight">\( BM25(Q, D) = \sum_{i=1}^{n} IDF(q_i) * \frac{f(q_i, D) * (k_1 + 1)}{f(q_i, D) + k_1 * (1 - b + b * \frac{| D |}{avgdl})}\)</span></p>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q\)</span> is our query with keywords <span class="math notranslate nohighlight">\(q_1, ..., q_n\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> is a document in our document base;</p></li>
<li><p><span class="math notranslate nohighlight">\(f(q_i, D)\)</span> is the frequency of <span class="math notranslate nohighlight">\(q_i\)</span> in document <span class="math notranslate nohighlight">\(D\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(|D|\)</span> is the document length;</p></li>
<li><p><span class="math notranslate nohighlight">\( avgdl \)</span> is the average document length in the document collection;</p></li>
<li><p><span class="math notranslate nohighlight">\(k_1\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are hyperparams with values usually set as <span class="math notranslate nohighlight">\( b = 2 \)</span> and <span class="math notranslate nohighlight">\( k_1 \in [1.2, 2]\)</span>. <span class="math notranslate nohighlight">\(k\)</span> controls the therm frequency component, <span class="math notranslate nohighlight">\(b\)</span> the document length component;</p></li>
<li><p><span class="math notranslate nohighlight">\(IDF(q_i)\)</span> is the inverse document frequency;</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(IDF(q_i) = ln(\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1)\)</span></p>
<p>With:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> total number of documents;</p></li>
<li><p><span class="math notranslate nohighlight">\(n(q_i)\)</span> total number of documents containing the term <span class="math notranslate nohighlight">\(q_i\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedder_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">EMBEDDER</span><span class="p">)</span>
<span class="n">tokenized_chunks</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedder_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">page_chunk</span><span class="p">,</span> <span class="n">add_special_tokens</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">page_chunk</span> <span class="ow">in</span> <span class="n">hpc_leonardo_docs</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]]</span>

<span class="n">bm_25_retriever</span> <span class="o">=</span> <span class="n">BM25Okapi</span><span class="p">(</span><span class="n">tokenized_chunks</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_colwidth</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?&quot;</span>
<span class="n">tokenized_query</span> <span class="o">=</span> <span class="n">embedder_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">add_special_tokens</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">bm_25_retriever</span><span class="o">.</span><span class="n">get_scores</span><span class="p">(</span><span class="n">tokenized_query</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;scores&quot;</span><span class="p">:</span> <span class="n">scores</span><span class="p">,</span> <span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">hpc_leonardo_docs</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]})</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;scores&quot;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>scores</th>
      <th>documents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>13.961787</td>
      <td>.. _leonardo_card:\n\nLeonardo\n========\n\nLeonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.\nThis guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.\n\n.. |ico2| image:: img/leonardo_logo.png\n   :height: 55px\n   :class: no-scaled-link\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. \n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:\n\n * login01-ext.leonardo.cineca.it\n * login02-ext.leonardo.cineca.it\n * login05-ext.leonardo.cineca.it\n * login07-ext.leonardo.cineca.it\n\n.. warning::\n    \n    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\nSystem Architecture\n-------------------\n\nThe cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:\n\n* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.\n* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.\n\nThe overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. \n\nThe **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.\nThe **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.\n\nHardware Details\n^^^^^^^^^^^^^^^^\n\n.. tab-set::\n\n    .. tab-item:: Booster</td>
    </tr>
    <tr>
      <th>12</th>
      <td>10.287257</td>
      <td>.. dropdown:: Network Topology - Distance Matrix\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    The attached compressed CSV file contains the distance matrix of all compute nodes in the cluster. The matrix uses the following metric to represent the network distance between any two nodes:\n\n    * **0** â Same nodes\n    * **1** â Same L1 switch, same cell.\n    * **2** â Different L1 switch, same cell.\n    * **3** â Different L1 switch and different cell.\n    \n    This matrix can be used to analyze communication locality and optimize node selection for distributed workloads.\n\n    :download:`Distance Matrix &lt;../files/ntopology-dst_mtx.tar.bz2&gt;`\n\n.. dropdown:: Switch Naming Format\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    .. code-block::\n      \n      isw&lt;RRrrSS&gt;\n\n    where ``&lt;RRrrSS&gt;`` is a 5- or 6-digits number varies based on the location and type of the switch.\n\n    Specifically:\n\n    * ``RR`` = region number (1 or 2 digits)\n    * ``rr`` = rack number (2 digits)\n    * ``SS`` = switch id (2 digits)\n\n    .. note::\n      If ``SS`` is an even number, it refers to an L1 switch; if it is an odd number, it refers to an L2 switch.\n\nDocuments\n---------\n\n* Article on Leonardo architecture and the technologies adopted for its GPU-accelerated partition: CINECA Supercomputing Centre, SuperComputing Applications and Innovation Department. (2024). âLEONARDO: A Pan-European Pre-Exascale Supercomputer for HPC and AI applications.â, Journal of large-scale research facilities, 8, A186. https://doi.org/10.17815/jlsrf-8-186\n* Details about new technologies included in the Witley platform with Intel Xeon Icelake contained in the Leonardo pre-exascale system (`link &lt;https://urldefense.com/v3/__https://software.intel.com/content/www/us/en/develop/articles/third-generation-xeon-scalable-family-overview.html__;!!P1tgJ-3e!TrmMus5wzdLQ963vkc3yfy0BlhC1Hu8vOoce4SgltsTbkSSDrX2p1zTXPCIrpPm3$&gt;`_)\n* Additional documents (`link &lt;https://urldefense.com/v3/__https://software.intel.com/content/www/us/en/develop/articles/xeon-performance-tuning-and-solution-guides.html__;!!P1tgJ-3e!TrmMus5wzdLQ963vkc3yfy0BlhC1Hu8vOoce4SgltsTbkSSDrX2p1zTXPKZ5awkS$&gt;`_)\n\nSome tuning guides for dedicated enviroments (ML/DL or HPC Clusters):\n\n* :download:`Tuning Guide &lt;../files/Tuning_guide.pdf&gt;`\n\n* :download:`Deep Learning &lt;../files/Deep_learning.pdf&gt;`</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.239758</td>
      <td>.. dropdown:: **$TMPDIR**\n\n * on the local SSD disks on login nodes (14 TB of capacity), mounted as ``/scratch_local`` (``TMPDIR=/scratch_local``). This is a shared area with no quota, remove all the files once they are not requested anymore. A cleaning procedure will be enforced in case of improper use of the area.   \n \n * on the local SSD disks on the serial node (``lrd_all_serial``, 14TB of capacity), managed via the Slurm ``job_container/tmpfs plugin``. This plugin provides a *job-specific*, private temporary file system space, with private instances of ``/tmp`` and ``/dev/shm`` in the job's user space (``TMPDIR=/tmp``, visible via the command ``df -h``), removed at the end of the serial job. You can request the resource via sbatch directive or srun option ``--gres=tmpfs:XX`` (for instance: ``--gres=tmpfs:200G``), with a maximum of 1 TB for the serial jobs. If not explicitly requested, the ``/tmp`` has the default dimension of 10 GB.\n \n * on the local SSD disks on DCGP nodes (3 TB  of capacity). As for the serial node, the local ``/tmp`` and ``/dev/shm`` areas are managed via plugin, which at the start of the jobs mounts private instances of ``/tmp`` and ``/dev/shm`` in the job's user space (``TMPDIR=/tmp``, visible via the command ``df -h /tmp``), and unmounts them at the end of the job (all data will be lost). You can request the resource via sbatch directive or srun option ``--gres=tmpfs:XX``, with a maximum of all the available 3 TB for DCGP nodes. As for the serial node, if not explicitly requested, the ``/tmp`` has the default dimension of 10 GB. Please note: for the DCGP jobs the requested amount of ``gres/tmpfs`` resource contributes to the consumed budget, changing the number of accounted equivalent core hours, see the dedicated section on the Accounting.\n \n * on RAM on the diskless booster nodes (with a fixed size of 10 GB, no increase is allowed, and the ``gres/tmpfs`` resource is disabled).\n\nJob Managing and Slurm Partitions \n---------------------------------\n\nIn the following table you can find informations about the Slurm partitions for **Booster** and **DCGP** partitions.  \n\n.. seealso:: \n  Further information about job submission are reported in the general section :ref:`hpc/hpc_scheduler:Scheduler and Job Submission`. \n\n.. tab-set::</td>
    </tr>
    <tr>
      <th>9</th>
      <td>4.020692</td>
      <td>.. note::\n\n          The partitions: **dcgp_fua_dbg, dcgp_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.\n\nNetwork Architecture\n--------------------\n\n.. raw:: html\n\n  &lt;p&gt;&lt;strong&gt;Leonardo&lt;/strong&gt; features a state-of-the-art interconnect system tailored for high-performance computing (HPC). It delivers &lt;em&gt;low latency&lt;/em&gt; and &lt;em&gt;high bandwidth&lt;/em&gt; by leveraging &lt;strong&gt;NVIDIA Mellanox InfiniBand HDR&lt;/strong&gt; (High Data Rate) technology, powered by &lt;a href="https://nvdam.widen.net/s/zmbw7rdjml/infiniband-qm8700-datasheet-us-nvidia-1746790-r12-web"&gt;NVIDIA QUANTUM QM8700 Smart Switches&lt;/a&gt;, and a &lt;strong&gt;&lt;a href="https://ieeexplore.ieee.org/document/7885210"&gt;Dragonfly+ topology&lt;/a&gt;&lt;/strong&gt;. Below is an overview of its architecture and key features:&lt;/p&gt;\n\n  &lt;ul&gt;\n    &lt;li&gt;&lt;strong&gt;Hierarchical Cell Structure:&lt;/strong&gt; The system is structured into multiple &lt;em&gt;cells&lt;/em&gt;, each comprising a group of interconnected compute nodes.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Inter-cell Connectivity:&lt;/strong&gt; As illustrated in the figure below, cells are connected via an all-to-all topology. Each pair of distinct cells is linked by 18 independent connections, each passing through a dedicated Layer 2 (L2) switch. This design ensures high availability and reduces congestion.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Intra-cell Topology:&lt;/strong&gt; Inside each cell, a non-blocking two-layer fat-tree topology is used, allowing scalable and efficient intra-cell communication.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;System Composition:&lt;/strong&gt;\n      &lt;ul&gt;\n        &lt;li&gt;19 cells dedicated to the &lt;em&gt;Booster&lt;/em&gt; partition.&lt;/li&gt;\n        &lt;li&gt;2 cells for the &lt;em&gt;DCGP&lt;/em&gt; (Data-Centric General Purpose) partition.&lt;/li&gt;\n        &lt;li&gt;1 hybrid cell with both accelerated (36 Booster nodes) and conventional (288 DCGP nodes) compute resources.&lt;/li&gt;\n        &lt;li&gt;1 cell allocated for management, storage, and login services.&lt;/li&gt;\n      &lt;/ul&gt;\n    &lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Adaptive Routing:&lt;/strong&gt; The network employs adaptive routing, dynamically optimizing data paths to alleviate congestion and maintain performance under load.&lt;/li&gt;\n  &lt;/ul&gt;\n  \n.. figure:: img/leo-net-all2all.png\n   :height: 350px\n   :align: center\n   :class: no-scaled-link\n\n.. image:: img/spacer.png\n   :align: center\n   :class: no-scaled-link\n   \n.. dropdown:: Cell Configuration and Intra-cell Connectivity\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n   .. tab-set::\n\n      .. tab-item:: Booster</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2.786747</td>
      <td>.. note::\n\n          The partitions: **boost_fua_dbg, boost_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.784774</td>
      <td>.. tab-item:: Booster\n\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**   | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+=================================+==============+=====================================+\n        | lrd_all_serial | normal             | 4 cores                 | 04:00:00     | 1 node / 4 cores                | 40           | No GPUs</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.641071</td>
      <td>.. list-table:: \n            :widths: 30 50\n            :header-rows: 1\n\n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2135, Da Vinci single-node GPU\n            * - Racks\n              - 116\n            * - Nodes\n              - 3456\n            * - Processors/node\n              - 1x `Intel Ice Lake Intel Xeon Platinum 8358 &lt;https://www.intel.com/content/www/us/en/products/sku/212282/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 32\n            * - Accelerators/node\n              - 4x `NVIDIA Ampere100 custom &lt;https://doi.org/10.17815/jlsrf-8-186&gt;`_, 64GiB HBM2e NVLink 3.0 (200 GB/s)\n            * - Local Storage/node (tmfs)\n              - (none)\n            * - RAM/node \n              - 512 GiB DDR4 3200 MHz\n            * - Rmax\n              - 241.2 PFlop/s (`top500 &lt;https://www.top500.org/system/180128/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology \n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n    .. tab-item:: DCGP\n\n        .. list-table::\n            :widths: 30 50\n            :header-rows: 1\n            \n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2140 three-node CPU blade\n            * - Racks\n              - 22\n            * - Nodes\n              - 1536\n            * - Processors/node\n              - 2x `Intel Sapphire Rapids Intel Xeon Platinum 8480+ &lt;https://www.intel.com/content/www/us/en/products/sku/231746/intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 112 cores/node\n            * - Accelerators\n              - (none)\n            * - Local Storage/node (tmfs)\n              - 3 TiB\n            * - RAM/node\n              - 512(8x64) GiB DDR5 4800 MHz\n            * - Rmax\n              - 7.84 PFlop/s (`top500 &lt;https://www.top500.org/system/180204/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology\n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n\nFile Systems and Data Managment\n-------------------------------\n\nThe storage organization conforms to **CINECA** infrastructure. General information are reported in :ref:`hpc/hpc_data_storage:File Systems and Data Management` section. In the following, only differences with respect to general behavior are listed and explained.</td>
    </tr>
    <tr>
      <th>11</th>
      <td>2.631267</td>
      <td>.. raw:: html\n\n          &lt;p&gt;Each DCGP cell is composed of:&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;8 Ã Atos BullSequana XH2000 racks&lt;/strong&gt;, each containing:\n              &lt;ul&gt;\n                &lt;li&gt;3 or 0 Level 2 (L2) switches&lt;/li&gt;\n                &lt;li&gt;2 Ã Level 1 (L1) switches&lt;/li&gt;\n                &lt;li&gt;78 compute nodes â each connected via a dedicated 100 Gbps port&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Total per DCGP cell:&lt;/strong&gt; 18 L2 switches, 16 L1 switches, and 624 compute nodes.&lt;/p&gt;\n\n          &lt;h4&gt;Connectivity Overview&lt;/h4&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 2 (L2) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 22 Ã 200 Gbps ports connecting to L2 switches in other cells&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 18 Ã 200 Gbps ports connecting to L1 switches within the same cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription ratio:&lt;/strong&gt; 0.8:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 1 (L1) Switches:&lt;/strong&gt; (divided into two groups):&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;9 switches with 40 downlinks:&lt;/strong&gt;\n              &lt;ul&gt;\n                &lt;li&gt;UP: 18 Ã 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n                &lt;li&gt;DOWN: 40 Ã 100 Gbps ports connect&lt;/strong&gt;ed to compute nodes&lt;/li&gt;\n                &lt;li&gt;Oversubscription ratio: 1.11:1&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;9 switches with 38 downlinks:&lt;/strong&gt;\n              &lt;ul&gt;\n                &lt;li&gt;UP: 18 Ã 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n                &lt;li&gt;DOWN: 38 Ã 100 Gbps ports connected to compute nodes&lt;/li&gt;\n                &lt;li&gt;Oversubscription ratio: 1.05:1&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n        .. figure:: img/leo-net-dcgp_cell.png\n          :height: 750px\n          :align: center\n\nAdvanced Information\n^^^^^^^^^^^^^^^^^^^^\n\n.. dropdown:: Network Topology - Map\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    The topology is presented in a table format, where each row corresponds to a compute node. For each node, the table specifies the associated L1 switch and cell, providing a clear overview of the physical and logical network layout within the cluster.\n\n    :download:`Network Topology - Map &lt;../files/ntopology.dat&gt;`</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1.973369</td>
      <td>.. raw:: html\n\n          &lt;p&gt;Each Booster cell is composed of:&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;6 Ã Atos BullSequana XH2000 racks&lt;/strong&gt;, each containing:\n              &lt;ul&gt;\n                &lt;li&gt;3 Ã Level 2 (L2) switches&lt;/li&gt;\n                &lt;li&gt;3 Ã Level 1 (L1) switches&lt;/li&gt;\n                &lt;li&gt;30 compute nodes â each equipped with 4 GPUs, each connected via a dedicated 100 Gbps port&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Total per Booster cell:&lt;/strong&gt; 18 L2 switches, 18 L1 switches, and 180 compute nodes.&lt;/p&gt;\n\n          &lt;h4&gt;Connectivity Overview&lt;/h4&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 2 (L2) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 22 Ã 200 Gbps ports connecting to L2 switches in other cells&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 18 Ã 200 Gbps ports connecting to L1 switches within the cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription:&lt;/strong&gt; 0.8:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 1 (L1) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 18 Ã 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 40 Ã 100 Gbps ports connected to GPUs across 10 compute nodes&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription:&lt;/strong&gt; 1.11:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n        .. figure:: img/leo-net-booster_cell.png\n          :height: 750px\n          :align: center\n      \n      .. tab-item:: DCGP\n        \n        .. raw:: html</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1.916602</td>
      <td>+----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**        | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+======================================+==============+=====================================+\n        | lrd_all_serial | normal             | max = 4 cores           | 04:00:00     | 1 node / 4 cores                     | 40           | Hyperthreading x 2                  |\n        |                |                    |                         |              |                                      |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                       |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_usr_prod  | normal             | 16 nodes                | 24:00:00     | 512 nodes per prj. account           | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_dbg       | 2 nodes                 | 00:30:00     | 2 nodes / 224 cores per user account | 80           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_bprod     | min = 17 nodes          | 24:00:00     | 128 nodes per user account           | 60           | GrpTRES = 1536 nodes                |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 128 nodes         |              | 512 nodes per prj. account           |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_lprod     | 3 nodes                 | 4-00:00:00   | 3 nodes / 336 cores per user account | 40           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_dbg   | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 224 cores                  | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_prod  | normal             | 16 nodes                | 24:00:00     |                                      | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.592530</td>
      <td>|                | dcgp_qos_fuabprod  | min = 17 nodes          | 24:00:00     | 64 nodes / 7168 cores                | 60           | Runs on 130 nodes                   |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 64 nodes          |              |                                      |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                      | 0            |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.527483</td>
      <td>, Hyperthreading x 2         |\n        |                |                    |                         |              |                                 |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                  |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_usr_prod | normal             | 64 nodes                | 24:00:00     |                                 | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_dbg      | 2 nodes                 | 00:30:00     | 2 nodes / 64 cores / 8 GPUs     | 80           |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_bprod    | min = 65 nodes          | 24:00:00     | 256 nodes                       | 60           |                                     |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    | max = 256 nodes         |              |                                 |              |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_lprod    | 3 nodes                 | 4-00:00:00   | 3 nodes / 12 GPUs               | 40           |                                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_fua_dbg  | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 64 cores / 8 GPUs     | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_fua_prod | normal             | 16 nodes                | 24:00:00     | 4 running jobs per user account | 40           |                                     |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    |                         |              | 32 nodes / 3584 cores           |              |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_fuabprod | min = 17 nodes          | 24:00:00     | 32 nodes / 3584 cores           | 60           | Runs on 49 nodes                    |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    | max = 32 nodes          |              |                                 |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                 | 0            |                                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.000000</td>
      <td>.. tab-item:: DCGP</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Still not good for our use case.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="contextual-retrieval">
<h1>Contextual retrieval<a class="headerlink" href="#contextual-retrieval" title="Link to this heading"></a></h1>
<p>Contextual retrieval is a technique <a class="reference external" href="https://www.anthropic.com/news/contextual-retrieval">introduced</a> by Anthropic which consist in enriching a text chunk with context before generating its embedding.</p>
<p>This context is produced by a large language model (LLM), which takes two inputs: the full document and the specific chunks that need contextualization. The model then returns a description of each chunk’s role within the overall document. The LLM generated description is prepended to the chunk before embedding. This <strong>will cause the embedding to capture not only local content but also higher-level contextual meaning</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Straight from https://www.anthropic.com/news/contextual-retrieval</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_context</span><span class="p">(</span><span class="n">chunk_content</span><span class="p">:</span><span class="n">Document</span><span class="p">,</span> <span class="n">full_document</span><span class="p">:</span><span class="n">Document</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a contextualized chunk.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            - chunk_content: The chunk that must be contextualized;</span>
<span class="sd">            - full_document: The entire langchain document to be used for contextualization;</span>
<span class="sd">            - llm: The llm client used to create the context;</span>


<span class="sd">        Returns:</span>
<span class="sd">            str: A string containing the context of the chunk</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">contextual_retrieval_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;[DOCUMENT]:</span>
<span class="s2">    </span><span class="si">{</span><span class="n">full_document</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span>
<span class="s2">    </span>
<span class="s2">    [DOCUMENT_CHUNK]:</span>
<span class="s2">    </span><span class="si">{</span><span class="n">chunk_content</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span>

<span class="s2">    [TASK]:</span>
<span class="s2">    Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. </span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="n">chunk_context</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;You are an helpful assistant, answer to the user questions in a precise and concise manner.&quot;</span><span class="p">),</span>
                                <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="n">contextual_retrieval_prompt</span><span class="p">)])</span><span class="o">.</span><span class="n">content</span>
    <span class="k">return</span> <span class="n">chunk_context</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">leonardo_doc</span> <span class="o">=</span> <span class="p">[</span><span class="n">document</span> <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span> <span class="k">if</span> <span class="n">document</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;leonardo.rst.txt&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">leonardo_chunks</span> <span class="o">=</span> <span class="p">[</span><span class="n">document</span> <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">chunks</span> <span class="k">if</span> <span class="n">document</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;leonardo.rst.txt&quot;</span><span class="p">]</span>

<span class="c1"># Send up to 12 queries in parallel</span>
<span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">chunk_contexts</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">e</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">generate_context</span><span class="p">,</span> <span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span><span class="p">,</span> <span class="n">full_document</span> <span class="o">=</span> <span class="n">leonardo_doc</span><span class="p">),</span> <span class="n">leonardo_chunks</span><span class="p">)]</span>

<span class="n">chunk_contexts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 32.1 ms, sys: 3.95 ms, total: 36.1 ms
Wall time: 3.43 s
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;This chunk is part of the &quot;Hardware Details&quot; section under &quot;System Architecture&quot; in the Leonardo supercomputer documentation. It provides specific details about the Booster partition, including the types of compute blades, processors, accelerators, and other hardware specifications.&#39;,
 &#39;This chunk provides detailed hardware specifications for the two main partitions of the Leonardo supercomputer: the Booster partition, which is GPU-based, and the DCGP partition, which is CPU-based. It includes information on the models, number of racks and nodes, processors, accelerators, local storage, RAM, peak performance (Rmax), internal network, and storage capacity for each partition.&#39;,
 &#39;This chunk is part of the &quot;File Systems and Data Management&quot; section of the Leonardo supercomputer documentation, specifically detailing the usage and characteristics of the `$TMPDIR` environment variable across different node types.&#39;,
 &#39;This chunk is part of the &quot;Job Managing and Slurm Partitions&quot; section of the Leonardo supercomputer documentation. It specifically details the Slurm partitions available for the Booster partition, including information on Quality of Service (QOS), core/GPU allocation per job, walltime limits, maximum resources per user, priority, and additional notes.&#39;,
 &#39;This chunk is part of the &quot;Job Managing and Slurm Partitions&quot; section of the Leonardo supercomputer documentation. It details the various Slurm partitions available for the Booster partition, including information on Quality of Service (QOS), core/GPU allocation per job, walltime limits, maximum nodes/cores/GPUs per user, priority, and specific notes for each partition.&#39;,
 &#39;This chunk is part of the &quot;Job Managing and Slurm Partitions&quot; section of the Leonardo supercomputer documentation, specifically detailing the partitions available for the Booster partition.&#39;,
 &#39;This chunk is part of the &quot;Hardware Details&quot; section under the &quot;System Architecture&quot; heading in the Leonardo supercomputer documentation. It provides specific details about the Data Centric General Purpose (DCGP) partition of the Leonardo cluster.&#39;,
 &#39;This chunk is part of the &quot;Job Managing and Slurm Partitions&quot; section of the Leonardo supercomputer documentation, specifically detailing the Slurm partitions for the DCGP (Data Centric General Purpose) partition.&#39;,
 &#39;This chunk is part of the &quot;Job Managing and Slurm Partitions&quot; section, specifically detailing the Slurm partitions for the DCGP (Data Centric General Purpose) partition of the Leonardo supercomputer. It provides information on the quality of service (QOS) settings, including the minimum and maximum number of nodes, walltime, and other constraints for different job types within the DCGP partition.&#39;,
 &#39;This chunk is part of the &quot;Network Architecture&quot; section of the Leonardo supercomputer documentation. It describes the advanced interconnect system designed for high-performance computing (HPC), including details on the hierarchical cell structure, inter-cell connectivity, intra-cell topology, system composition, and adaptive routing.&#39;,
 &#39;This chunk is part of the &quot;Network Architecture&quot; section of the Leonardo supercomputer documentation. It details the specific configuration and connectivity of the Booster partition\&#39;s network topology, including the structure of each Booster cell, the number of switches, and the connectivity overview for both Level 2 (L2) and Level 1 (L1) switches.&#39;,
 &#39;This chunk is part of the &quot;Network Architecture&quot; section of the Leonardo supercomputer documentation. It specifically details the configuration and connectivity of the Data Centric General Purpose (DCGP) partition cells, including the number of switches and compute nodes, as well as the oversubscription ratios for the Level 1 and Level 2 switches.&#39;,
 &#39;This chunk is part of the &quot;Network Architecture&quot; section of the Leonardo supercomputer documentation, specifically detailing advanced information about the network topology and switch naming conventions.&#39;]
</pre></div>
</div>
</div>
</div>
<p>Let’s embed the chunks again and check the new similarity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">contextualized_chunks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">leonardo_chunks</span><span class="p">)):</span>
    <span class="n">chunk_content</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;[CONTEXT]:</span>
<span class="s2">    </span><span class="si">{</span><span class="n">chunk_contexts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span>

<span class="s2">    [CHUNK_CONTENT]:</span>
<span class="s2">    </span><span class="si">{</span><span class="n">leonardo_chunks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="n">contextualized_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk_content</span><span class="p">)</span>

<span class="n">contextualized_embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">contextualized_chunks</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_colwidth</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">([</span><span class="s2">&quot;What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?&quot;</span><span class="p">]),</span> <span class="n">contextualized_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;scores&quot;</span><span class="p">:</span> <span class="n">scores</span><span class="p">,</span> <span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">document</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">leonardo_chunks</span><span class="p">]})</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="s2">&quot;scores&quot;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>scores</th>
      <th>documents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>0.666583</td>
      <td>.. tab-item:: Booster\n\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**   | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+=================================+==============+=====================================+\n        | lrd_all_serial | normal             | 4 cores                 | 04:00:00     | 1 node / 4 cores                | 40           | No GPUs</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.648405</td>
      <td>, Hyperthreading x 2         |\n        |                |                    |                         |              |                                 |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                  |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_usr_prod | normal             | 64 nodes                | 24:00:00     |                                 | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_dbg      | 2 nodes                 | 00:30:00     | 2 nodes / 64 cores / 8 GPUs     | 80           |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_bprod    | min = 65 nodes          | 24:00:00     | 256 nodes                       | 60           |                                     |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    | max = 256 nodes         |              |                                 |              |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_lprod    | 3 nodes                 | 4-00:00:00   | 3 nodes / 12 GPUs               | 40           |                                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_fua_dbg  | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 64 cores / 8 GPUs     | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        | boost_fua_prod | normal             | 16 nodes                | 24:00:00     | 4 running jobs per user account | 40           |                                     |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    |                         |              | 32 nodes / 3584 cores           |              |                                     |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | boost_qos_fuabprod | min = 17 nodes          | 24:00:00     | 32 nodes / 3584 cores           | 60           | Runs on 49 nodes                    |\n        |                |                    |                         |              |                                 |              |                                     |\n        |                |                    | max = 32 nodes          |              |                                 |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+\n        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                 | 0            |                                     |\n        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.630339</td>
      <td>.. _leonardo_card:\n\nLeonardo\n========\n\nLeonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.\nThis guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.\n\n.. |ico2| image:: img/leonardo_logo.png\n   :height: 55px\n   :class: no-scaled-link\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. \n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:\n\n * login01-ext.leonardo.cineca.it\n * login02-ext.leonardo.cineca.it\n * login05-ext.leonardo.cineca.it\n * login07-ext.leonardo.cineca.it\n\n.. warning::\n    \n    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\nSystem Architecture\n-------------------\n\nThe cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:\n\n* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.\n* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.\n\nThe overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. \n\nThe **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.\nThe **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.\n\nHardware Details\n^^^^^^^^^^^^^^^^\n\n.. tab-set::\n\n    .. tab-item:: Booster</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.621514</td>
      <td>.. list-table:: \n            :widths: 30 50\n            :header-rows: 1\n\n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2135, Da Vinci single-node GPU\n            * - Racks\n              - 116\n            * - Nodes\n              - 3456\n            * - Processors/node\n              - 1x `Intel Ice Lake Intel Xeon Platinum 8358 &lt;https://www.intel.com/content/www/us/en/products/sku/212282/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 32\n            * - Accelerators/node\n              - 4x `NVIDIA Ampere100 custom &lt;https://doi.org/10.17815/jlsrf-8-186&gt;`_, 64GiB HBM2e NVLink 3.0 (200 GB/s)\n            * - Local Storage/node (tmfs)\n              - (none)\n            * - RAM/node \n              - 512 GiB DDR4 3200 MHz\n            * - Rmax\n              - 241.2 PFlop/s (`top500 &lt;https://www.top500.org/system/180128/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology \n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n    .. tab-item:: DCGP\n\n        .. list-table::\n            :widths: 30 50\n            :header-rows: 1\n            \n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2140 three-node CPU blade\n            * - Racks\n              - 22\n            * - Nodes\n              - 1536\n            * - Processors/node\n              - 2x `Intel Sapphire Rapids Intel Xeon Platinum 8480+ &lt;https://www.intel.com/content/www/us/en/products/sku/231746/intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 112 cores/node\n            * - Accelerators\n              - (none)\n            * - Local Storage/node (tmfs)\n              - 3 TiB\n            * - RAM/node\n              - 512(8x64) GiB DDR5 4800 MHz\n            * - Rmax\n              - 7.84 PFlop/s (`top500 &lt;https://www.top500.org/system/180204/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology\n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n\nFile Systems and Data Managment\n-------------------------------\n\nThe storage organization conforms to **CINECA** infrastructure. General information are reported in :ref:`hpc/hpc_data_storage:File Systems and Data Management` section. In the following, only differences with respect to general behavior are listed and explained.</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.618120</td>
      <td>.. note::\n\n          The partitions: **boost_fua_dbg, boost_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.616491</td>
      <td>.. raw:: html\n\n          &lt;p&gt;Each Booster cell is composed of:&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;6 Ã Atos BullSequana XH2000 racks&lt;/strong&gt;, each containing:\n              &lt;ul&gt;\n                &lt;li&gt;3 Ã Level 2 (L2) switches&lt;/li&gt;\n                &lt;li&gt;3 Ã Level 1 (L1) switches&lt;/li&gt;\n                &lt;li&gt;30 compute nodes â each equipped with 4 GPUs, each connected via a dedicated 100 Gbps port&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Total per Booster cell:&lt;/strong&gt; 18 L2 switches, 18 L1 switches, and 180 compute nodes.&lt;/p&gt;\n\n          &lt;h4&gt;Connectivity Overview&lt;/h4&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 2 (L2) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 22 Ã 200 Gbps ports connecting to L2 switches in other cells&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 18 Ã 200 Gbps ports connecting to L1 switches within the cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription:&lt;/strong&gt; 0.8:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 1 (L1) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 18 Ã 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 40 Ã 100 Gbps ports connected to GPUs across 10 compute nodes&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription:&lt;/strong&gt; 1.11:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n        .. figure:: img/leo-net-booster_cell.png\n          :height: 750px\n          :align: center\n      \n      .. tab-item:: DCGP\n        \n        .. raw:: html</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.601556</td>
      <td>|                | dcgp_qos_fuabprod  | min = 17 nodes          | 24:00:00     | 64 nodes / 7168 cores                | 60           | Runs on 130 nodes                   |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 64 nodes          |              |                                      |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                      | 0            |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.555121</td>
      <td>+----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**        | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+======================================+==============+=====================================+\n        | lrd_all_serial | normal             | max = 4 cores           | 04:00:00     | 1 node / 4 cores                     | 40           | Hyperthreading x 2                  |\n        |                |                    |                         |              |                                      |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                       |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_usr_prod  | normal             | 16 nodes                | 24:00:00     | 512 nodes per prj. account           | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_dbg       | 2 nodes                 | 00:30:00     | 2 nodes / 224 cores per user account | 80           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_bprod     | min = 17 nodes          | 24:00:00     | 128 nodes per user account           | 60           | GrpTRES = 1536 nodes                |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 128 nodes         |              | 512 nodes per prj. account           |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_lprod     | 3 nodes                 | 4-00:00:00   | 3 nodes / 336 cores per user account | 40           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_dbg   | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 224 cores                  | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_prod  | normal             | 16 nodes                | 24:00:00     |                                      | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.522800</td>
      <td>.. raw:: html\n\n          &lt;p&gt;Each DCGP cell is composed of:&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;8 Ã Atos BullSequana XH2000 racks&lt;/strong&gt;, each containing:\n              &lt;ul&gt;\n                &lt;li&gt;3 or 0 Level 2 (L2) switches&lt;/li&gt;\n                &lt;li&gt;2 Ã Level 1 (L1) switches&lt;/li&gt;\n                &lt;li&gt;78 compute nodes â each connected via a dedicated 100 Gbps port&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Total per DCGP cell:&lt;/strong&gt; 18 L2 switches, 16 L1 switches, and 624 compute nodes.&lt;/p&gt;\n\n          &lt;h4&gt;Connectivity Overview&lt;/h4&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 2 (L2) Switches:&lt;/strong&gt;&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;UP:&lt;/strong&gt; 22 Ã 200 Gbps ports connecting to L2 switches in other cells&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;DOWN:&lt;/strong&gt; 18 Ã 200 Gbps ports connecting to L1 switches within the same cell&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Oversubscription ratio:&lt;/strong&gt; 0.8:1&lt;/li&gt;\n          &lt;/ul&gt;\n\n          &lt;p&gt;&lt;strong&gt;Level 1 (L1) Switches:&lt;/strong&gt; (divided into two groups):&lt;/p&gt;\n          &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;9 switches with 40 downlinks:&lt;/strong&gt;\n              &lt;ul&gt;\n                &lt;li&gt;UP: 18 Ã 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n                &lt;li&gt;DOWN: 40 Ã 100 Gbps ports connect&lt;/strong&gt;ed to compute nodes&lt;/li&gt;\n                &lt;li&gt;Oversubscription ratio: 1.11:1&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;9 switches with 38 downlinks:&lt;/strong&gt;\n              &lt;ul&gt;\n                &lt;li&gt;UP: 18 Ã 200 Gbps ports connected to all L2 switches in the cell&lt;/li&gt;\n                &lt;li&gt;DOWN: 38 Ã 100 Gbps ports connected to compute nodes&lt;/li&gt;\n                &lt;li&gt;Oversubscription ratio: 1.05:1&lt;/li&gt;\n              &lt;/ul&gt;\n            &lt;/li&gt;\n          &lt;/ul&gt;\n\n        .. figure:: img/leo-net-dcgp_cell.png\n          :height: 750px\n          :align: center\n\nAdvanced Information\n^^^^^^^^^^^^^^^^^^^^\n\n.. dropdown:: Network Topology - Map\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    The topology is presented in a table format, where each row corresponds to a compute node. For each node, the table specifies the associated L1 switch and cell, providing a clear overview of the physical and logical network layout within the cluster.\n\n    :download:`Network Topology - Map &lt;../files/ntopology.dat&gt;`</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.519628</td>
      <td>.. tab-item:: DCGP</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.505805</td>
      <td>.. dropdown:: **$TMPDIR**\n\n * on the local SSD disks on login nodes (14 TB of capacity), mounted as ``/scratch_local`` (``TMPDIR=/scratch_local``). This is a shared area with no quota, remove all the files once they are not requested anymore. A cleaning procedure will be enforced in case of improper use of the area.   \n \n * on the local SSD disks on the serial node (``lrd_all_serial``, 14TB of capacity), managed via the Slurm ``job_container/tmpfs plugin``. This plugin provides a *job-specific*, private temporary file system space, with private instances of ``/tmp`` and ``/dev/shm`` in the job's user space (``TMPDIR=/tmp``, visible via the command ``df -h``), removed at the end of the serial job. You can request the resource via sbatch directive or srun option ``--gres=tmpfs:XX`` (for instance: ``--gres=tmpfs:200G``), with a maximum of 1 TB for the serial jobs. If not explicitly requested, the ``/tmp`` has the default dimension of 10 GB.\n \n * on the local SSD disks on DCGP nodes (3 TB  of capacity). As for the serial node, the local ``/tmp`` and ``/dev/shm`` areas are managed via plugin, which at the start of the jobs mounts private instances of ``/tmp`` and ``/dev/shm`` in the job's user space (``TMPDIR=/tmp``, visible via the command ``df -h /tmp``), and unmounts them at the end of the job (all data will be lost). You can request the resource via sbatch directive or srun option ``--gres=tmpfs:XX``, with a maximum of all the available 3 TB for DCGP nodes. As for the serial node, if not explicitly requested, the ``/tmp`` has the default dimension of 10 GB. Please note: for the DCGP jobs the requested amount of ``gres/tmpfs`` resource contributes to the consumed budget, changing the number of accounted equivalent core hours, see the dedicated section on the Accounting.\n \n * on RAM on the diskless booster nodes (with a fixed size of 10 GB, no increase is allowed, and the ``gres/tmpfs`` resource is disabled).\n\nJob Managing and Slurm Partitions \n---------------------------------\n\nIn the following table you can find informations about the Slurm partitions for **Booster** and **DCGP** partitions.  \n\n.. seealso:: \n  Further information about job submission are reported in the general section :ref:`hpc/hpc_scheduler:Scheduler and Job Submission`. \n\n.. tab-set::</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.499677</td>
      <td>.. note::\n\n          The partitions: **dcgp_fua_dbg, dcgp_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.\n\nNetwork Architecture\n--------------------\n\n.. raw:: html\n\n  &lt;p&gt;&lt;strong&gt;Leonardo&lt;/strong&gt; features a state-of-the-art interconnect system tailored for high-performance computing (HPC). It delivers &lt;em&gt;low latency&lt;/em&gt; and &lt;em&gt;high bandwidth&lt;/em&gt; by leveraging &lt;strong&gt;NVIDIA Mellanox InfiniBand HDR&lt;/strong&gt; (High Data Rate) technology, powered by &lt;a href="https://nvdam.widen.net/s/zmbw7rdjml/infiniband-qm8700-datasheet-us-nvidia-1746790-r12-web"&gt;NVIDIA QUANTUM QM8700 Smart Switches&lt;/a&gt;, and a &lt;strong&gt;&lt;a href="https://ieeexplore.ieee.org/document/7885210"&gt;Dragonfly+ topology&lt;/a&gt;&lt;/strong&gt;. Below is an overview of its architecture and key features:&lt;/p&gt;\n\n  &lt;ul&gt;\n    &lt;li&gt;&lt;strong&gt;Hierarchical Cell Structure:&lt;/strong&gt; The system is structured into multiple &lt;em&gt;cells&lt;/em&gt;, each comprising a group of interconnected compute nodes.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Inter-cell Connectivity:&lt;/strong&gt; As illustrated in the figure below, cells are connected via an all-to-all topology. Each pair of distinct cells is linked by 18 independent connections, each passing through a dedicated Layer 2 (L2) switch. This design ensures high availability and reduces congestion.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Intra-cell Topology:&lt;/strong&gt; Inside each cell, a non-blocking two-layer fat-tree topology is used, allowing scalable and efficient intra-cell communication.&lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;System Composition:&lt;/strong&gt;\n      &lt;ul&gt;\n        &lt;li&gt;19 cells dedicated to the &lt;em&gt;Booster&lt;/em&gt; partition.&lt;/li&gt;\n        &lt;li&gt;2 cells for the &lt;em&gt;DCGP&lt;/em&gt; (Data-Centric General Purpose) partition.&lt;/li&gt;\n        &lt;li&gt;1 hybrid cell with both accelerated (36 Booster nodes) and conventional (288 DCGP nodes) compute resources.&lt;/li&gt;\n        &lt;li&gt;1 cell allocated for management, storage, and login services.&lt;/li&gt;\n      &lt;/ul&gt;\n    &lt;/li&gt;\n\n    &lt;li&gt;&lt;strong&gt;Adaptive Routing:&lt;/strong&gt; The network employs adaptive routing, dynamically optimizing data paths to alleviate congestion and maintain performance under load.&lt;/li&gt;\n  &lt;/ul&gt;\n  \n.. figure:: img/leo-net-all2all.png\n   :height: 350px\n   :align: center\n   :class: no-scaled-link\n\n.. image:: img/spacer.png\n   :align: center\n   :class: no-scaled-link\n   \n.. dropdown:: Cell Configuration and Intra-cell Connectivity\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n   .. tab-set::\n\n      .. tab-item:: Booster</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.498936</td>
      <td>.. dropdown:: Network Topology - Distance Matrix\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    The attached compressed CSV file contains the distance matrix of all compute nodes in the cluster. The matrix uses the following metric to represent the network distance between any two nodes:\n\n    * **0** â Same nodes\n    * **1** â Same L1 switch, same cell.\n    * **2** â Different L1 switch, same cell.\n    * **3** â Different L1 switch and different cell.\n    \n    This matrix can be used to analyze communication locality and optimize node selection for distributed workloads.\n\n    :download:`Distance Matrix &lt;../files/ntopology-dst_mtx.tar.bz2&gt;`\n\n.. dropdown:: Switch Naming Format\n   :animate: fade-in-slide-down\n   :chevron: down-up\n\n    .. code-block::\n      \n      isw&lt;RRrrSS&gt;\n\n    where ``&lt;RRrrSS&gt;`` is a 5- or 6-digits number varies based on the location and type of the switch.\n\n    Specifically:\n\n    * ``RR`` = region number (1 or 2 digits)\n    * ``rr`` = rack number (2 digits)\n    * ``SS`` = switch id (2 digits)\n\n    .. note::\n      If ``SS`` is an even number, it refers to an L1 switch; if it is an odd number, it refers to an L2 switch.\n\nDocuments\n---------\n\n* Article on Leonardo architecture and the technologies adopted for its GPU-accelerated partition: CINECA Supercomputing Centre, SuperComputing Applications and Innovation Department. (2024). âLEONARDO: A Pan-European Pre-Exascale Supercomputer for HPC and AI applications.â, Journal of large-scale research facilities, 8, A186. https://doi.org/10.17815/jlsrf-8-186\n* Details about new technologies included in the Witley platform with Intel Xeon Icelake contained in the Leonardo pre-exascale system (`link &lt;https://urldefense.com/v3/__https://software.intel.com/content/www/us/en/develop/articles/third-generation-xeon-scalable-family-overview.html__;!!P1tgJ-3e!TrmMus5wzdLQ963vkc3yfy0BlhC1Hu8vOoce4SgltsTbkSSDrX2p1zTXPCIrpPm3$&gt;`_)\n* Additional documents (`link &lt;https://urldefense.com/v3/__https://software.intel.com/content/www/us/en/develop/articles/xeon-performance-tuning-and-solution-guides.html__;!!P1tgJ-3e!TrmMus5wzdLQ963vkc3yfy0BlhC1Hu8vOoce4SgltsTbkSSDrX2p1zTXPKZ5awkS$&gt;`_)\n\nSome tuning guides for dedicated enviroments (ML/DL or HPC Clusters):\n\n* :download:`Tuning Guide &lt;../files/Tuning_guide.pdf&gt;`\n\n* :download:`Deep Learning &lt;../files/Deep_learning.pdf&gt;`</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The first chunks are now more relevant. Let’s re-embed our entire document base using context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># Drop this db if exists</span>
<span class="n">chroma_client</span> <span class="o">=</span> <span class="n">chromadb</span><span class="o">.</span><span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="n">chroma_path</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">chroma_client</span><span class="o">.</span><span class="n">delete_collection</span><span class="p">(</span><span class="s2">&quot;hpc_contextualized_wiki&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="n">NotFoundError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">create_collection</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;hpc_contextualized_wiki&quot;</span><span class="p">)</span>

<span class="c1"># For each document, split it and generate contextualized chunks</span>
<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">document_contextualized_chunks</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">document_chunks</span> <span class="o">=</span> <span class="n">rcts_chunker</span><span class="o">.</span><span class="n">split_documents</span><span class="p">([</span><span class="n">document</span><span class="p">])</span>

    <span class="c1"># You can save also the contexts as metadata in the vector db, it could be interesting...</span>
    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">chunk_contexts</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">e</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">generate_context</span><span class="p">,</span> <span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span><span class="p">,</span> <span class="n">full_document</span> <span class="o">=</span> <span class="n">document</span><span class="p">),</span> <span class="n">document_chunks</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">document_chunks</span><span class="p">)):</span>
        <span class="n">chunk_content</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;[CONTEXT]:</span>
<span class="s2">        </span><span class="si">{</span><span class="n">chunk_contexts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span>
<span class="s2">    </span>
<span class="s2">        [CHUNK_CONTENT]:</span>
<span class="s2">        </span><span class="si">{</span><span class="n">document_chunks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span>
<span class="s2">        &quot;&quot;&quot;</span>
    
        <span class="n">document_contextualized_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk_content</span><span class="p">)</span>
    <span class="c1"># Embed the contextualized chunk content</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">document_contextualized_chunks</span><span class="p">)</span>
    
    <span class="c1"># Add all chunks to the collection</span>
    <span class="n">collection</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">document_chunks</span><span class="p">],</span>
                   <span class="n">metadatas</span>  <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">document_chunks</span><span class="p">],</span>
                   <span class="n">ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;__&quot;</span> <span class="o">+</span> \
                          <span class="nb">str</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;start_index&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">document_chunks</span><span class="p">],</span>
                   <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 9.61 s, sys: 429 ms, total: 10 s
Wall time: 2min 2s
</pre></div>
</div>
</div>
</div>
<p>Let’s test whether the new database performs better than the old one using our test suite.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpc_contextualized_store</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;hpc_contextualized_wiki&quot;</span><span class="p">,</span> <span class="n">embedding_function</span> <span class="o">=</span> <span class="n">lc_embedder</span><span class="p">,</span> <span class="n">persist_directory</span><span class="o">=</span> <span class="n">chroma_path</span><span class="p">)</span>

<span class="n">k_thresh_tests_contextualized</span> <span class="o">=</span> <span class="n">optimize_retriever</span><span class="p">(</span><span class="n">vector_store</span> <span class="o">=</span> <span class="n">hpc_contextualized_store</span><span class="p">,</span> <span class="n">qa_set</span> <span class="o">=</span> <span class="n">qa_set</span><span class="p">,</span> <span class="n">max_k</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">reranker_name</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-07-28 16:58:55.500175 - Testing k_threshold 1
2025-07-28 16:58:56.800131 - Testing k_threshold 2
2025-07-28 16:58:58.089317 - Testing k_threshold 3
2025-07-28 16:58:59.393307 - Testing k_threshold 4
2025-07-28 16:59:00.691047 - Testing k_threshold 5
2025-07-28 16:59:01.987567 - Testing k_threshold 6
2025-07-28 16:59:03.284972 - Testing k_threshold 7
2025-07-28 16:59:04.584109 - Testing k_threshold 8
2025-07-28 16:59:05.884367 - Testing k_threshold 9
2025-07-28 16:59:07.187672 - Testing k_threshold 10
2025-07-28 16:59:08.493321 - Testing k_threshold 11
2025-07-28 16:59:09.828080 - Testing k_threshold 12
2025-07-28 16:59:11.134806 - Testing k_threshold 13
2025-07-28 16:59:12.442254 - Testing k_threshold 14
2025-07-28 16:59:13.751521 - Testing k_threshold 15
2025-07-28 16:59:15.066636 - Testing k_threshold 16
2025-07-28 16:59:16.386172 - Testing k_threshold 17
2025-07-28 16:59:17.701877 - Testing k_threshold 18
2025-07-28 16:59:19.020208 - Testing k_threshold 19
2025-07-28 16:59:20.341073 - Testing k_threshold 20
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">k_thresh_tests_contextualized</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;precision_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;precision_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;recall_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;recall_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;f1_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;f1_k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;K threshold tests&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k</th>
      <th>precision_k</th>
      <th>recall_k</th>
      <th>f1_k</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.970149</td>
      <td>0.580838</td>
      <td>0.726634</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.671642</td>
      <td>0.723887</td>
      <td>0.696786</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0.512438</td>
      <td>0.772284</td>
      <td>0.616083</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0.421642</td>
      <td>0.809344</td>
      <td>0.554439</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0.361194</td>
      <td>0.830378</td>
      <td>0.503415</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>0.308458</td>
      <td>0.833857</td>
      <td>0.450331</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>0.275053</td>
      <td>0.843199</td>
      <td>0.414798</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>0.248134</td>
      <td>0.850351</td>
      <td>0.384168</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>0.227197</td>
      <td>0.857803</td>
      <td>0.359245</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>0.210448</td>
      <td>0.865013</td>
      <td>0.338534</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>0.192673</td>
      <td>0.866369</td>
      <td>0.315239</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>0.179104</td>
      <td>0.870598</td>
      <td>0.297090</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>0.166475</td>
      <td>0.874330</td>
      <td>0.279696</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>0.154584</td>
      <td>0.874330</td>
      <td>0.262719</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>0.147264</td>
      <td>0.878587</td>
      <td>0.252247</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>0.138060</td>
      <td>0.878587</td>
      <td>0.238623</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>0.130817</td>
      <td>0.879266</td>
      <td>0.227749</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>0.126036</td>
      <td>0.883157</td>
      <td>0.220592</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>0.120974</td>
      <td>0.887020</td>
      <td>0.212911</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>0.115672</td>
      <td>0.887699</td>
      <td>0.204673</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../../../_images/ad6bc3480493cedeea95038f091e658fdc84e0a224efcb8c961d776f52d74cca.png" src="../../../../_images/ad6bc3480493cedeea95038f091e658fdc84e0a224efcb8c961d776f52d74cca.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;precision_k&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;precision_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;precision_k - diff&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;recall_k&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;recall_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;recall_k - diff&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;k&quot;</span><span class="p">],</span> <span class="n">k_thresh_tests_contextualized</span><span class="p">[</span><span class="s2">&quot;f1_k&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">k_thresh_tests</span><span class="p">[</span><span class="s2">&quot;f1_k&quot;</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;f1_k - diff&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;K threshold tests (Contextualization improvement)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/a18b65de6d27955e1c96f89fe23565e819df440bf0afd84dcefaffc196e8b9fd.png" src="../../../../_images/a18b65de6d27955e1c96f89fe23565e819df440bf0afd84dcefaffc196e8b9fd.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">semantic_retriever_context</span> <span class="o">=</span> <span class="n">SemanticRetriever</span><span class="p">(</span><span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;hpc_contextualized_wiki&quot;</span><span class="p">,</span> <span class="n">chroma_path</span> <span class="o">=</span> <span class="n">chroma_path</span><span class="p">,</span> <span class="n">embedder_name</span> <span class="o">=</span> <span class="n">EMBEDDER</span><span class="p">,</span> <span class="n">reranker_name</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">answ</span> <span class="o">=</span> <span class="n">semantic_retriever_context</span><span class="o">.</span><span class="n">generate_answ</span><span class="p">(</span><span class="s2">&quot;What are the partitions available in Leonardo BOOST and their associated QOS?&quot;</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">answ</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The partitions available in Leonardo BOOST and their associated QOS are as follows:

1. **boost_usr_prod**
   - **QOS**: normal
     - **#Cores/#GPU per job**: 64 nodes
     - **Walltime**: 24:00:00
     - **Max Nodes/cores/GPUs/user**: Not specified
     - **Priority**: 40
   - **QOS**: boost_qos_dbg
     - **#Cores/#GPU per job**: 2 nodes
     - **Walltime**: 00:30:00
     - **Max Nodes/cores/GPUs/user**: 2 nodes / 64 cores / 8 GPUs
     - **Priority**: 80
   - **QOS**: boost_qos_bprod
     - **#Cores/#GPU per job**: min = 65 nodes, max = 256 nodes
     - **Walltime**: 24:00:00
     - **Max Nodes/cores/GPUs/user**: 256 nodes
     - **Priority**: 60
   - **QOS**: boost_qos_lprod
     - **#Cores/#GPU per job**: 3 nodes
     - **Walltime**: 4-00:00:00
     - **Max Nodes/cores/GPUs/user**: 3 nodes / 12 GPUs
     - **Priority**: 40

2. **boost_fua_dbg**
   - **QOS**: normal
     - **#Cores/#GPU per job**: 2 nodes
     - **Walltime**: 00:10:00
     - **Max Nodes/cores/GPUs/user**: 2 nodes / 64 cores / 8 GPUs
     - **Priority**: 40
     - **Notes**: Runs on 2 nodes

3. **boost_fua_prod**
   - **QOS**: normal
     - **#Cores/#GPU per job**: 16 nodes
     - **Walltime**: 24:00:00
     - **Max Nodes/cores/GPUs/user**: 4 running jobs per user account, 32 nodes / 3584 cores
     - **Priority**: 40
   - **QOS**: boost_qos_fuabprod
     - **#Cores/#GPU per job**: min = 17 nodes, max = 32 nodes
     - **Walltime**: 24:00:00
     - **Max Nodes/cores/GPUs/user**: 32 nodes / 3584 cores
     - **Priority**: 60
     - **Notes**: Runs on 49 nodes, Min is 17 FULL nodes
   - **QOS**: qos_fualowprio
     - **#Cores/#GPU per job**: 16 nodes
     - **Walltime**: 08:00:00
     - **Max Nodes/cores/GPUs/user**: Not specified
     - **Priority**: 0
</pre></div>
</div>
</div>
</div>
<p>Now, that’s an answer…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">test_questions</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[QUESTION]: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">answ</span> <span class="o">=</span> <span class="n">semantic_retriever_context</span><span class="o">.</span><span class="n">generate_answ</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[ANSWER]: &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">answ</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[QUESTION]: What GPUs are available on Leonardo?
[ANSWER]: The Leonardo supercomputer features the following GPUs:

- **NVIDIA Ampere A100-64 accelerators** in the Booster partition. Each node in this partition has 4 NVIDIA Ampere A100 custom GPUs, each with 64 GiB HBM2e memory and NVLink 3.0 (200 GB/s) connectivity.

[QUESTION]: Is there any partition without gpus?
[ANSWER]: Yes, there are partitions without GPUs. Here are some examples:

1. **Galileo**:
   - `g100_all_serial` (default)
   - `g100_usr_dbg`
   - `g100_usr_prod`
   - `g100_usr_bmem`

2. **Leonardo**:
   - `lrd_all_serial` (default)
   - `dcgp_usr_prod`
   - `dcgp_fua_dbg`
   - `dcgp_fua_prod`

3. **Pitagora**:
   - `dcgp_usr_prod`
   - `dcgp_fua_dbg`
   - `dcgp_fua_prod`

These partitions are designed for CPU-only computations and do not include GPU resources.

[QUESTION]: What GPUs are available on the Cloud?
[ANSWER]: The available GPUs on the Cloud are:

- Nvidia A30
- Nvidia L40s
- Nvidia H100 NVL

[QUESTION]: Can I associate a domain name to a vm?
[ANSWER]: Yes, you can associate a domain name with a virtual machine (VM). To do this, you need to request a DNS name association from CINECA by sending an email to superc@cineca.it. Here are the key points to follow:

1. **Reverse DNS (PTR Record)**: The reverse of the Floating IP must be set to the hostname of the VM. The naming convention is:
   - For external users: `&lt;VM-name&gt;.ext.cineca.it`
   - For CINECA staff: `&lt;VM-name&gt;.cineca.it`

2. **Record A**: The DNS record A will be set accordingly.

3. **CNAME**: If you need the service to be exposed with a different name, you can request a CNAME with the chosen different name. If no other information is provided, only the record A will be set.

4. **External DNS**: You can also set up a CNAME with your DNS provider of choice.

5. **Restrictions**: It is not possible to set the PTR record in CINECA DNS if the record A has been set on an external DNS.

For detailed guidelines, refer to the DNS guidelines provided by CINECA.

[QUESTION]: What are the naming conventions I should follow when asking for a domain name for a vm machine?
[ANSWER]: When asking for a domain name for a VM machine, follow these naming conventions:

1. **Reverse of the Floating IP (PTR record)**:
   - For external users: `&lt;VM-name&gt;.ext.cineca.it`
   - For CINECA staff: `&lt;VM-name&gt;.cineca.it`

2. **Record A**: This will be set accordingly to the PTR record.

3. **CNAME**: If the service should be exposed with a different name, you can request a CNAME with the chosen different name. If no other information is provided, only the record A will be set.

4. **PTR Record**: It is not possible to set the PTR record in CINECA DNS if the record A has been set on an external DNS.

To request a DNS name, send an email to `superc@cineca.it` with the desired VM name and specify whether you are an external user or CINECA staff.

[QUESTION]: What is Cineca AI and how do I enable it?
[ANSWER]: Cineca AI is a collection of artificial intelligence packages optimized for Cineca&#39;s high-performance computing (HPC) clusters. These packages include popular AI frameworks like TensorFlow, PyTorch, and XGBoost, among others.

### How to Enable Cineca AI

To enable Cineca AI, follow these steps:

1. **Load the Cineca AI Module:**
   First, you need to load the Cineca AI module. This can be done using the `module load` command. The specific version of the module may vary, so you might need to check the available versions.

   ```bash
   module load profile/deeplrn
   module av cineca-ai
   module load cineca-ai/&lt;version&gt;
   ```

   Replace `&lt;version&gt;` with the specific version number you want to use.

2. **Verify the Installation:**
   After loading the module, you can verify the installation by listing the available Python packages.

   ```bash
   python -m pip list
   ```

3. **Use a Specific Package:**
   To use a specific package from the Cineca AI suite, you can import it in your Python script or interactive session.

   ```bash
   python -c &quot;import &lt;package&gt;&quot;
   ```

   Replace `&lt;package&gt;` with the name of the package you want to use.

4. **Create a Virtual Environment (Optional):**
   If you need to install additional packages or want to create an isolated environment, you can create a virtual environment.

   ```bash
   module load profile/deeplrn
   module av cineca-ai
   module load cineca-ai/&lt;version&gt;
   python -m venv my_env --system-site-packages
   source my_env/bin/activate
   pip install &lt;package&gt;
   ```

   Replace `&lt;package&gt;` with the name of the package you want to install.

5. **Deactivate the Virtual Environment:**
   Once you are done working in the virtual environment, you can deactivate it.

   ```bash
   deactivate
   ```

By following these steps, you should be able to enable and use Cineca AI on Cineca&#39;s HPC clusters.

[QUESTION]: What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?
[ANSWER]: The QOS queues available on the Leonardo supercomputer&#39;s BOOSTER partition are:

1. **normal**
2. **boost_qos_dbg**
3. **boost_qos_bprod**
4. **boost_qos_lprod**
5. **boost_qos_fuabprod**
6. **qos_fualowprio**
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="a-few-considerations-about-contextual-retrieval">
<h1>A few considerations about contextual retrieval<a class="headerlink" href="#a-few-considerations-about-contextual-retrieval" title="Link to this heading"></a></h1>
<ol class="arabic simple">
<li><p>Production pipelines with many calls to LLMs become costly and slow, especially if you need to re-index your data frequently. An interesting approach here would be to generate context only for chunks containing tables. You can identify chunks containing only tables by using well-written regular expressions or by checking the entropy of the chunk.</p></li>
<li><p>Simple approaches (e.g., BM25) are better if they work well with your data.</p></li>
</ol>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="final-remarks">
<h1>Final remarks<a class="headerlink" href="#final-remarks" title="Link to this heading"></a></h1>
<p>When you create a RAG system, you don’t chunk just for the sake of chunking. You want to have a testing set to evaluate the performance of various approaches. Everything must be measured, and improvements must be quantified.</p>
<p>You also want a reference set of expert-made questions to quantify the performance of your system and identify which techniques produce the best results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total execution time </span><span class="si">{</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total execution time 0:20:49.053584
</pre></div>
</div>
</div>
</div>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>