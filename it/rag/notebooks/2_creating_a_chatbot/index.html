

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>General &mdash; CASTIEL2 Multi-GPU AI  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=9c3e77be" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx_lesson.css?v=e9df6548" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/term_role_formatting.css?v=4194e21c" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx_rtd_theme_ext_color_contrast.css?v=8e8ea19f" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/overrides.css?v=d560b895" />

  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
      <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../_static/documentation_options.js?v=187304be"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../_static/copybutton.js?v=35a8b989"></script>
      <script src="../../../../_static/minipres.js?v=a0d29692"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../../../_static/togglebutton.js?v=1ae7504c"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../search/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../" class="icon icon-home">
            CASTIEL2 Multi-GPU AI
              <img src="../../../../_static/CASTIEL2.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The lesson materials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../leonardo/README/">1M: Access to Leonardo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../se/deep-learning-intro/">1A: Introduction to Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nl/">2M: PyTorch Distributed Data Parallel</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../quick-reference/">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide/">Instructor’s guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../directives/">Directives</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../">CASTIEL2 Multi-GPU AI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">General</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/castiel-multi-gpu-ai/blob/main/content/it/rag/notebooks/2_creating_a_chatbot.ipynb" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="general">
<h1>General<a class="headerlink" href="#general" title="Link to this heading"></a></h1>
<p>In the previous notebook, we created a document base and tested it with a set of questions. In this notebook, we will explore two strategies to deploy a RAG system. Specifically, we will evaluate a workflow-based approach and an agentic setup.</p>
<p>These correspond to steps 4 and 5 described in the previous notebook.</p>
<p>For this tutorial, <strong>we add two additional collections to our vectorized data. One collection contains Linux information, and the other contains information about Slurm. This slightly complicates the retrieval step</strong>: what happens when we receive a query? We could launch a semantic search over all our data and keep the top three relevant documents from all the collections, or we could:</p>
<ol class="arabic simple">
<li><p>Create a workflow that routes a question through a specific retriever;</p></li>
<li><p>Create an agent that does this automatically.</p></li>
</ol>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="imports">
<h1>Imports<a class="headerlink" href="#imports" title="Link to this heading"></a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">base64</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">ast</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_chroma.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span> 
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers.document_compressors</span><span class="w"> </span><span class="kn">import</span> <span class="n">CrossEncoderReranker</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.cross_encoders</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceCrossEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.retrievers</span><span class="w"> </span><span class="kn">import</span> <span class="n">ContextualCompressionRetriever</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.documents</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.prebuilt</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_react_agent</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="env-config">
<h1>Env config<a class="headerlink" href="#env-config" title="Link to this heading"></a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="n">date</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[ :-]&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">t0</span><span class="p">)[:</span><span class="mi">19</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Last execution </span><span class="si">{</span><span class="n">t0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">load_dotenv</span><span class="p">()</span>

<span class="c1"># Models</span>
<span class="n">EMBEDDER</span> <span class="o">=</span> <span class="s2">&quot;BAAI/bge-m3&quot;</span>
<span class="n">RERANKER</span> <span class="o">=</span> <span class="s2">&quot;BAAI/bge-reranker-v2-m3&quot;</span>
<span class="n">LLM</span> <span class="o">=</span> <span class="s2">&quot;mistralai/Mistral-Small-3.1-24B-Instruct-2503&quot;</span>

<span class="c1"># Endpoints</span>
<span class="n">VLLM_OPENAI_ENDPOINT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_OPENAI_ENDPOINT&quot;</span><span class="p">]</span>
<span class="n">VLLM_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_KEY&quot;</span><span class="p">]</span>

<span class="c1"># Paths</span>
<span class="n">PROMPT_PATH</span> <span class="o">=</span> <span class="s2">&quot;../data/prompts&quot;</span>
<span class="n">CHROMA_PATH</span> <span class="o">=</span> <span class="s2">&quot;../data/output/chunking/chroma&quot;</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">VLLM_OPENAI_ENDPOINT</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">VLLM_KEY</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">LLM</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_completion_tokens</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>

<span class="c1"># Use GPU 3 for this notebook. GPUs 0 and 1 are used to load the llm</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Last execution 2025-07-28 16:51:48.413683
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Util function to plot mermaid graphs</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mm</span><span class="p">(</span><span class="n">graph</span><span class="p">):</span>
    <span class="n">graphbytes</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf8&quot;</span><span class="p">)</span>
    <span class="n">base64_bytes</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">urlsafe_b64encode</span><span class="p">(</span><span class="n">graphbytes</span><span class="p">)</span>
    <span class="n">base64_string</span> <span class="o">=</span> <span class="n">base64_bytes</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;ascii&quot;</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://mermaid.ink/img/&quot;</span> <span class="o">+</span> <span class="n">base64_string</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-collections">
<h1>The collections<a class="headerlink" href="#the-collections" title="Link to this heading"></a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is basically the class we implemented in the previous notebook, except for the fact that we removed the llm generation call</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SemanticRetriever</span><span class="p">():</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_k</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">collection_name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">chroma_path</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">embedder_name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">reranker_name</span><span class="p">:</span><span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reranker_name</span> <span class="o">=</span> <span class="n">reranker_name</span>
        <span class="n">lc_embedder</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">embedder_name</span><span class="p">)</span>
        <span class="n">vector_store</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span><span class="n">collection_name</span> <span class="o">=</span> <span class="n">collection_name</span><span class="p">,</span> <span class="n">embedding_function</span> <span class="o">=</span> <span class="n">lc_embedder</span><span class="p">,</span> <span class="n">persist_directory</span> <span class="o">=</span> <span class="n">chroma_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="mi">20</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reranker_name</span> <span class="k">else</span> <span class="mi">3</span><span class="p">})</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reranker_name</span><span class="p">:</span>
            <span class="n">reranker</span> <span class="o">=</span> <span class="n">HuggingFaceCrossEncoder</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">reranker_name</span><span class="p">)</span>
            <span class="n">compressor</span> <span class="o">=</span> <span class="n">CrossEncoderReranker</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">reranker</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">compression_retriever</span> <span class="o">=</span> <span class="n">ContextualCompressionRetriever</span><span class="p">(</span><span class="n">base_compressor</span><span class="o">=</span><span class="n">compressor</span><span class="p">,</span> <span class="n">base_retriever</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">retriever</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">retrieve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reranker_name</span><span class="p">:</span>
            <span class="n">retrieved_chunks</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compression_retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">retrieved_chunks</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

        <span class="n">resources</span> <span class="o">=</span> <span class="s2">&quot;[RETRIEVED_RESOURCES]:</span><span class="se">\n\n</span><span class="s2">&quot;</span> 
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">retrieved_chunks</span><span class="p">:</span>
            <span class="n">page_content</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">page_content</span>
            <span class="n">doc_name</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;doc_name&quot;</span><span class="p">]</span>
            <span class="n">resources</span> <span class="o">+=</span> <span class="s2">&quot;[DOCUMENT_TITLE]: &quot;</span> <span class="o">+</span> <span class="n">doc_name</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[DOCUMENT_CONTENT]: &quot;</span> <span class="o">+</span> <span class="n">page_content</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="k">return</span> <span class="n">resources</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SlurmDocumentationRetriever</span><span class="p">():</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slurm_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;SLURM Overview: SLURM (Simple Linux Utility for Resource Management) is an open-source workload manager used for scheduling and managing jobs on HPC clusters, handling job queues, and allocating resources on compute nodes.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;SLURM Architecture: Understand how SLURM components interact, including the controller daemon (slurmctld), compute node daemons (slurmd), and optional database daemon (slurmdbd) for accounting.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Comparison with Other Workload Managers: Compare SLURM with PBS, LSF, and other schedulers to understand differences in syntax, features, and usage within HPC environments.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;SLURM Key Components: Learn about slurmctld for managing jobs, slurmd for executing jobs on compute nodes, slurmdbd for accounting, and configuration files like slurm.conf, cgroup.conf, and gres.conf for resource management.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Basic SLURM User Commands: Commands like sbatch (submit jobs), squeue (check job queue), scontrol show job (inspect job details), scancel (cancel jobs), and srun or salloc (interactive job execution).&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;SLURM Job Scripts: Structure and syntax for writing SLURM batch scripts, including SBATCH directives for setting job names, outputs, errors, wall time, partitions, node and CPU requests, memory, and GPU allocations.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Resource Management: Learn about partitions and priorities in SLURM, scheduling policies like backfill and fair-share, node features, GPU scheduling, and requesting memory and CPU resources accurately.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Monitoring and Debugging in SLURM: Tools like sinfo (cluster state), squeue and scontrol (job monitoring), sacct and sreport (accounting), seff (job efficiency check), and methods for debugging stuck or failed jobs.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Advanced SLURM Usage: Using job arrays for batch submission, managing job dependencies, checkpointing, requeueing failed jobs, heterogeneous job scheduling, and requesting advanced resources like licenses or specialized hardware.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;SLURM Accounting and Reporting: Setting up slurmdbd, configuring accounting_storage, managing users and accounts with sacctmgr, using QoS for prioritization, and generating usage and efficiency reports using sreport.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;SLURM Configuration and Administration: Writing and maintaining slurm.conf, configuring partitions and node states (drain, down, idle), managing fair-share policies, and using cgroup integration for resource control.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Integrations and Extensions: Integration of SLURM with MPI for parallel workloads, using Singularity or Apptainer containers with SLURM jobs, profiling jobs with sstat and sacct, and integrating with monitoring tools like Prometheus and Grafana.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;SLURM REST API: Learn about slurmrestd for exposing SLURM capabilities over HTTP/REST for programmatic management and monitoring of SLURM clusters.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Security and Policies in SLURM: Managing user authentication, permission controls, node isolation, job sandboxing, enforcing resource limits, and setting job timeout policies within a SLURM-managed HPC cluster.&quot;</span><span class="p">),</span>
                           <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Best Practices for SLURM: Writing efficient and clear job scripts, effectively using job arrays for scalable workloads, managing GPU and node usage for efficiency, profiling and optimizing SLURM jobs, and systematic debugging of issues.&quot;</span><span class="p">)</span>
                          <span class="p">]</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">retrieve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">retrieved_chunks</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slurm_docs</span><span class="p">)]</span>
        
        <span class="n">resources</span> <span class="o">=</span> <span class="s2">&quot;[RETRIEVED_RESOURCES]:</span><span class="se">\n\n</span><span class="s2">&quot;</span> 
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">retrieved_chunks</span><span class="p">:</span>
            <span class="n">page_content</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">page_content</span>
            <span class="n">resources</span> <span class="o">+=</span> <span class="s2">&quot;[DOCUMENT_CONTENT]: &quot;</span> <span class="o">+</span> <span class="n">page_content</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            
        <span class="k">return</span> <span class="n">resources</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LinuxDocumentationRetriever</span><span class="p">():</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linux_doc</span> <span class="o">=</span> <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Linux Overview: Linux is an open-source, Unix-like operating system kernel used for managing hardware resources and providing essential services to run applications on servers, desktops, and embedded systems.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Linux Distributions: Learn about different Linux distributions (distros) such as Ubuntu, CentOS, Debian, Fedora, and Arch, which package the Linux kernel with user-space utilities and package managers for different use cases.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Linux Filesystem Hierarchy: Understand the Linux filesystem structure, including root (/), /home, /etc, /var, /usr, /tmp, and the purpose of each directory in system organization.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;File and Directory Management: Basic file operations using commands like ls, cp, mv, rm, mkdir, rmdir, and advanced file management using find, locate, and file permissions (chmod, chown, chgrp).&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;User and Group Management: Managing users and groups with commands like useradd, userdel, passwd, groupadd, and understanding /etc/passwd, /etc/shadow, and /etc/group files.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Process Management: Understanding Linux processes, using ps, top, htop, kill, pkill, nice, renice, and background/foreground job control with &amp;, fg, bg, and jobs.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Package Management: Installing, updating, and removing packages using apt (Debian/Ubuntu), yum/dnf (RHEL/CentOS/Fedora), and pacman (Arch), and building packages from source.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;System Services and Daemons: Managing system services using systemd (systemctl), service, and chkconfig, and understanding how daemons run in the background to provide essential system services.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Networking Basics: Configuring network interfaces using ip, ifconfig, and nmcli, checking connectivity with ping, traceroute, and managing network services like SSH and firewalls.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Disk and Filesystem Management: Managing disks and partitions using fdisk, lsblk, blkid, formatting with mkfs, mounting filesystems with mount/umount, and checking disk usage with df and du.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;System Monitoring and Logging: Using monitoring tools like top, htop, iostat, vmstat, free, and inspecting logs in /var/log using tail, less, journalctl, and log rotation.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Shell Scripting: Writing basic and advanced shell scripts using bash, using variables, loops, conditionals, and creating reusable scripts for automation of system tasks.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Security and Permissions: Managing file permissions, using sudo for privilege escalation, configuring SSH security, using fail2ban, and keeping the system updated for security patches.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;System Backup and Recovery: Using tar, rsync, and cron for backups, creating disk images, and recovery strategies to restore system states in case of failures.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Advanced Linux Topics: Kernel modules management, performance tuning, using system calls, cgroups for resource management, and using containers (Docker, Podman) on Linux systems.&quot;</span><span class="p">),</span>
                          <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Linux Best Practices: Keeping systems updated, securing SSH access, using least privilege, monitoring system resources regularly, automating tasks with cron, and maintaining clean logs and disk usage.&quot;</span><span class="p">)</span>
                         <span class="p">]</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">retrieve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">retrieved_chunks</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linux_doc</span><span class="p">)]</span>

        <span class="n">resources</span> <span class="o">=</span> <span class="s2">&quot;[RETRIEVED_RESOURCES]:</span><span class="se">\n\n</span><span class="s2">&quot;</span> 
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">retrieved_chunks</span><span class="p">:</span>
            <span class="n">page_content</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">page_content</span>
            <span class="n">resources</span> <span class="o">+=</span> <span class="s2">&quot;[DOCUMENT_CONTENT]: &quot;</span> <span class="o">+</span> <span class="n">page_content</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            
        <span class="k">return</span> <span class="n">resources</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpc_retriever</span> <span class="o">=</span> <span class="n">SemanticRetriever</span><span class="p">(</span><span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;hpc_contextualized_wiki&quot;</span><span class="p">,</span> <span class="n">chroma_path</span> <span class="o">=</span> <span class="n">CHROMA_PATH</span><span class="p">,</span> <span class="n">embedder_name</span> <span class="o">=</span> <span class="n">EMBEDDER</span><span class="p">,</span> <span class="n">reranker_name</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">slurm_retriever</span> <span class="o">=</span> <span class="n">SlurmDocumentationRetriever</span><span class="p">()</span>
<span class="n">linux_retriever</span> <span class="o">=</span> <span class="n">LinuxDocumentationRetriever</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/leonardo/home/userinternal/rmioli00/git/cinecaxtpc25/session_5/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="workflow">
<h1>Workflow<a class="headerlink" href="#workflow" title="Link to this heading"></a></h1>
<p>We can ask the llm what is the most relevant collection given a question. Then, we trigger search into that collection.</p>
<p><img alt="workflow" src="../../../../_images/router_rag.drawio.png" /></p>
<p>We need to <strong>constrain the model’s answers to a specified set of categories</strong> (e.g., HPC, Slurm, etc.). We can reduce unwanted outputs from the model (e.g., prepending “Absolutely, here is your category”) when returning answers by using guided decoding.</p>
<p>Unlike standard decoding methods such as greedy search, sampling, or beam search, guided decoding incorporates additional rules during generation to influence the model’s output. During token generation, the model’s output logits (the scores before applying softmax) are influenced by <strong>masking invalid options. Valid options are specified using Pydantic models</strong>, JSON schemas, or similar tools.</p>
<p><img alt="guided" src="../../../../_images/method1.png" /><br />
Image courtesy of: https://lmsys.org/blog/2024-02-05-compressed-fsm/.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Collection</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">Enum</span><span class="p">):</span>
    <span class="n">LINUX</span> <span class="o">=</span> <span class="s2">&quot;Linux&quot;</span>
    <span class="n">SLURM</span> <span class="o">=</span> <span class="s2">&quot;Slurm&quot;</span>
    <span class="n">HPC</span> <span class="o">=</span> <span class="s2">&quot;HPC&quot;</span>
    <span class="n">NA</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span>

<span class="k">class</span><span class="w"> </span><span class="nc">QueryCategory</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">category</span><span class="p">:</span><span class="n">Collection</span>

<span class="k">def</span><span class="w"> </span><span class="nf">documentation_router</span><span class="p">(</span><span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
    <span class="n">few_shot_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Given the following query, answer with a category chosen from the following:</span>
<span class="s2">- Linux: Use this category if the question is about Linux.</span>
<span class="s2">- Slurm: Use this category if the question is a general question about Slurm.</span>
<span class="s2">- HPC: Use this category if the question is about Cineca, Leonardo, or is a specific Slurm question about how Slurm is configured on Leonardo.</span>
<span class="s2">- None: Use this category if the question is generic and does not fall into any of the previous categories.</span>

<span class="s2">Here are some examples:</span>
<span class="s2">[QUERY]: How many GPUs does Leonardo have?</span>
<span class="s2">[ANSWER]: HPC</span>
<span class="s2">[QUERY]: Have you ever read Lord of the Rings?</span>
<span class="s2">[ANSWER]: None</span>
<span class="s2">[QUERY]: What is Linus Torvalds&#39; favourite distro?</span>
<span class="s2">[ANSWER]: Linux</span>
<span class="s2">[QUERY]: When was Slurm invented?</span>
<span class="s2">[ANSWER]: Slurm</span>

<span class="s2">[QUERY]: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span>
<span class="s2">[ANSWER]:&quot;</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">QueryCategory</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">([(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;Answer accordingly to the user&#39;s instructions.&quot;</span><span class="p">),</span>
                           <span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">few_shot_prompt</span><span class="p">)])</span>

    <span class="k">if</span> <span class="n">answer</span><span class="o">.</span><span class="n">category</span> <span class="ow">is</span> <span class="n">Collection</span><span class="o">.</span><span class="n">LINUX</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">linux_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">answer</span><span class="o">.</span><span class="n">category</span> <span class="ow">is</span> <span class="n">Collection</span><span class="o">.</span><span class="n">HPC</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">hpc_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">answer</span><span class="o">.</span><span class="n">category</span> <span class="ow">is</span> <span class="n">Collection</span><span class="o">.</span><span class="n">SLURM</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">slurm_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;[RETRIEVED_RESOURCES]: No resources where retrieved for this question.&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query_set</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Tell me something about Linux&quot;</span><span class="p">,</span>
             <span class="s2">&quot;Tell me something about Slurm&quot;</span><span class="p">,</span>
             <span class="s2">&quot;What&#39;s the weather like in Italy?&quot;</span><span class="p">,</span>
             <span class="s2">&quot;What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?&quot;</span><span class="p">,</span>
             <span class="s2">&quot;What is the scheduler used on leonardo? slurm or pbs?&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">query_set</span><span class="p">:</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">documentation_router</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span>
    <span class="n">question</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[QUESTION]:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="se">\n\n</span><span class="si">{</span><span class="n">documents</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[ANSWER]: &quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">answer</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=======================================================&quot;</span><span class="p">)</span>
    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[QUESTION]:

Tell me something about Linux

[RETRIEVED_RESOURCES]:

[DOCUMENT_CONTENT]: Disk and Filesystem Management: Managing disks and partitions using fdisk, lsblk, blkid, formatting with mkfs, mounting filesystems with mount/umount, and checking disk usage with df and du.


[ANSWER]: 
Linux is a widely-used open-source operating system known for its stability, security, and flexibility. It is based on the Unix operating system and is often used in servers, desktops, and embedded systems. Here are some key points about Linux, particularly focusing on disk and filesystem management:

### Disk and Filesystem Management

1. **Managing Disks and Partitions:**
   - **fdisk:** A command-line utility used to create, delete, and manipulate disk partitions. It provides a text-based interface for managing disk partitions.
   - **lsblk:** Lists information about all available or the specified block devices. It provides a tree-like structure of the block devices.
   - **blkid:** Displays or manipulates attributes of block devices. It is useful for identifying the type of filesystem on a device.

2. **Formatting Filesystems:**
   - **mkfs:** A command used to create a filesystem on a device. It can be used with various options to specify the type of filesystem (e.g., ext4, xfs, vfat).

3. **Mounting Filesystems:**
   - **mount:** Used to mount a filesystem, making it accessible to the system. For example, `mount /dev/sda1 /mnt` mounts the partition `/dev/sda1` to the directory `/mnt`.
   - **umount:** Used to unmount a filesystem, making it no longer accessible. For example, `umount /mnt` unmounts the filesystem mounted at `/mnt`.

4. **Checking Disk Usage:**
   - **df:** Reports the amount of disk space used and available on filesystem mounts. It is useful for checking the overall disk usage.
   - **du:** Estimates filesystem space usage. It can be used to check the size of directories and files. For example, `du -sh /path/to/directory` shows the total size of the directory in a human-readable format.

### Other Key Features of Linux

- **Open Source:** Linux is open-source, meaning its source code is freely available for anyone to view, modify, and distribute.
- **Customizable:** Users can customize almost every aspect of the system, from the desktop environment to the kernel.
- **Security:** Linux is known for its robust security features, including user permissions, firewalls, and regular security updates.
- **Community Support:** A large and active community provides support, documentation, and software development.
- **Versatility:** Linux can run on a wide range of hardware, from small embedded systems to large-scale servers and supercomputers.

### Popular Distributions

- **Ubuntu:** Known for its user-friendly interface and extensive community support.
- **Fedora:** A cutting-edge distribution that often includes the latest software and technologies.
- **CentOS:** A stable and enterprise-grade distribution, often used in server environments.
- **Debian:** Known for its stability and extensive software repositories.

Linux continues to evolve, with new features and improvements being added regularly, making it a powerful and versatile operating system for a wide range of applications.
=======================================================
[QUESTION]:

Tell me something about Slurm

[RETRIEVED_RESOURCES]:

[DOCUMENT_CONTENT]: Basic SLURM User Commands: Commands like sbatch (submit jobs), squeue (check job queue), scontrol show job (inspect job details), scancel (cancel jobs), and srun or salloc (interactive job execution).


[ANSWER]: 
Slurm (Simple Linux Utility for Resource Management) is an open-source workload manager designed for Linux clusters. It provides a framework for managing and scheduling jobs on high-performance computing (HPC) systems. Slurm is widely used in academic, research, and industrial settings to efficiently allocate and manage computational resources.

Here are some basic Slurm user commands and their functions:

1. **sbatch**: This command is used to submit a job script to the Slurm job scheduler. The job script typically contains the commands and parameters needed to run the job.

   ```sh
   sbatch job_script.sh
   ```

2. **squeue**: This command allows users to check the status of jobs in the queue. It provides information about jobs that are pending, running, or completed.

   ```sh
   squeue
   ```

3. **scontrol show job**: This command is used to inspect detailed information about a specific job. It provides comprehensive details about the job&#39;s status, resource usage, and other attributes.

   ```sh
   scontrol show job &lt;job_id&gt;
   ```

4. **scancel**: This command is used to cancel a job that is either pending or running. It is useful when you need to stop a job for any reason.

   ```sh
   scancel &lt;job_id&gt;
   ```

5. **srun or salloc**: These commands are used for interactive job execution. `srun` is typically used to run a single command or a script interactively, while `salloc` allocates resources for an interactive session.

   ```sh
   srun --pty /bin/bash
   ```

   ```sh
   salloc -N 1 -t 01:00:00
   ```

Slurm is highly configurable and can be tailored to fit the specific needs of different HPC environments. It supports a wide range of features, including job scheduling, resource management, and accounting, making it a powerful tool for managing complex computational workloads.
=======================================================
[QUESTION]:

What&#39;s the weather like in Italy?

[RETRIEVED_RESOURCES]: No resources where retrieved for this question.
[ANSWER]: 
I&#39;m sorry, but I don&#39;t have real-time access to the internet to provide current weather information. However, you can easily find the weather in Italy by checking a reliable weather website or application, such as The Weather Channel, AccuWeather, or your local weather service. These platforms provide up-to-date forecasts and detailed information for various regions in Italy.
=======================================================
[QUESTION]:

What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?

[RETRIEVED_RESOURCES]:

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: .. tab-item:: Booster

        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**   | **Priority** | **Notes**                           |
        +================+====================+=========================+==============+=================================+==============+=====================================+
        | lrd_all_serial | normal             | 4 cores                 | 04:00:00     | 1 node / 4 cores                | 40           | No GPUs

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: , Hyperthreading x 2         |
        |                |                    |                         |              |                                 |              |                                     |
        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                  |              | **Budget Free**                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_usr_prod | normal             | 64 nodes                | 24:00:00     |                                 | 40           |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_dbg      | 2 nodes                 | 00:30:00     | 2 nodes / 64 cores / 8 GPUs     | 80           |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_bprod    | min = 65 nodes          | 24:00:00     | 256 nodes                       | 60           |                                     |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    | max = 256 nodes         |              |                                 |              |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_lprod    | 3 nodes                 | 4-00:00:00   | 3 nodes / 12 GPUs               | 40           |                                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_fua_dbg  | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 64 cores / 8 GPUs     | 40           | Runs on 2 nodes                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_fua_prod | normal             | 16 nodes                | 24:00:00     | 4 running jobs per user account | 40           |                                     |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    |                         |              | 32 nodes / 3584 cores           |              |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_fuabprod | min = 17 nodes          | 24:00:00     | 32 nodes / 3584 cores           | 60           | Runs on 49 nodes                    |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    | max = 32 nodes          |              |                                 |              | Min is 17 FULL nodes                |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                 | 0            |                                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: .. _leonardo_card:

Leonardo
========

Leonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.
This guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.

.. |ico2| image:: img/leonardo_logo.png
   :height: 55px
   :class: no-scaled-link

Access to the System
--------------------

The machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. 

The connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:

 * login01-ext.leonardo.cineca.it
 * login02-ext.leonardo.cineca.it
 * login05-ext.leonardo.cineca.it
 * login07-ext.leonardo.cineca.it

.. warning::
    
    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.

System Architecture
-------------------

The cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:

* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.
* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.

The overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. 

The **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.
The **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.

Hardware Details
^^^^^^^^^^^^^^^^

.. tab-set::

    .. tab-item:: Booster


[ANSWER]: 
Based on the provided resources, the Quality of Service (QoS) queues available on the Leonardo supercomputer&#39;s BOOSTER partition are:

1. **normal** (default for `lrd_all_serial` and `boost_usr_prod`)
2. **boost_qos_dbg**
3. **boost_qos_bprod**
4. **boost_qos_lprod**
5. **boost_qos_fuabprod**
6. **qos_fualowprio**

These QoS queues define different job characteristics such as the number of cores/GPUs per job, walltime, maximum nodes/cores/GPUs per user, and priority.
=======================================================
[QUESTION]:

What is the scheduler used on leonardo? slurm or pbs?

[RETRIEVED_RESOURCES]:

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: .. tab-item:: Booster

        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**   | **Priority** | **Notes**                           |
        +================+====================+=========================+==============+=================================+==============+=====================================+
        | lrd_all_serial | normal             | 4 cores                 | 04:00:00     | 1 node / 4 cores                | 40           | No GPUs

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: .. dropdown:: **$TMPDIR**

 * on the local SSD disks on login nodes (14 TB of capacity), mounted as ``/scratch_local`` (``TMPDIR=/scratch_local``). This is a shared area with no quota, remove all the files once they are not requested anymore. A cleaning procedure will be enforced in case of improper use of the area.   
 
 * on the local SSD disks on the serial node (``lrd_all_serial``, 14TB of capacity), managed via the Slurm ``job_container/tmpfs plugin``. This plugin provides a *job-specific*, private temporary file system space, with private instances of ``/tmp`` and ``/dev/shm`` in the job&#39;s user space (``TMPDIR=/tmp``, visible via the command ``df -h``), removed at the end of the serial job. You can request the resource via sbatch directive or srun option ``--gres=tmpfs:XX`` (for instance: ``--gres=tmpfs:200G``), with a maximum of 1 TB for the serial jobs. If not explicitly requested, the ``/tmp`` has the default dimension of 10 GB.
 
 * on the local SSD disks on DCGP nodes (3 TB  of capacity). As for the serial node, the local ``/tmp`` and ``/dev/shm`` areas are managed via plugin, which at the start of the jobs mounts private instances of ``/tmp`` and ``/dev/shm`` in the job&#39;s user space (``TMPDIR=/tmp``, visible via the command ``df -h /tmp``), and unmounts them at the end of the job (all data will be lost). You can request the resource via sbatch directive or srun option ``--gres=tmpfs:XX``, with a maximum of all the available 3 TB for DCGP nodes. As for the serial node, if not explicitly requested, the ``/tmp`` has the default dimension of 10 GB. Please note: for the DCGP jobs the requested amount of ``gres/tmpfs`` resource contributes to the consumed budget, changing the number of accounted equivalent core hours, see the dedicated section on the Accounting.
 
 * on RAM on the diskless booster nodes (with a fixed size of 10 GB, no increase is allowed, and the ``gres/tmpfs`` resource is disabled).

Job Managing and Slurm Partitions 
---------------------------------

In the following table you can find informations about the Slurm partitions for **Booster** and **DCGP** partitions.  

.. seealso:: 
  Further information about job submission are reported in the general section :ref:`hpc/hpc_scheduler:Scheduler and Job Submission`. 

.. tab-set::

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: , Hyperthreading x 2         |
        |                |                    |                         |              |                                 |              |                                     |
        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                  |              | **Budget Free**                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_usr_prod | normal             | 64 nodes                | 24:00:00     |                                 | 40           |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_dbg      | 2 nodes                 | 00:30:00     | 2 nodes / 64 cores / 8 GPUs     | 80           |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_bprod    | min = 65 nodes          | 24:00:00     | 256 nodes                       | 60           |                                     |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    | max = 256 nodes         |              |                                 |              |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_lprod    | 3 nodes                 | 4-00:00:00   | 3 nodes / 12 GPUs               | 40           |                                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_fua_dbg  | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 64 cores / 8 GPUs     | 40           | Runs on 2 nodes                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_fua_prod | normal             | 16 nodes                | 24:00:00     | 4 running jobs per user account | 40           |                                     |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    |                         |              | 32 nodes / 3584 cores           |              |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_fuabprod | min = 17 nodes          | 24:00:00     | 32 nodes / 3584 cores           | 60           | Runs on 49 nodes                    |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    | max = 32 nodes          |              |                                 |              | Min is 17 FULL nodes                |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                 | 0            |                                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+


[ANSWER]: 
The scheduler used on Leonardo is **Slurm**. This is evident from the references to Slurm-specific commands and configurations in the provided documentation, such as the use of `sbatch` and `srun` commands, and the mention of Slurm partitions and the Slurm `job_container/tmpfs plugin`.
=======================================================
</pre></div>
</div>
</div>
</div>
<p>Pros:</p>
<ul class="simple">
<li><p>Except for the llm categorization part, you are in control of the workflow;</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>If you add more datasources, you will need to update your code and the few shot prompt;</p></li>
<li><p>If you code a bunch of if-then elses clauses (like in this case) you can miss cases where it is useful to search for a question in multiple document bases. However, can solve this by asking the model to return a list of categories and then launching the semantic search step on each category…</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="agents-and-agentic-rag">
<h1>Agents and agentic RAG<a class="headerlink" href="#agents-and-agentic-rag" title="Link to this heading"></a></h1>
<p>In an agentic rag setup, we leave to the model to burden of deciding where has to be searched. Basically, the model “has agency”.
Let’s build an agent from scratch and then let’s use a framework to do the same thing.</p>
<p>Agents use the <strong>ReAct Pattern</strong>. ReAct is based on <strong>Thought-Action-Observation</strong> cycle.</p>
<p><img alt="react_flow" src="../../../../_images/react.png" />
Image courtesy of: https://www.ibm.com/think/topics/react-agent.</p>
<ul class="simple">
<li><p>Thought: the LLM checks the conversation and decides what the next step should be;</p></li>
<li><p>Action: A tool is called or a snippet of code is written;</p>
<ul>
<li><p>An external parser executes the snippet of code or the tool the LLM has called;</p></li>
</ul>
</li>
<li><p>Observation: The result of the function call is added to the history of messages and the model is given this result.
These steps are specified in the system prompt and are executed in loop until the model decides that the task was accomplished.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">apply_chat_template</span><span class="p">(</span><span class="n">system_prompt</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">user_messages</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">system_messages</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Util function to create a chat history with the correct format.&quot;&quot;&quot;</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span> <span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span> <span class="p">:</span> <span class="n">system_prompt</span><span class="p">}]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">user_messages</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">system_messages</span><span class="p">))):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span> <span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span> <span class="p">:</span> <span class="n">user_messages</span><span class="p">[</span><span class="n">i</span><span class="p">]})</span>
        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span> <span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span> <span class="p">:</span> <span class="n">system_messages</span><span class="p">[</span><span class="n">i</span><span class="p">]})</span>
        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="k">return</span> <span class="n">messages</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">search_hpc_wiki</span><span class="p">(</span><span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Retrieves relevant information from Cineca&#39;s HPC internal wiki in response to a natural language query, returning the results as a plain text string.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        - query: a string with the user query.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: a string with the retrieved context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">hpc_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">search_linux_wiki</span><span class="p">(</span><span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns general trivia from the internal Linux documentation wiki in response to a natural language query, returning the results as a plain text string.</span>
<span class="sd"> </span>
<span class="sd">    Arguments:</span>
<span class="sd">        - query: a string with the user query.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: a string with the retrieved context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">linux_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    
<span class="k">def</span><span class="w"> </span><span class="nf">search_slurm_wiki</span><span class="p">(</span><span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Retrieves general trivia from the internal Slurm documentation wiki in response to a natural language query, returning the results as a plain text string.</span>
<span class="sd"> </span>
<span class="sd">    Arguments:</span>
<span class="sd">        - query: a string with the user query.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: a string with the retrieved context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">slurm_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[NAME]: &quot;</span> <span class="o">+</span> <span class="n">search_hpc_wiki</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[DESCRIPTION]: &quot;</span> <span class="o">+</span> <span class="n">search_hpc_wiki</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[NAME]: search_hpc_wiki
[DESCRIPTION]: Retrieves relevant information from Cineca&#39;s HPC internal wiki in response to a natural language query, returning the results as a plain text string.

    Arguments:
        - query: a string with the user query.

    Returns:
        str: a string with the retrieved context.
    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_hpc_wiki</span><span class="p">,</span> <span class="n">search_linux_wiki</span><span class="p">,</span> <span class="n">search_slurm_wiki</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">functions_to_str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="n">functions_to_str</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="vm">__doc__</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">functions</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">functions_to_str</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>- search_hpc_wiki: Retrieves relevant information from Cineca&#39;s HPC internal wiki in response to a natural language query, returning the results as a plain text string.

    Arguments:
        - query: a string with the user query.

    Returns:
        str: a string with the retrieved context.
    
- search_linux_wiki: Returns general trivia from the internal Linux documentation wiki in response to a natural language query, returning the results as a plain text string.

    Arguments:
        - query: a string with the user query.

    Returns:
        str: a string with the retrieved context.
    
- search_slurm_wiki: Retrieves general trivia from the internal Slurm documentation wiki in response to a natural language query, returning the results as a plain text string.

    Arguments:
        - query: a string with the user query.

    Returns:
        str: a string with the retrieved context.
    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FUNCTION_CALLING_PROMPT</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;You are a helpful assistant trained to answer user questions. &quot;</span>
    <span class="s2">&quot;You will receive a question from the user. You must answer using your knowledge or &quot;</span>
    <span class="s2">&quot;tools. Here is a list of the available tools.</span><span class="se">\n\n</span><span class="s2">[TOOLS]:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">functions_to_str</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;When you call a tool you need to use the following format: &quot;</span>
    <span class="s2">&quot;TOOL_CALL: {&#39;tool_name&#39;: &lt;name_of_the_tool&gt;, &#39;parameters&#39;:[&#39;&lt;parameter_value&gt;&#39;]}</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;Here is an example: TOOL_CALL: {&#39;tool_name&#39;: &#39;count_occurrences&#39;, &#39;parameters&#39;: [&#39;s&#39;, &#39;strawberry&#39;]}</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;Use only the tools specified in the TOOLS section. Do not invent tools.</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;The tool you choose will be executed by an external Python interpreter and &quot;</span>
    <span class="s2">&quot;the result will be given to you in the following form: TOOL_RESULT: &lt;value&gt;.</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;You can use the tool result to give a final answer. &quot;</span>
    <span class="s2">&quot;When you know the final answer, always start your answer with FINAL_ANSWER:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;Do not invent answers. &quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">FUNCTION_CALLING_PROMPT</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>You are a helpful assistant trained to answer user questions. You will receive a question from the user. You must answer using your knowledge or tools. Here is a list of the available tools.

[TOOLS]:
- search_hpc_wiki: Retrieves relevant information from Cineca&#39;s HPC internal wiki in response to a natural language query, returning the results as a plain text string.

    Arguments:
        - query: a string with the user query.

    Returns:
        str: a string with the retrieved context.
    
- search_linux_wiki: Returns general trivia from the internal Linux documentation wiki in response to a natural language query, returning the results as a plain text string.

    Arguments:
        - query: a string with the user query.

    Returns:
        str: a string with the retrieved context.
    
- search_slurm_wiki: Retrieves general trivia from the internal Slurm documentation wiki in response to a natural language query, returning the results as a plain text string.

    Arguments:
        - query: a string with the user query.

    Returns:
        str: a string with the retrieved context.
    
When you call a tool you need to use the following format: TOOL_CALL: {&#39;tool_name&#39;: &lt;name_of_the_tool&gt;, &#39;parameters&#39;:[&#39;&lt;parameter_value&gt;&#39;]}
Here is an example: TOOL_CALL: {&#39;tool_name&#39;: &#39;count_occurrences&#39;, &#39;parameters&#39;: [&#39;s&#39;, &#39;strawberry&#39;]}
Use only the tools specified in the TOOLS section. Do not invent tools.
The tool you choose will be executed by an external Python interpreter and the result will be given to you in the following form: TOOL_RESULT: &lt;value&gt;.
You can use the tool result to give a final answer. When you know the final answer, always start your answer with FINAL_ANSWER:
Do not invent answers. 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="n">apply_chat_template</span><span class="p">(</span><span class="n">FUNCTION_CALLING_PROMPT</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;What GPUs are used on Leonardo?&quot;</span><span class="p">],</span> <span class="p">[])</span> 
<span class="n">completion</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">completion</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TOOL_CALL: {&#39;tool_name&#39;: &#39;search_hpc_wiki&#39;, &#39;parameters&#39;: [&#39;What GPUs are used on Leonardo?&#39;]}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">model_ans_parser</span><span class="p">(</span><span class="n">model_answ</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">user_queries</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calls a tool if the model answer was a tool call.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">TOOL_CALL_PLACEHOLDER</span> <span class="o">=</span> <span class="s2">&quot;TOOL_CALL: &quot;</span>
    <span class="c1"># This is a function call, we need to parse model output</span>
    <span class="k">if</span> <span class="n">TOOL_CALL_PLACEHOLDER</span> <span class="ow">in</span> <span class="n">model_answ</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">answer</span> <span class="o">=</span> <span class="n">model_answ</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">answer</span> <span class="o">=</span> <span class="n">answer</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">TOOL_CALL_PLACEHOLDER</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Cast the string to a real dict</span>
            <span class="n">answer</span> <span class="o">=</span> <span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
            <span class="c1"># Cast string to function</span>
            <span class="n">function_name</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">answer</span><span class="p">[</span><span class="s2">&quot;tool_name&quot;</span><span class="p">])</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">answer</span><span class="p">[</span><span class="s2">&quot;parameters&quot;</span><span class="p">]</span>
            <span class="c1"># Eval the expression</span>
            <span class="n">user_queries</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TOOL_RESULT: </span><span class="si">{</span><span class="n">function_name</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model_answ</span><span class="p">,</span> <span class="n">user_queries</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key error while parsing model answer&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># This is not a function call, we simply return the model answ list</span>
        <span class="k">return</span> <span class="n">model_answ</span><span class="p">,</span> <span class="n">user_queries</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model_ans_parser</span><span class="p">([</span><span class="n">completion</span><span class="o">.</span><span class="n">content</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Tell me something about Linux&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>([&quot;TOOL_CALL: {&#39;tool_name&#39;: &#39;search_hpc_wiki&#39;, &#39;parameters&#39;: [&#39;What GPUs are used on Leonardo?&#39;]}&quot;], [&#39;Tell me something about Linux&#39;, &#39;TOOL_RESULT: [RETRIEVED_RESOURCES]:\n\n[DOCUMENT_TITLE]: leonardo.rst.txt\n[DOCUMENT_CONTENT]: .. _leonardo_card:\n\nLeonardo\n========\n\nLeonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.\nThis guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.\n\n.. |ico2| image:: img/leonardo_logo.png\n   :height: 55px\n   :class: no-scaled-link\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. \n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:\n\n * login01-ext.leonardo.cineca.it\n * login02-ext.leonardo.cineca.it\n * login05-ext.leonardo.cineca.it\n * login07-ext.leonardo.cineca.it\n\n.. warning::\n    \n    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\nSystem Architecture\n-------------------\n\nThe cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:\n\n* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.\n* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.\n\nThe overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. \n\nThe **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.\nThe **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.\n\nHardware Details\n^^^^^^^^^^^^^^^^\n\n.. tab-set::\n\n    .. tab-item:: Booster\n\n[DOCUMENT_TITLE]: leonardo.rst.txt\n[DOCUMENT_CONTENT]: .. list-table:: \n            :widths: 30 50\n            :header-rows: 1\n\n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2135, Da Vinci single-node GPU\n            * - Racks\n              - 116\n            * - Nodes\n              - 3456\n            * - Processors/node\n              - 1x `Intel Ice Lake Intel Xeon Platinum 8358 &lt;https://www.intel.com/content/www/us/en/products/sku/212282/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 32\n            * - Accelerators/node\n              - 4x `NVIDIA Ampere100 custom &lt;https://doi.org/10.17815/jlsrf-8-186&gt;`_, 64GiB HBM2e NVLink 3.0 (200 GB/s)\n            * - Local Storage/node (tmfs)\n              - (none)\n            * - RAM/node \n              - 512 GiB DDR4 3200 MHz\n            * - Rmax\n              - 241.2 PFlop/s (`top500 &lt;https://www.top500.org/system/180128/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology \n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n    .. tab-item:: DCGP\n\n        .. list-table::\n            :widths: 30 50\n            :header-rows: 1\n            \n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2140 three-node CPU blade\n            * - Racks\n              - 22\n            * - Nodes\n              - 1536\n            * - Processors/node\n              - 2x `Intel Sapphire Rapids Intel Xeon Platinum 8480+ &lt;https://www.intel.com/content/www/us/en/products/sku/231746/intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 112 cores/node\n            * - Accelerators\n              - (none)\n            * - Local Storage/node (tmfs)\n              - 3 TiB\n            * - RAM/node\n              - 512(8x64) GiB DDR5 4800 MHz\n            * - Rmax\n              - 7.84 PFlop/s (`top500 &lt;https://www.top500.org/system/180204/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology\n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n\nFile Systems and Data Managment\n-------------------------------\n\nThe storage organization conforms to **CINECA** infrastructure. General information are reported in :ref:`hpc/hpc_data_storage:File Systems and Data Management` section. In the following, only differences with respect to general behavior are listed and explained.\n\n[DOCUMENT_TITLE]: leonardo.rst.txt\n[DOCUMENT_CONTENT]: +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**        | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+======================================+==============+=====================================+\n        | lrd_all_serial | normal             | max = 4 cores           | 04:00:00     | 1 node / 4 cores                     | 40           | Hyperthreading x 2                  |\n        |                |                    |                         |              |                                      |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                       |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_usr_prod  | normal             | 16 nodes                | 24:00:00     | 512 nodes per prj. account           | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_dbg       | 2 nodes                 | 00:30:00     | 2 nodes / 224 cores per user account | 80           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_bprod     | min = 17 nodes          | 24:00:00     | 128 nodes per user account           | 60           | GrpTRES = 1536 nodes                |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 128 nodes         |              | 512 nodes per prj. account           |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_lprod     | 3 nodes                 | 4-00:00:00   | 3 nodes / 336 cores per user account | 40           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_dbg   | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 224 cores                  | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_prod  | normal             | 16 nodes                | 24:00:00     |                                      | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n\n&#39;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">check_final_answer</span><span class="p">(</span><span class="n">model_answ</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
    <span class="n">FINAL_ANSW_PLACEHOLDER</span> <span class="o">=</span> <span class="s2">&quot;FINAL_ANSWER:&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_answ</span><span class="p">):</span>
        <span class="c1"># Check the last answer</span>
        <span class="k">if</span> <span class="n">FINAL_ANSW_PLACEHOLDER</span> <span class="ow">in</span> <span class="n">model_answ</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="c1"># Last answer had the final answ placeholder</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Last answer was not the final answer</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># First interaction, model answ is empty</span>
        <span class="k">return</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<p>Now we just need a while loop so that model will continue to call function until he has all the resources needed to produce his final answer. Remember that the final answer is different from all other messges because will contain the “FINAL_ANSWER” tag.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">user_queries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;What GPUs are used on Leonardo?&quot;</span><span class="p">]</span>
<span class="n">model_answers</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">check_final_answer</span><span class="p">(</span><span class="n">model_answers</span><span class="p">):</span>
    <span class="c1"># Build the chat history</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">apply_chat_template</span><span class="p">(</span><span class="n">FUNCTION_CALLING_PROMPT</span><span class="p">,</span> <span class="n">user_queries</span><span class="p">,</span> <span class="n">model_answers</span><span class="p">)</span> 
    <span class="c1"># Send messages to the model</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="c1"># Extract model answer</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">completion</span><span class="o">.</span><span class="n">content</span>
    <span class="c1"># Append the answer to the list of hanswers</span>
    <span class="n">model_answers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
    <span class="c1"># Parse the answer</span>
    <span class="n">model_answers</span><span class="p">,</span> <span class="n">user_queries</span> <span class="o">=</span> <span class="n">model_ans_parser</span><span class="p">(</span><span class="n">model_answers</span><span class="p">,</span> <span class="n">user_queries</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model_answers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FINAL_ANSWER: The GPUs used on Leonardo are NVIDIA Ampere A100-64 accelerators.
</pre></div>
</div>
</div>
</div>
<p>Let’s inspect what happened.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">FUNCTION_CALLING_PROMPT</span><span class="p">,</span> <span class="n">user_queries</span><span class="p">,</span> <span class="n">model_answers</span><span class="p">)</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &quot;You are a helpful assistant trained to answer user questions. You will receive a question from the user. You must answer using your knowledge or tools. Here is a list of the available tools.\n\n[TOOLS]:\n- search_hpc_wiki: Retrieves relevant information from Cineca&#39;s HPC internal wiki in response to a natural language query, returning the results as a plain text string.\n\n    Arguments:\n        - query: a string with the user query.\n\n    Returns:\n        str: a string with the retrieved context.\n    \n- search_linux_wiki: Returns general trivia from the internal Linux documentation wiki in response to a natural language query, returning the results as a plain text string.\n\n    Arguments:\n        - query: a string with the user query.\n\n    Returns:\n        str: a string with the retrieved context.\n    \n- search_slurm_wiki: Retrieves general trivia from the internal Slurm documentation wiki in response to a natural language query, returning the results as a plain text string.\n\n    Arguments:\n        - query: a string with the user query.\n\n    Returns:\n        str: a string with the retrieved context.\n    \nWhen you call a tool you need to use the following format: TOOL_CALL: {&#39;tool_name&#39;: &lt;name_of_the_tool&gt;, &#39;parameters&#39;:[&#39;&lt;parameter_value&gt;&#39;]}\nHere is an example: TOOL_CALL: {&#39;tool_name&#39;: &#39;count_occurrences&#39;, &#39;parameters&#39;: [&#39;s&#39;, &#39;strawberry&#39;]}\nUse only the tools specified in the TOOLS section. Do not invent tools.\nThe tool you choose will be executed by an external Python interpreter and the result will be given to you in the following form: TOOL_RESULT: &lt;value&gt;.\nYou can use the tool result to give a final answer. When you know the final answer, always start your answer with FINAL_ANSWER:\nDo not invent answers. &quot;}, {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;What GPUs are used on Leonardo?&#39;}, {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;TOOL_CALL: {&#39;tool_name&#39;: &#39;search_hpc_wiki&#39;, &#39;parameters&#39;: [&#39;What GPUs are used on Leonardo?&#39;]}&quot;}, {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;TOOL_RESULT: [RETRIEVED_RESOURCES]:\n\n[DOCUMENT_TITLE]: leonardo.rst.txt\n[DOCUMENT_CONTENT]: .. _leonardo_card:\n\nLeonardo\n========\n\nLeonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.\nThis guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.\n\n.. |ico2| image:: img/leonardo_logo.png\n   :height: 55px\n   :class: no-scaled-link\n\nAccess to the System\n--------------------\n\nThe machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. \n\nThe connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:\n\n * login01-ext.leonardo.cineca.it\n * login02-ext.leonardo.cineca.it\n * login05-ext.leonardo.cineca.it\n * login07-ext.leonardo.cineca.it\n\n.. warning::\n    \n    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.\n\nSystem Architecture\n-------------------\n\nThe cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:\n\n* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.\n* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.\n\nThe overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. \n\nThe **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.\nThe **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.\n\nHardware Details\n^^^^^^^^^^^^^^^^\n\n.. tab-set::\n\n    .. tab-item:: Booster\n\n[DOCUMENT_TITLE]: leonardo.rst.txt\n[DOCUMENT_CONTENT]: .. list-table:: \n            :widths: 30 50\n            :header-rows: 1\n\n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2135, Da Vinci single-node GPU\n            * - Racks\n              - 116\n            * - Nodes\n              - 3456\n            * - Processors/node\n              - 1x `Intel Ice Lake Intel Xeon Platinum 8358 &lt;https://www.intel.com/content/www/us/en/products/sku/212282/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 32\n            * - Accelerators/node\n              - 4x `NVIDIA Ampere100 custom &lt;https://doi.org/10.17815/jlsrf-8-186&gt;`_, 64GiB HBM2e NVLink 3.0 (200 GB/s)\n            * - Local Storage/node (tmfs)\n              - (none)\n            * - RAM/node \n              - 512 GiB DDR4 3200 MHz\n            * - Rmax\n              - 241.2 PFlop/s (`top500 &lt;https://www.top500.org/system/180128/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology \n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n    .. tab-item:: DCGP\n\n        .. list-table::\n            :widths: 30 50\n            :header-rows: 1\n            \n            * - **Type**\n              - **Specific**\n            * - Models\n              - Atos BullSequana X2140 three-node CPU blade\n            * - Racks\n              - 22\n            * - Nodes\n              - 1536\n            * - Processors/node\n              - 2x `Intel Sapphire Rapids Intel Xeon Platinum 8480+ &lt;https://www.intel.com/content/www/us/en/products/sku/231746/intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz/specifications.html&gt;`_\n            * - CPU/node\n              - 112 cores/node\n            * - Accelerators\n              - (none)\n            * - Local Storage/node (tmfs)\n              - 3 TiB\n            * - RAM/node\n              - 512(8x64) GiB DDR5 4800 MHz\n            * - Rmax\n              - 7.84 PFlop/s (`top500 &lt;https://www.top500.org/system/180204/&gt;`_)\n            * - Internal Network\n              - 200 Gbps NVIDIA Mellanox HDR InfiniBand - Dragonfly+ Topology\n            * - Storage (raw capacity)\n              - 106 PiB based on DDN ES7990X and Hard Drive Disks (Capacity Tier) \n              \n                5.7 PiB based on DDN ES400NVX2 and Solid State Drives (Fast Tier)\n\n\nFile Systems and Data Managment\n-------------------------------\n\nThe storage organization conforms to **CINECA** infrastructure. General information are reported in :ref:`hpc/hpc_data_storage:File Systems and Data Management` section. In the following, only differences with respect to general behavior are listed and explained.\n\n[DOCUMENT_TITLE]: leonardo.rst.txt\n[DOCUMENT_CONTENT]: +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**        | **Priority** | **Notes**                           |\n        +================+====================+=========================+==============+======================================+==============+=====================================+\n        | lrd_all_serial | normal             | max = 4 cores           | 04:00:00     | 1 node / 4 cores                     | 40           | Hyperthreading x 2                  |\n        |                |                    |                         |              |                                      |              |                                     |\n        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                       |              | **Budget Free**                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_usr_prod  | normal             | 16 nodes                | 24:00:00     | 512 nodes per prj. account           | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_dbg       | 2 nodes                 | 00:30:00     | 2 nodes / 224 cores per user account | 80           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_bprod     | min = 17 nodes          | 24:00:00     | 128 nodes per user account           | 60           | GrpTRES = 1536 nodes                |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    | max = 128 nodes         |              | 512 nodes per prj. account           |              | Min is 17 FULL nodes                |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        |                | dcgp_qos_lprod     | 3 nodes                 | 4-00:00:00   | 3 nodes / 336 cores per user account | 40           |                                     |\n        |                |                    |                         |              |                                      |              |                                     |\n        |                |                    |                         |              | 512 nodes per prj. account           |              |                                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_dbg   | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 224 cores                  | 40           | Runs on 2 nodes                     |\n        +----------------+--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n        | dcgp_fua_prod  | normal             | 16 nodes                | 24:00:00     |                                      | 40           |                                     |\n        +                +--------------------+-------------------------+--------------+--------------------------------------+--------------+-------------------------------------+\n\n&#39;}, {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;FINAL_ANSWER: The GPUs used on Leonardo are NVIDIA Ampere A100-64 accelerators.&#39;}]
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="considerations">
<h1>Considerations<a class="headerlink" href="#considerations" title="Link to this heading"></a></h1>
<p>Obviously the loop we scripted is very educative, but not production ready (e.g. for function call we should have used guided decoding, error handling is missing, etc). In real industrial scenarios you probably want to use something more “battle tested”.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="langchain">
<h1>Langchain<a class="headerlink" href="#langchain" title="Link to this heading"></a></h1>
<p>Here we use the lang-graph framework. The document base becomes a tool of our agent. The method <code class="docutils literal notranslate"><span class="pre">create_react_agent</span></code> creates a Langchain graph which encodes what we scripted before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">search_hpc_wiki</span><span class="p">(</span><span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Retrieves relevant information from Cineca&#39;s HPC internal wiki in response to a natural language query, returning the results as a plain text string.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">        - query: a string with the user query.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: a string with the retrieved context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">hpc_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">search_linux_wiki</span><span class="p">(</span><span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns general trivia from the internal Linux documentation wiki in response to a natural language query, returning the results as a plain text string.</span>
<span class="sd"> </span>
<span class="sd">    Arguments:</span>
<span class="sd">        - query: a string with the user query.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: a string with the retrieved context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">linux_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">search_slurm_wiki</span><span class="p">(</span><span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Retrieves general trivia from the internal Slurm documentation wiki in response to a natural language query, returning the results as a plain text string.</span>
<span class="sd"> </span>
<span class="sd">    Arguments:</span>
<span class="sd">        - query: a string with the user query.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: a string with the retrieved context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">slurm_retriever</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="n">langchain_tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_hpc_wiki</span><span class="p">,</span> <span class="n">search_linux_wiki</span><span class="p">,</span> <span class="n">search_slurm_wiki</span><span class="p">]</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">create_react_agent</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">langchain_tools</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mm</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><img src="https://mermaid.ink/img/LS0tCmNvbmZpZzoKICBmbG93Y2hhcnQ6CiAgICBjdXJ2ZTogbGluZWFyCi0tLQpncmFwaCBURDsKCV9fc3RhcnRfXyhbPHA-X19zdGFydF9fPC9wPl0pOjo6Zmlyc3QKCWFnZW50KGFnZW50KQoJdG9vbHModG9vbHMpCglfX2VuZF9fKFs8cD5fX2VuZF9fPC9wPl0pOjo6bGFzdAoJX19zdGFydF9fIC0tPiBhZ2VudDsKCWFnZW50IC0uLT4gX19lbmRfXzsKCWFnZW50IC0uLT4gdG9vbHM7Cgl0b29scyAtLT4gYWdlbnQ7CgljbGFzc0RlZiBkZWZhdWx0IGZpbGw6I2YyZjBmZixsaW5lLWhlaWdodDoxLjIKCWNsYXNzRGVmIGZpcnN0IGZpbGwtb3BhY2l0eTowCgljbGFzc0RlZiBsYXN0IGZpbGw6I2JmYjZmYwo="/></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">query_set</span><span class="p">:</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">query</span><span class="p">]})</span>

    <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
        <span class="n">message</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;================================================================================&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

Tell me something about Linux
==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================
Tool Calls:
  search_linux_wiki (QaPWBAFPA)
 Call ID: QaPWBAFPA
  Args:
    query: Linux
=================================<span class=" -Color -Color-Bold"> Tool Message </span>=================================
Name: search_linux_wiki

[RETRIEVED_RESOURCES]:

[DOCUMENT_CONTENT]: Linux Distributions: Learn about different Linux distributions (distros) such as Ubuntu, CentOS, Debian, Fedora, and Arch, which package the Linux kernel with user-space utilities and package managers for different use cases.


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

Linux is an open-source operating system kernel that was first released on September 17, 1991, by Linus Torvalds. It has since grown to become one of the most widely used and influential operating systems in the world. Linux is known for its stability, security, and flexibility, making it a popular choice for servers, desktops, and embedded systems.

There are many different Linux distributions, often referred to as &quot;distros,&quot; each tailored to specific use cases and user preferences. Some of the most popular Linux distributions include:

1. **Ubuntu**: Known for its user-friendly interface and extensive community support, Ubuntu is a great choice for both beginners and experienced users. It is based on Debian and is widely used on desktops and servers.

2. **CentOS**: CentOS is a community-supported distribution derived from sources freely provided to the public by Red Hat Enterprise Linux (RHEL). It is popular for server deployments due to its stability and long-term support.

3. **Debian**: One of the oldest and most respected Linux distributions, Debian is known for its strict adherence to free software principles and its extensive repository of software packages.

4. **Fedora**: Sponsored by Red Hat, Fedora is a cutting-edge distribution that often includes the latest software and technologies. It serves as a testing ground for features that may eventually make their way into RHEL.

5. **Arch Linux**: Arch Linux is a lightweight and flexible Linux distribution that tries to Keep It Simple. It is designed for users who want to have complete control over their system and is known for its rolling release model, which provides continuous updates.

Each of these distributions packages the Linux kernel with a set of user-space utilities and a package manager, allowing users to install, update, and manage software easily. The choice of distribution often depends on the user&#39;s specific needs, such as ease of use, stability, or access to the latest features.
================================================================================
================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

Tell me something about Slurm
==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================
Tool Calls:
  search_slurm_wiki (jSarb5swA)
 Call ID: jSarb5swA
  Args:
    query: What is Slurm?
=================================<span class=" -Color -Color-Bold"> Tool Message </span>=================================
Name: search_slurm_wiki

[RETRIEVED_RESOURCES]:

[DOCUMENT_CONTENT]: Security and Policies in SLURM: Managing user authentication, permission controls, node isolation, job sandboxing, enforcing resource limits, and setting job timeout policies within a SLURM-managed HPC cluster.


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

Slurm (Simple Linux Utility for Resource Management) is an open-source workload manager designed for Linux clusters. It is widely used in high-performance computing (HPC) environments to manage and allocate resources efficiently. Slurm provides a framework for managing jobs, nodes, and workloads, ensuring that computational resources are used optimally.

Key features of Slurm include:

1. **Job Scheduling**: Slurm schedules jobs based on resource availability and priority, ensuring that jobs are executed in an efficient manner.
2. **Resource Management**: It manages the allocation of computational resources, including CPUs, GPUs, and memory, across the cluster.
3. **Workload Distribution**: Slurm distributes workloads across the cluster nodes, balancing the load to maximize performance.
4. **Monitoring and Accounting**: It provides tools for monitoring the status of jobs and nodes, as well as accounting for resource usage.
5. **Security and Policies**: Slurm includes features for managing user authentication, permission controls, node isolation, job sandboxing, enforcing resource limits, and setting job timeout policies within a SLURM-managed HPC cluster.

Slurm is highly configurable and can be tailored to meet the specific needs of different HPC environments, making it a popular choice for research institutions, universities, and enterprises that require robust and scalable computing solutions.
================================================================================
================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

What&#39;s the weather like in Italy?
==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

I&#39;m sorry, but I don&#39;t have the capability to provide real-time information like weather updates. However, you can easily find this information by checking a weather website or application.
================================================================================
================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

What are the names of the QOS queues available on the Leonardo supercomputer BOOSTER partition?
==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================
Tool Calls:
  search_hpc_wiki (rbruFCqff)
 Call ID: rbruFCqff
  Args:
    query: QOS queues available on the Leonardo supercomputer BOOSTER partition
=================================<span class=" -Color -Color-Bold"> Tool Message </span>=================================
Name: search_hpc_wiki

[RETRIEVED_RESOURCES]:

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: .. tab-item:: Booster

        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**   | **Priority** | **Notes**                           |
        +================+====================+=========================+==============+=================================+==============+=====================================+
        | lrd_all_serial | normal             | 4 cores                 | 04:00:00     | 1 node / 4 cores                | 40           | No GPUs

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: .. note::

          The partitions: **boost_fua_dbg, boost_fua_prod** can be exclusively used by Eurofusion users. For more information see the dedicated :ref:`specific_users/specific_users:Eurofusion` section.

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: , Hyperthreading x 2         |
        |                |                    |                         |              |                                 |              |                                     |
        | (**default**)  |                    | (8 logical cores)       |              | (30800 MB RAM)                  |              | **Budget Free**                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_usr_prod | normal             | 64 nodes                | 24:00:00     |                                 | 40           |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_dbg      | 2 nodes                 | 00:30:00     | 2 nodes / 64 cores / 8 GPUs     | 80           |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_bprod    | min = 65 nodes          | 24:00:00     | 256 nodes                       | 60           |                                     |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    | max = 256 nodes         |              |                                 |              |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_lprod    | 3 nodes                 | 4-00:00:00   | 3 nodes / 12 GPUs               | 40           |                                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_fua_dbg  | normal             | 2 nodes                 | 00:10:00     | 2 nodes / 64 cores / 8 GPUs     | 40           | Runs on 2 nodes                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | boost_fua_prod | normal             | 16 nodes                | 24:00:00     | 4 running jobs per user account | 40           |                                     |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    |                         |              | 32 nodes / 3584 cores           |              |                                     |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | boost_qos_fuabprod | min = 17 nodes          | 24:00:00     | 32 nodes / 3584 cores           | 60           | Runs on 49 nodes                    |
        |                |                    |                         |              |                                 |              |                                     |
        |                |                    | max = 32 nodes          |              |                                 |              | Min is 17 FULL nodes                |
        +                +--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        |                | qos_fualowprio     | 16 nodes                | 08:00:00     |                                 | 0            |                                     |
        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

The QOS queues available on the Leonardo supercomputer BOOSTER partition are:

- normal
- boost_qos_dbg
- boost_qos_bprod
- boost_qos_lprod
- boost_qos_fuabprod
- qos_fualowprio
================================================================================
================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

What is the scheduler used on leonardo? slurm or pbs?
==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================
Tool Calls:
  search_hpc_wiki (BIemP3LGZ)
 Call ID: BIemP3LGZ
  Args:
    query: scheduler used on leonardo
=================================<span class=" -Color -Color-Bold"> Tool Message </span>=================================
Name: search_hpc_wiki

[RETRIEVED_RESOURCES]:

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: .. _leonardo_card:

Leonardo
========

Leonardo is the *pre-exascale* Tier-0 supercomputer of the EuroHPC Joint Undertaking (JU), hosted by **CINECA** and currently located at the Bologna DAMA-Technopole in Italy.
This guide provides specific information about the **Leonardo** cluster, including details that differ from the general behavior described in the broader HPC Clusters section.

.. |ico2| image:: img/leonardo_logo.png
   :height: 55px
   :class: no-scaled-link

Access to the System
--------------------

The machine is reachable via ``ssh`` (secure Shell) protocol at hostname point: **login.leonardo.cineca.it**. 

The connection is established, automatically, to one of the available login nodes. It is possible to connect to **Leonardo** using one the specific login hostname points:

 * login01-ext.leonardo.cineca.it
 * login02-ext.leonardo.cineca.it
 * login05-ext.leonardo.cineca.it
 * login07-ext.leonardo.cineca.it

.. warning::
    
    **The mandatory access to Leonardo si the two-factor authetication (2FA)**. Get more information at section :ref:`general/access:Access to the Systems`.

System Architecture
-------------------

The cluster, supplied by EVIDEN ATOS, is based on two new specifically-designed compute blades, which are available throught two distinc Slurm partitios on the Cluster:

* X2135 **GPU** blade based on NVIDIA Ampere A100-64 accelerators - **Booster** partition.
* X2140 **CPU**-only blade based on Intel Sapphire Rapids processors - **Data Centric General Purpose (DCGP)** partition.

The overall system architecture uses NVIDIA Mellanox InfiniBand High Data Rate (HDR) connectivity, with smart in-network computing acceleration engines that enable extremely low latency and high data throughput to provide the highest AI and HPC application performance and scalability. 

The **Booster** partition entered pre-production in May 2023 and moved to **full production in July 2023**.
The **DCGP** partition followed, starting pre-production in January 2024 and reaching **full production in February 2024**.

Hardware Details
^^^^^^^^^^^^^^^^

.. tab-set::

    .. tab-item:: Booster

[DOCUMENT_TITLE]: hpc_intro.rst.txt
[DOCUMENT_CONTENT]: .. dropdown:: Example
   :animate: fade-in-slide-down
   :chevron: down-up

   A user requests 1 node, 4 CPUs, 4 GPUs, and 3 hours of walltime on the Booster partition of Leonardo. However, the job runs for only 2 hours.

   From this information, we have:

   * T = 2 h (elapsed time)

   * N = 1 node

   * C = 32 CPUs (number of CPUs available on a Leonardo Booster compute node â see :ref:`hpc/leonardo:Hardware Details`)

   and, since:

   .. math::

      \frac{\text{Allocated}(\text{CPU})}{\text{Total}(\text{CPU})} = \frac{4}{32} = 0.125

   .. math::

      \frac{\text{Allocated}(\text{GPU})}{\text{Total}(\text{GPU})} = \frac{4}{4} = 1.0

   the maximum of the resources requested per node is determined by the GPUs, therefore *R* = 1.0, and the billed hours are then calculated as:

   .. math::

      B_{H} = T \cdot N \cdot R \cdot C = 2 \cdot 1 \cdot 1.0 \cdot 32 = 64 \text{CPUh}

   This means the job consumes 64 effective CPU hours from the project&#39;s budget.

----

.. note::

   * The **serial partition** is available for limited post-production data analysis and can be used even after a Project Account has expired. Usage of this partition is excluded from STDH billing (**free of charge**).

   * By default, the amount of memory allocated per node is proportional to the number of CPUs requested.

   * When nodes are requested in **exclusive mode** (see :ref:`hpc/hpc_scheduler:Scheduler and Job Submission` section), the entire node is reserved for the job, regardless of the specific resources requested. In such cases, the allocated resources may exceed the explicitly requested ones.

   * The **resources per node** are listed in the **Hardware Details** section for each cluster. Refer to the :ref:`hpc/hpc_clusters:Cluster Specifics` section for the complete list of Cineca&#39;s HPC systems.

Budget Linearization
^^^^^^^^^^^^^^^^^^^^

A linearization policy governs the priority of scheduled jobs across Cineca clusters. To each Project Account is assigned a monthly quota (MQ) calculated as: 

.. math::

   MQ = TB/NM

TB = total assigned budget

NM = total number of months

Beginning on the first day of each month, any User Accounts belonging a Project Account may utilize their quota at full priority. 
As the budget is consumed, submitted jobs progressively lose priority until the monthly quota is exhausted. 
Subsequently, these jobs are still considered for execution but with reduced priority compared to accounts with remaining quota. 
This policy aligns with practices at other prominent HPC centers globally, aiming to enhance response times by aligning CPU hour usage with budget sizes.

[DOCUMENT_TITLE]: leonardo.rst.txt
[DOCUMENT_CONTENT]: .. tab-item:: Booster

        +----------------+--------------------+-------------------------+--------------+---------------------------------+--------------+-------------------------------------+
        | **Partition**  | **QOS**            | **#Cores/#GPU per job** | **Walltime** | **Max Nodes/cores/GPUs/user**   | **Priority** | **Notes**                           |
        +================+====================+=========================+==============+=================================+==============+=====================================+
        | lrd_all_serial | normal             | 4 cores                 | 04:00:00     | 1 node / 4 cores                | 40           | No GPUs


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

The scheduler used on Leonardo is **Slurm**.
================================================================================
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="next-steps">
<h1>Next steps<a class="headerlink" href="#next-steps" title="Link to this heading"></a></h1>
<p>In this tutorial, we have seen how a RAG system can be implemented using semantic search and how multiple document bases can be integrated into a workflow and agentic setups.</p>
<p>Although this tutorial was very introductory, I hope it helps you on your GenAI journey. If you want to take your knowledge to the next level, here are some additional topics you might want to explore:</p>
<ul class="simple">
<li><p>Hypothetical document embeddings and hypothetical questions (a powerful technique to improve retrieval results);</p></li>
<li><p>Memory management in agents;</p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>